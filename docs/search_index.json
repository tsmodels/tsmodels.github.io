[["index.html", "A Time Series Framework Chapter 1 Installation", " A Time Series Framework The tsmodels team 2020-10-29 Chapter 1 Installation Until such time as the packages make it to CRAN, please use the following: devtools::install_github(&quot;tsmodels/tsmethods&quot;) devtools::install_github(&quot;tsmodels/tsdatasets&quot;) devtools::install_github(&quot;tsmodels/tsaux&quot;) devtools::install_github(&quot;tsmodels/tsets&quot;) devtools::install_github(&quot;tsmodels/tsissm&quot;) devtools::install_github(&quot;tsmodels/tsvets&quot;) devtools::install_github(&quot;tsmodels/tsforeign&quot;) The first 2 are required to be installed before any of the other packages. Also note, that additional dependencies may be required such as Rcpp and RcppArmadillo, and bsts for tsforeign. "],["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction The tsmodels framework provides a set of probabilistic time series forecasting models with common calling conventions and methods. Our objective was to provide a set of models which offer fast estimation, explainable decomposition and probabilistic predictions which can also be ensembled. Many of the models we have so far released are based on R. Hyndman et al. (2008), some of which are already implemented in the forecast package, but we have chosen to re-write the models from scratch based on our own design choices. Some of these choices include exclusive use of xts objects to represent time series data, data.table for certain outputs, probabilistic distributions for predictions via simulation as well as a feature rich set of methods to work with these models. We would be remiss if we were not to mention that alternative implementations exist within the single source of error (SEM) framework in R, including the forecast package of Hyndman et al. (2020) and smooth package of Svetunkov (2020), with their own design decisions. We make no representation as to what is likely to be more useful for a particular user, simply that our representation and framework is the right one for us. The set of packages we have released and plan to release may include bugs, and we encourage users to submit bug reports through the github issues system. A summary of the currently available packages is given below: Package Description tsmethods Time Series S3 Methods, plotting functionality and ensembling tsaux Auxilliary functions used by all packages tsdatasets Datasets for benchmarking and examples tsets ETS Models tsissm Linear Innovations State Space Models with Multiple Seasonality tsvets Vector Innovation Linear State Space Models tsforeign Wrapper for other models [bsts (BSTS package), auto.arima (forecast package)] The following packages will be released in the future as time allows: Package Description tspyramid Hierarchical Probabilistic Reconcilitation using Tree Stuctures tsssm Linear State Space Models tslifecycle Models for Life Cycle Prediction tsfactor Factor Models tscausal Time Series Causal Inference References "],["tsmethods.html", "Chapter 3 tsmethods package 3.1 Introduction 3.2 Ensembling Forecast Distributions 3.3 Demonstration", " Chapter 3 tsmethods package 3.1 Introduction The tsmethods package provides a set of common methods for use in other packages in our framework. Additionally, it also exports plot methods for objects of class tsmodel.predict, which is the output class from all calls to the predict method, as well as for objects of class tsmodel.distribution, which is a subclass within tsmodel.predict providing the simulated or MCMC based forecast distribution.1 Finally, the package also includes the methods for ensembling distributions. Table 3.1 below provides the currently exported methods for each of the packages which have been released. We have tried to work with existing S3 methods in the stats package where possible including: summary, coef, logLik, AIC, fitted, residuals, predict and simulate. However, we have also created custom methods such as estimate (for model estimation from a specification object), tsdecompose (for time series decomposition of structural time series type models), tsfilter for online filtering,2 tsbacktest (for automated backtesting) as well as a number of other methods documented in their individual packages. TABLE 3.1: Implemented Methods methods tsets tsissm tsvets tsforeign estimate     summary     coef    logLik    AIC    fitted     residuals     plot     tsmetrics     tsspec    tsdiagnose    tsdecompose     tsfilter    tsprofile   predict     tsbacktest     simulate    tsbenchmark    tsreport  tscor  tscov  tsaggregate  tsconvert  tsequation 3.2 Ensembling Forecast Distributions Ensembling in the tsmodels framework proceeds as follows: (Separate) models are estimated for either the same series or a set of different series. The residuals of the estimated series are collected and the residual correlation is calculated. For very large-dimensional systems (where \\(N\\) &gt; \\(T\\)) it is possible instead to use a factor model to extract the correlations, but we leave this for a separate discussion. The estimated correlations are then used to generate correlated samples on the unit hypercube (using for instance a copula). These samples are then passed to the innov argument in the predict method of each model, at which time they are transformed to normal random variables with variance equal to the estimated model variance.3 A key reason for using uniform variates and transforming them back into normal variates within the predict method is the possible presence of the Box Cox transformation, which requires that the variance be on the transformed scale rather than the final series scale. It was therefore decided that this approach was more general and less prone to user error.4 The predictive distribution, which is generated by simulation, will then be infused with the cross-sectional correlation of the residuals passed to the predict method. Note: This approach works well for models with a single source of error such as ARIMA and ETS, but NOT for multiple source of error models such as BSTS. The predictive distributions are then passed to the ensemble_modelspec function for validation, followed by calling the tsensemble method on the resulting object with a vector of user specified weights. The next section provides a demonstration of this approach. 3.3 Demonstration Step 1: Estimation suppressMessages(library(tsmethods)) suppressWarnings(suppressMessages(library(tsets))) suppressMessages(library(xts)) suppressMessages(library(copula)) suppressWarnings(suppressMessages(library(viridis))) data(austretail, package = &quot;tsdatasets&quot;) y1 &lt;- austretail[,&quot;SA.DS&quot;] y2 &lt;- austretail[,&quot;ACF.DS&quot;] spec1 &lt;- ets_modelspec(y1, model = &quot;MMM&quot;, frequency = 12) spec2 &lt;- ets_modelspec(y2, model = &quot;MMM&quot;, frequency = 12) mod1 &lt;- estimate(spec1) mod2 &lt;- estimate(spec2) Step 2: Residual Correlation and Copula Construction/Sampling C &lt;- cor(residuals(mod1), residuals(mod2)) cop &lt;- normalCopula(as.numeric(C), dim = 2, dispstr = &quot;un&quot;) set.seed(100) U &lt;- rCopula(5000 * 12, cop) cor(U) ## [,1] [,2] ## [1,] 1.00000 0.58779 ## [2,] 0.58779 1.00000 Step 3: Prediction p1.joint &lt;- predict(mod1, h = 12, nsim = 5000, innov = U[,1]) p2.joint &lt;- predict(mod2, h = 12, nsim = 5000, innov = U[,2]) p1.indep &lt;- predict(mod1, h = 12, nsim = 5000) p2.indep &lt;- predict(mod2, h = 12, nsim = 5000) par(mfrow = c(2,2), mar = c(3,3,3,3)) plot(p1.joint, main = &quot;Correlated Predictions[y1]&quot;, n_original = 52) plot(p2.joint, main = &quot;Correlated Predictions[y2]&quot;, n_original = 52) plot(p1.indep, main = &quot;Independent Predictions[y1]&quot;, n_original = 52) plot(p2.indep, main = &quot;Independent Predictions[y2]&quot;, n_original = 52) We can also plot the distributions directly, since there is a plot method for both tsmodel.predict and tsmodel.distribution, and we can also overlay one on top of another by using the add = TRUE argument. plot(p1.joint$distribution, gradient_color = &quot;whitesmoke&quot;, median_col = &quot;grey&quot;, median_width = 4, interval_color = 3, interval_quantiles = c(0.01, 0.99), main = &quot;SA.DS 12 Month Prediction&quot;) plot(p1.indep$distribution, add = TRUE, median_col = 2, median_width = 1, gradient_color = &quot;whitesmoke&quot;, interval_color = 2, interval_quantiles = c(0.01, 0.99), interval_width = 0.5) Evaluation of Predictive Distributions on Forecast Growth Rates p1.growth.joint &lt;- tsgrowth(p1.joint, d = 1, type = &#39;diff&#39;) p2.growth.joint &lt;- tsgrowth(p2.joint, d = 1, type = &#39;diff&#39;) p1.growth.indep &lt;- tsgrowth(p1.indep, d = 1, type = &#39;diff&#39;) p2.growth.indep &lt;- tsgrowth(p2.indep, d = 1, type = &#39;diff&#39;) D.joint &lt;- cor(p1.growth.joint$distribution, p2.growth.joint$distribution) D.indep &lt;- cor(p1.growth.indep$distribution, p2.growth.indep$distribution) print(round(data.frame(Correlated = diag(D.joint), Independent = diag(D.indep)), 3)) ## Correlated Independent ## 2019-01-31 0.600 -0.021 ## 2019-02-28 0.600 -0.004 ## 2019-03-31 0.597 0.018 ## 2019-04-30 0.606 0.012 ## 2019-05-31 0.598 0.004 ## 2019-06-30 0.602 0.002 ## 2019-07-31 0.604 0.004 ## 2019-08-31 0.609 0.002 ## 2019-09-30 0.624 0.000 ## 2019-10-31 0.608 -0.003 ## 2019-11-30 0.612 -0.009 ## 2019-12-31 0.601 -0.017 par(mfrow = c(2,1), mar = c(3,3,3,3)) image(as.matrix(D.joint), col = rev(grey(seq(0, 1, length = 25))), zlim = c(0, 1), main = &quot;Correlated&quot;) contour(cor(p1.joint$distribution, p2.joint$distribution), add = TRUE, drawlabels = TRUE) image(D.indep, col = rev(grey(seq(0, 1, length = 25))), zlim = c(0, 1), main = &quot;Independent&quot;) contour(D.indep, add = TRUE, drawlabels = TRUE) Step 4: Forecast Ensembling par(mfrow = c(1,1), mar = c(3,3,3,3)) spec.joint &lt;- ensemble_modelspec(p1.joint, p2.joint) ensemble.joint &lt;- tsensemble(spec.joint, weights = c(0.5, 0.5)) plot(ensemble.joint, main = &quot;Ensemble Weighted Forecast&quot;, n_original = 52) The current choice of using base R plotting may change in the future to use either ggplot2 or plotly. Unlike some, we have avoided using stats::filter as this is a function rather than a method. The samples can be transformed to any other distribution using the quantile method of the distribution. Additionally the marginal distributions of the individual series may not all be the same, so this approach provides some greater degree of flexibility. "],["tsets.html", "Chapter 4 tsets package 4.1 Introduction 4.2 Taxonomy of Models 4.3 Extensions 4.4 Some Encompasing Alternatives 4.5 Package Implementation 4.6 Demonstration", " Chapter 4 tsets package 4.1 Introduction Exponential smoothing was proposed by Robert G. Brown, originally in Brown (1959) and later in Brown (1962), where he developed the general exponential smoothing methodology in the context of inventory management, production planning and control. Independently, Charles C. Holt developed a similar method for exponential smoothing of additive trends and an entirely different method for smoothing seasonal data in Holt (1957). This approach gained popularity following Winters (1960), which tested Holts methods with empirical data, from whence the now popular Holt-Winters forecasting system came to prominence. More recently, Hyndman et al. (2002) and Taylor (2003) formalized the framework and provided a taxonomy of the various models under different assumptions on the type of Error (E), Trend (T) and Seasonality (S) components. In their most basic form, exponential smoothing methods are weighted sums of past observations, with the weights decaying exponentially with older observations. They form a simpler alternative to the more complex structural time series models (see Harvey (1990) and West and Harrison (2006)), by adopting the innovations formulation of the state space representation with all sources of error perfectly correlated.5 The Single Source of Errors model is observationally equivalent to the Multiple Source of Errors model under non-restrictive assumptions, and the interested reader is referred to Casals, Sotoca, and Jerez (1999) for a proof of this. Formally, the general linear innovations state space model can be written as: \\[\\begin{equation} \\begin{array}{l} {y_t} &amp;= {\\bf{w&#39;}}{{\\bf{x}}_{t - 1}} + {\\varepsilon _t} ,\\\\ {{\\bf{x}}_t} &amp;= {\\bf{F}}{{\\bf{x}}_{t - 1}} + {\\bf{g}}{\\varepsilon _t}, \\end{array} \\tag{4.1} \\end{equation}\\] where \\(y_t\\) is the observed value at time \\(t\\), \\(\\mathbf{x}_t\\) the vector of state variables (which may include information about the level, slope, seasonal patterns and exogenous regressors), \\(\\mathbf{w}\\) is the observation matrix and \\(\\mathbf{F}\\) the state transition matrix. An innovations state space model can be reduced to an equivalent ARIMA model with the help of the lag operator \\(L\\). The state equation can be rewritten as:6 \\[\\begin{equation} \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){\\mathbf{x}_t} = {\\bf{g}}{\\varepsilon _t}. \\tag{4.2} \\end{equation}\\] Since \\({\\bf{I}} - {\\bf{F}}L\\) may not have an inverse, both sides are multiplied by its adjugate7 \\(adj{\\left( {{\\bf{I}} - {\\bf{F}}L} \\right)}\\) to obtain: \\[\\begin{equation} \\det\\left( {{\\bf{I}} - {\\bf{F}}L} \\right){\\mathbf{x}_t} = adj{\\left( {{\\bf{I}} - {\\bf{F}}L} \\right)}{\\bf{g}}{\\varepsilon _t}. \\tag{4.3} \\end{equation}\\] Applying the \\(\\det\\left( {{\\bf{I}} - {\\bf{F}}L} \\right)\\) to the observation equation: \\[\\begin{equation} \\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){y_t} = {\\bf{w&#39;}}\\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){{\\mathbf{x}}_{t - 1}} + \\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){\\varepsilon _t}, \\tag{4.4} \\end{equation}\\] and finally replacing \\(\\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){{\\mathbf{x}}_{t - 1}}\\) with the state equation formula we obtain: \\[\\begin{equation} \\det \\left({\\bf{I}} - {\\bf{F}}L \\right){y_t} = {\\bf{w&#39;}}\\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){\\bf{g}}{\\varepsilon _{t - 1}} + \\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){\\varepsilon _t}. \\tag{4.5} \\end{equation}\\] Defining \\(\\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right)\\) as \\(\\eta\\left(L\\right)\\) and \\({\\bf{w&#39;}}\\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){\\bf{g}}{L} + \\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right)\\) as \\(\\theta\\left(L\\right)\\) we obtain the typical ARIMA representation8 \\(\\eta \\left( L \\right){y_t} = \\theta \\left( L \\right){\\varepsilon _t}\\), where \\(\\eta\\left(L\\right)\\) and \\(\\theta\\left(L\\right)\\) are polynomials in the lag operator \\(L\\) and may include powers of \\(L\\) related to the seasonal period \\(m\\). To obtain the ARIMA representation, set \\(\\eta \\left( L \\right){y_t} = \\phi \\left( L \\right)\\delta \\left( L \\right){y_t}\\), where \\(\\delta\\left(L\\right)\\) contains all unit roots of the polynomial. Typical examples include the damped local trend model,9 which can be represented by an ARIMA(1,1,2) model and the local linear trend model, which can be represented as an ARIMA(0,2,2) model. On the other hand, an ARIMA(2,0,2) model with complex roots, which gives rise to cyclical behavior, cannot be represented by an exponential smoothing model. 4.2 Taxonomy of Models Table 4.1 presents the taxonomy proposed by Pegels (1969) and Gardner Jr (1985) for exponential smoothing models. In addition, for each of the 12 model combinations presented, it is possible to have either additive or multiplicative errors, giving rise to the ETS formulation of Error, Trend and Seasonal. For instance, the MAM model corresponds to Multiplicative Error, Additive Trend and Multiplicative Seasonality. TABLE 4.1: Exponential Smoothing Model Taxonomy N A M (none) (additive) (multiplicative) N (none) NN NA NM A (additive) AN AA AM M (multiplicative) MN MA MM D (damped) DN DA DM Following Ord, Koehler, and Snyder (1997) and Hyndman et al. (2002), we present a state space formulation the ETS modelling system. \\[\\begin{equation} \\begin{array}{l} {y_t} = h\\left( {{\\mathbf{x}_{t - 1}}} \\right) + k\\left( {{\\mathbf{x}_{t - 1}}} \\right){\\varepsilon _t},\\\\ {\\mathbf{x}_t} = f\\left( {{\\mathbf{x}_{t - 1}}} \\right) + g\\left( {{\\mathbf{x}_{t - 1}}} \\right){\\varepsilon _t}, \\end{array} \\tag{4.6} \\end{equation}\\] where \\({\\varepsilon _t} \\sim N\\left( {0,{\\sigma ^2}} \\right)\\) and \\({\\mathbf{x}_t} = \\left\\{ {{l_t},{b_t},{s_{t - 1}},...,{s_{t - \\left( {m - 1} \\right)}}} \\right\\}\\). Defining \\({e_t} = k\\left( {{x_{t - 1}}} \\right){\\varepsilon _t}\\) and \\({\\mu _t} = h\\left( {{x_{t - 1}}} \\right)\\), then \\({y_t} = {\\mu _t} + {e_t}\\). When errors are additive, then \\(y_t=\\mu_t + \\varepsilon_t\\) and \\(k\\left( {{x_{t - 1}}} \\right)=1\\), whilst when errors are multiplicative, then \\(y_t=\\mu_t\\left(1+\\varepsilon_t\\right)\\) and therefore \\(k\\left( {{x_{t - 1}}} \\right)=\\mu_t\\) so that \\(\\varepsilon_t=\\frac{\\left(y_t-\\mu_t\\right)}{\\mu_t}\\) represents a relative error. For illustration, we show below the additive error with additive trend and seasonality: \\[\\begin{equation} \\begin{array}{l} {{\\hat y}_t} = {l_{t - 1}} + \\phi {b_{t - 1}} + {s_{t - m}},\\\\ {\\varepsilon _t} = {y_t} - {{\\hat y}_t},\\\\ {l_t} = {l_{t - 1}} + \\phi {b_{t - 1}} + \\alpha {\\varepsilon _t},\\\\ {b_t} = \\phi {b_{t - 1}} + \\beta {\\varepsilon _t},\\\\ {s_t} = {s_{t - m}} + \\gamma {\\varepsilon _t}, \\end{array} \\tag{4.7} \\end{equation}\\] with \\(\\phi\\) representing the damping parameter which is equal to one when there is no damping. The \\(h\\)-step ahead forecast is then given as: \\[\\begin{equation} {y_{t + h}} = \\left\\{ {\\begin{array}{*{20}{c}} {{l_t} + h{b_t} + {s_{t - m + h_m^ + }}},&amp;{\\phi \\in \\left\\{ \\emptyset \\right\\}},\\\\ {{l_t} + {\\phi^h}{b_t} + {s_{t - m + h_m^ + }}},&amp;{\\phi \\in [0,1]}, \\end{array}} \\right. \\tag{4.8} \\end{equation}\\] where \\(h_m^ + = \\left[ {\\left( {h - 1} \\right)\\bmod m} \\right] + 1\\). 4.3 Extensions A number of extensions have been suggested and too numerous to outline here. One interesting model proposed by Koehler, Snyder, and Ord (2001) is the decomposition of the MAM model to include power terms as follows: \\[\\begin{equation} \\begin{array}{l} {y_t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right){s_{t - m}} + {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^\\theta }{s^\\delta }_{t - m}{\\varepsilon _t},\\\\ {l_t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right) + \\alpha {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^\\theta }{s_{t - m}^{\\delta - 1}}{\\varepsilon _t},\\\\ {b_t} = \\phi {b_{t - 1}} + \\beta {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^\\theta }{s_{t - m}^{\\delta - 1}}{\\varepsilon _t},\\\\ {s_t} = {s_{t - m}} + \\gamma {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^{\\theta - 1}}{s_{t - m}^\\delta}{\\varepsilon _t}. \\end{array} \\tag{4.9} \\end{equation}\\] When \\(\\theta=1\\) and \\(\\delta=1\\), this reduces to the standard MAM model. When \\(\\theta=0\\) and \\(\\delta=0\\) this is reduces to the AAM model or the AAN model if there is no seasonal term. The exponents can be thought as controlling the degree of heteroscedasticity in the data, since the unscaled residuals \\(\\epsilon_t\\) are distributed as: \\[\\begin{equation} {\\epsilon_t} = {y_t} - \\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right){s_{t - m}},\\quad {\\epsilon_t} | \\mathcal{F}_{t-1} \\sim N\\left( {0,{{\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)}^{2\\theta} }{s_{t-m}^{2\\delta}} \\sigma^2 } \\right), \\tag{4.10} \\end{equation}\\] which is an appealing alternative to Box Cox and related transformations. For instance, as R. Hyndman et al. (2008) 4.4.5 notes, a value of \\(\\theta=1/3\\) would produce a variance proportional to the \\(2/3\\) power of the mean, similar to the cube root transformation. Normalized seasonality, discussed in Roberts (1982) and McKenzie (1986), can be used to de-seasonalize the data, by acting as a filter, and is required for implementing the Wiener-Kolmogorov (WK) filter (smoother). This is discussed in more detail in Chapter 8 of R. Hyndman et al. (2008), and is implemented as an option in the tsets package. Another avenue of interest is in the multivariate generalization of the model presented in De Silva, Hyndman, and Snyder (2010). It has the ability to incorporate common levels, trends or seasonality and is implemented in the tsvets package. 4.4 Some Encompasing Alternatives The general state space representation (see Harvey (1990)), based on the multiple sources of error (MSOE) state space model, provides a more general implementation of the unobserved components model, albeit requiring the use of the Kalman filter for estimation. The model is flexible enough to incorporate many types of additive models, including cyclical behavior and regressors, although the nonlinear (multiplicative) variations require the extended Kalman filter for estimation. The bsts package of Scott and Varian (2015) provides fast and efficient computation of Bayesian Unobserved Components, with the option of a spike and slab prior for regressor regularization, for which we provide a wrapper in the tsforeign package. 4.5 Package Implementation The tsets package implements 4 families of models, whose equations are given in Tables 4.2 and 4.3. These are the full equations assuming all variables (trend, damped, seasonal and regressors) enter the model, but any and all combinations of the variables are allowed. Methods implemented include Quasi-ML estimation, prediction, simulation, plotting and post-estimation diagnostics. TABLE 4.2: AAA and MMM Model Equations Equation AAA MMM Observation \\({y_t} = {l_{t - 1}} + \\phi{b_{t - 1}} + {s_{t - m}} + \\bf{x}_{t - 1}\\bf{w} + \\varepsilon_t\\) \\({y_t} = {l_{t - 1}}{b_{t-1}^\\phi}\\left(\\bf{x}_{t-1}\\bf{w}\\right){s_{t - m}}\\left(1 + \\varepsilon _t\\right)\\) Mean \\({\\mu _t} = {l_{t - 1}} + \\phi{b_{t - 1}} + {s_{t - m}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}\\) \\({\\mu_t} = {l_{t - 1}}{b^\\phi_{t - 1}}\\left( {{{\\bf{x}}_{t-1}}{\\bf{w}}} \\right){s_{t - m}}\\) Distribution \\({\\hat y_t}\\left| {{\\Im _{t - 1}} \\sim N\\left( {{\\mu _t},\\sigma } \\right)} \\right.\\) \\({{\\hat y}_t}\\left| {{\\Im _{t - 1}} \\sim N\\left( {{\\mu _t},{l_{t - 1}}{b_{t-1}^\\phi}\\left( {{{\\bf{x}}_{t-1}}{\\bf{w}}} \\right){s_{t - m}}\\sigma } \\right)} \\right.\\) Error \\({\\varepsilon _t} = {y_t} - {\\mu _t}\\) \\({\\varepsilon _t} = \\frac{{{y_t} - {\\mu _t}}}{{{l_{t - 1}}{b}_{t - 1}^\\phi \\left( {{{\\bf{x}}_{t-1}}{\\bf{w}}} \\right){s_{t - m}}}}\\) Level[State] \\({l_t} = {l_{t - 1}} + \\phi {b_{t - 1}} + \\alpha {\\varepsilon _t}\\) \\({l_t} = {l_{t - 1}}{b_{t-1}^\\phi}\\left( {1 + \\alpha {\\varepsilon _t}} \\right)\\) Trend[State] \\({b_t} = \\phi {b_{t - 1}} + \\beta {\\varepsilon _t}\\) \\({b_t} = {b_{t-1}^\\phi}\\left( {1 + \\beta {\\varepsilon _t}} \\right)\\) Seasonal[State] \\({s_t} = {s_{t - m}} + \\gamma {\\varepsilon _t}\\) \\({s_t} = {s_{t - m}}\\left( {1 + \\gamma {\\varepsilon _t}} \\right)\\) TABLE 4.3: MAM and powerMAM Model Equations Equation MAM powerMAM Observation \\({y_t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right){s_{t - m}}\\left( {1 + {\\varepsilon _t}} \\right)\\) \\({y_t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right){s_{t - m}} + {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right)^\\theta }s_{t - m}^\\delta {\\varepsilon _t}\\) Mean \\({\\mu _t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right){s_{t - m}}\\) \\({\\mu _t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right){s_{t - m}}\\) Distribution \\({\\hat y_t}\\left| {{\\Im _{t - 1}} \\sim N\\left( {{\\mu _t},\\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right){s_{t - m}}\\sigma } \\right)} \\right.\\) \\({\\hat y_t}\\left| {{\\Im _{t - 1}} \\sim N\\left( {{\\mu _t},{{\\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right)}^\\theta }s_{t - m}^\\delta \\sigma } \\right)} \\right.\\) Error \\({\\varepsilon _t} = \\frac{{{y_t} - {\\mu _t}}}{{\\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right){s_{t - m}}}}\\) \\({\\varepsilon _t} = \\frac{{{y_t} - {\\mu _t}}}{{{{\\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right)}^\\theta }s_{t - m}^\\delta }}\\) Level[State] \\({l_t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)\\left( {1 + \\alpha {\\varepsilon _t}} \\right)\\) \\({l_t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right) + \\alpha {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^\\theta }s_{t - m}^{\\delta - 1}{\\varepsilon _t}\\) Trend[State] \\({b_t} = \\phi {b_{t - 1}} + \\beta \\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right){\\varepsilon _t}\\) \\({b_t} = \\phi {b_{t - 1}} + \\beta {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^\\theta }s_{t - m}^{\\delta - 1}{\\varepsilon _t}\\) Seasonal[State] \\({s_t} = {s_{t - m}}\\left( {1 + \\gamma {\\varepsilon _t}} \\right)\\) \\({s_t} = {s_{t - m}} + \\gamma {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^{\\theta - 1}}s_{t - m}^\\delta {\\varepsilon _t}\\) 4.5.1 Constraints Variables in the additive ETS type models need to be constrained in order to achieve stability and forecastability, and the interested reader should consult Section 10.2 of R. J. Hyndman, Akram, and Archibald (2008) for more details. These conditions become increasingly complex depending on the components included in the model. In our implementation we have opted for the following simple conditions: \\(\\alpha \\in \\left[ {0,1} \\right]\\), \\(\\beta \\in \\left[ {0,\\alpha } \\right]\\), \\(\\gamma \\in \\left[ {0,1 - \\alpha } \\right]\\), \\(\\phi \\in \\left[ {0.5,1} \\right]\\), \\(\\theta \\in \\left[ {0,1} \\right]\\), \\(\\delta \\in \\left[ {0,1} \\right]\\), \\(\\sigma \\in {{\\bf{R}}_+ }\\). For multiplicative models, we simply impose that \\(\\max\\left(\\alpha,\\beta,\\gamma\\right)&lt;1\\) and \\(\\varepsilon_t&gt;-1\\), although we do not impose the last condition for estimation (only for simulation). 4.5.2 Initialization To obtain a reasonable set of parameters for the initialization conditions of the states as well as the parameters, we obtain values for \\(l_0\\), \\(b_0\\) and \\(s_0\\) using the heuristic approach described in Section 5.2 of R. Hyndman et al. (2008). There is also an option for estimating the initial states for the seasonal component. 4.5.3 Transformations Variance stabilizing transformations form an important part of the pre-processing of the outcome variable in order to achieve certain statistical properties which help reduce misspecification of the model. At present, we implement the Box-Cox transformation with an option for automatic tuning of the parameter \\(\\lambda\\) using the method of Guerrero (1993), from the tsaux package. 4.6 Demonstration 4.6.1 The Specification Object The entry specification function is called ets_modelspec suppressWarnings(suppressMessages(library(tsets))) args(ets_modelspec) ## function (y, model = &quot;AAN&quot;, damped = FALSE, power = FALSE, xreg = NULL, ## frequency = NULL, lambda = NULL, normalized_seasonality = TRUE, ## fixed_pars = NULL, scale = FALSE, seasonal_init = &quot;fixed&quot;, ## lambda_lower = 0, lambda_upper = 1, sampling = NULL, ...) ## NULL This requires passing in an xts vector y, followed by a number of options described below: model: The ETS model type. damped: Whether to dampen the trend. power: The power MAM model (only applicable to the MAM). xreg: An xts matrix of regressors. frequency: The seasonal frequency of y, only needed if using a seasonal model. lambda: The Box Box transformation parameter. If NA, will estimate it. normalized_seasonality: Whether to impose the normalized approach of McKenzie (1986). fixed_pars: An optional named vector of fixed parameters. scale: Whether to pre-scale the data prior to estimation (will rescale back after estimation). seasonal_init: Whether to estimate or use the heuristic (fixed) values for the initial states. sampling: An optional string denoting the sampling frequency of the data (will try to discover it if NULL). 4.6.2 Estimation For illustration, we use the gas dataset from the tsdatasets package, representing weekly US finished motor gasoline products supplied (in thousands of barrels per day) from February 1991 to May 2005. data(gas, package = &quot;tsdatasets&quot;) spec &lt;- ets_modelspec(gas[1:(NROW(gas) - 52)], model = &quot;AAA&quot;, frequency = 52, lambda = NA) str(spec, max.level = 1) ## List of 5 ## $ target :List of 6 ## $ model :List of 14 ## $ seasonal :List of 1 ## $ transform:List of 3 ## $ xreg :List of 3 ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;tsets.spec&quot; &quot;tsmodel.spec&quot; The specification object returns a list which inherits the tsmodel.spec class and has a number of slots. Common across the tsmodels framework will be the target slot, which has the target variable with details on its sampling frequency among others, the transform slot, which contains the Box Cox transformation and its inverse, and the xreg slot, which contains any optionally including external regressors. mod &lt;- estimate(spec) # automatic selection # mod_auto &lt;- auto_ets(gas[1:(NROW(gas) - 52)], frequency = 52, lambda = NA, # metric = &quot;MSLRE&quot;, cores = 4) summary(mod) ## ## ETS Model [ AAA ] ## ## Param Description Est[Value] ## ------------------------------------------------ ## alpha State[Level-coef] 3.453e-02 ## beta State[Slope-coef] 1.000e-04 ## gamma State[Seasonal-coef] 0.000e+00 ## l0 State[Level-init] 7.108e+03 ## b0 State[Slope-init] 2.762e+00 ## sigma Observation[Standard Deviation] 2.621e+02 ## ## AIC BIC AICc ## 12364.03 12622.87 12374.44 ## ## MAPE MASE MSLRE BIAS ## 0.0262 0.6369 0.0011 9e-04 A number of methods exist for post-estimation inference which we illustrate below. logLik(mod) ## &#39;log Lik.&#39; -6125.014 (df=6) AIC(mod) ## [1] 12364.03 coef(mod) ## alpha beta gamma l0 b0 ## 0.03453123 0.00010000 0.00000000 7107.77952558 2.76244932 ## s0 s1 s2 s3 s4 ## -506.79631332 -447.01230297 -476.68115042 -131.08931857 -75.93870995 ## s5 s6 s7 s8 s9 ## 143.22584577 46.91957489 43.84601545 -181.85203203 -122.91905990 ## s10 s11 s12 s13 s14 ## -40.83137039 103.34065156 62.24143451 58.63763652 132.14003123 ## s15 s16 s17 s18 s19 ## 59.26762186 -31.24376860 -111.26657179 32.98945507 80.23597147 ## s20 s21 s22 s23 s24 ## 168.50761693 317.79677244 249.90941754 359.85242961 244.12706217 ## s25 s26 s27 s28 s29 ## 407.14465533 203.11825390 351.43412704 234.96291063 362.88019416 ## s30 s31 s32 s33 s34 ## 323.72973094 240.69933477 46.86023690 50.98268170 76.04708428 ## s35 s36 s37 s38 s39 ## 186.30912129 45.32728297 -17.11459876 -162.83816007 160.46100413 ## s40 s41 s42 s43 s44 ## -55.24277755 -142.44683934 -168.46179046 -133.42406098 -90.89932875 ## s45 s46 s47 s48 s49 ## -1.46974286 -278.23593662 -266.81064348 -225.95612406 -232.58290454 ## s50 sigma ## -478.24412479 262.06262972 head(residuals(mod, raw = TRUE)) ## residuals ## 1991-02-01 -80.31776 ## 1991-02-08 -203.58198 ## 1991-02-15 -296.03378 ## 1991-02-22 346.48235 ## 1991-03-01 23.83767 ## 1991-03-08 103.65627 head(residuals(mod, raw = FALSE)) ## residuals ## 1991-02-01 -80.36452 ## 1991-02-08 -203.70024 ## 1991-02-15 -296.20633 ## 1991-02-22 346.68537 ## 1991-03-01 23.85160 ## 1991-03-08 103.71685 head(fitted(mod)) ## fitted ## 1991-02-01 6701.365 ## 1991-02-08 6636.700 ## 1991-02-15 6878.206 ## 1991-02-22 6877.315 ## 1991-03-01 6851.148 ## 1991-03-08 6843.283 plot(mod) tsdiagnose(mod) ## ## Weighted Ljung-Box Test [scaled residuals] ## ------------------------------------------ ## Lag statistic pvalue ## [1] 9.09 0.00256 ## [2] 9.44 0.00269 ## [3] 10.79 0.00195 ## [4] 11.49 0.00220 ## ## Parameter Bounds and Conditions ## ------------------------------------------ ## coef value &gt;lb &lt;ub condition condition_pass ## alpha 0.0345 TRUE TRUE NA NA ## beta 0.0001 TRUE TRUE &lt; alpha TRUE ## gamma 0.0000 TRUE TRUE &lt; (1 - alpha) TRUE ## phi NA TRUE TRUE NA NA ## theta NA TRUE TRUE NA NA ## delta NA TRUE TRUE NA NA ## ## Outlier Diagnostics (based on Rosner Test) ## ------------------------------------------ ## Outliers: 1998-03-27 tsmetrics(mod) ## n no.pars LogLik AIC BIC AICc MAPE MASE ## 1 693 56 -6125.014 12364.03 12622.87 12374.44 0.02621505 0.636897 ## MSLRE BIAS ## 1 0.001093048 0.0008632586 tsd_mod &lt;- tsdecompose(mod) tsd_mod &lt;- do.call(cbind, lapply(1:length(tsd_mod), function(i) tsd_mod[[i]])) head(tsd_mod) ## fitted Residuals Level Slope Seasonal ## 1991-02-01 6701.365 -80.31776 7107.769 2.754418 -478.244125 ## 1991-02-08 6636.700 -203.58198 7103.493 2.734059 -232.582905 ## 1991-02-15 6878.206 -296.03378 7096.005 2.704456 -225.956124 ## 1991-02-22 6877.315 346.48235 7110.674 2.739104 -266.810643 ## 1991-03-01 6851.148 23.83767 7114.236 2.741488 -278.235937 ## 1991-03-08 6843.283 103.65627 7120.557 2.751854 -1.469743 4.6.3 Prediction All prediction objects in the tsmodels framework are of class tsmodel.predict, with slots for the original series and the forecast distribution (the latter being of class tsmodel.distribution). Some prediction objects will contain additional slots, usually the original specification object as well as the state component predicted decomposition (also of class tsmodel.distribution). p &lt;- predict(mod, h = 52, nsim = 5000) class(p) ## [1] &quot;tsets.predict&quot; &quot;tsmodel.predict&quot; str(p, max.level = 1) ## List of 6 ## $ distribution : &#39;tsets.distribution&#39; num [1:5000, 1:52] 9362 8703 8938 9120 9283 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## ..- attr(*, &quot;date_class&quot;)= chr &quot;Date&quot; ## $ original_series:&#39;zoo&#39; series from 1991-02-01 to 2004-05-07 ## Data: num [1:693] 6621 6433 6582 7224 6875 ... ## Index: Date[1:693], format: &quot;1991-02-01&quot; &quot;1991-02-08&quot; ... ## $ h : num 52 ## $ spec :List of 5 ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;tsets.spec&quot; &quot;tsmodel.spec&quot; ## $ decomposition :List of 6 ## $ mean :&#39;zoo&#39; series from 2004-05-14 to 2005-05-06 ## Data: Named num [1:52] 9147 9125 9126 9316 9403 ... ## ..- attr(*, &quot;names&quot;)= chr [1:52] &quot;2004-05-14&quot; &quot;2004-05-21&quot; &quot;2004-05-28&quot; &quot;2004-06-04&quot; ... ## Index: Date[1:52], format: &quot;2004-05-14&quot; &quot;2004-05-21&quot; ... ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;tsets.predict&quot; &quot;tsmodel.predict&quot; plot(p, n_original = 52*4) p_decomp &lt;- tsdecompose(p) par(mfrow = c(2,2), mar = c(3,3,3,3)) plot(p_decomp$Level, main = &quot;Level[State] Predicted Distribution&quot;) plot(p_decomp$Slope, main = &quot;Slope[State] Predicted Distribution&quot;) plot(p_decomp$Seasonal, main = &quot;Seasonal[State] Predicted Distribution&quot;) plot(p_decomp$Error, main = &quot;Simulated Error Distribution&quot;) Since we left 52 points for out-of-sample testing, we are able to evaluate the prediction using the tsmetrics method on a predicted object. This method also takes the original series as an input in order to calculate MASE in the presence of seasonality. The alpha parameter is the coverage rate for calculation of the Mean Interval Score of Gneiting and Raftery (2007). tsmetrics(p, tail(gas, 52), original_series = spec$target$y_orig, alpha = 0.05) ## h MAPE MASE MSLRE BIAS MIS ## 1 52 0.01445789 0.4013524 0.0003933292 0.003362753 1088.3 tsd_predict &lt;- tsdecompose(p) par(mfrow = c(2,2), mar = c(3,3,3,3)) plot(tsd_predict$forecast, main = &quot;Prediction&quot;) plot(tsd_predict$Level, main = &quot;Level[State]&quot;) plot(tsd_predict$Slope, main = &quot;Slope[State]&quot;) plot(tsd_predict$Seasonal, main = &quot;Seasonal[State]&quot;) 4.6.4 Filtering Online filtering is when new data arrives and instead of re-estimating the model, we instead just filter the new data based on an existing model. In the tsets package the tsfilter method updates an object of class tsmodel.estimate with new data as the example below illustrates. Because the class of the model is retained and only updated (both data and states) with new information, it is also possible to apply any method to that which admits that object (e.g. predict). mod_filter &lt;- tsfilter(mod, y = gas[(NROW(gas) - 52 + 1)]) tail(fitted(mod),1) ## fitted ## 2004-05-07 9259.272 tail(fitted(mod_filter),2) ## fitted ## 2004-05-07 9259.272 ## 2004-05-14 9146.997 mod_filter &lt;- tsfilter(mod_filter, y = gas[(NROW(gas) - 52 + 2)]) tail(fitted(mod),1) ## fitted ## 2004-05-07 9259.272 tail(fitted(mod_filter),3) ## fitted ## 2004-05-07 9259.272 ## 2004-05-14 9146.997 ## 2004-05-21 9125.058 head(predict(mod, h = 12)$mean) ## 2004-05-14 2004-05-21 2004-05-28 2004-06-04 2004-06-11 2004-06-18 ## 9162.706 9139.950 9118.101 9322.523 9406.679 9467.352 head(predict(mod_filter, h = 12)$mean) ## 2004-05-28 2004-06-04 2004-06-11 2004-06-18 2004-06-25 2004-07-02 ## 9122.006 9316.075 9409.362 9436.874 9325.672 9452.225 4.6.5 Simulation An estimated object can also be simulated from, with the parameters and initial states overridden by passing them as named values in the pars argument. The default is to initialize the states from the seed states used in the estimated object, with h equal to the length of the original series (default for NULL h). Innovations for the simulation can either be parametric (normal for additive or truncated normal for multiplicative error models), based on the estimated residuals (bootstrap argument) or a user supplied set of uniform random numbers (which are then translated into normal or truncated normal using standard deviation equal to the model sigma and optionally scaled by sigma_scale). sim &lt;- simulate(mod, nsim = 10, h = 52*2) plot(sim) par(mar = c(3,3,3,3), mfrow = c(4, 1)) matplot(as.Date(colnames(sim$Simulated)), t(sim$Simulated[1:10, ]), type = &quot;l&quot;, ylab = &quot;&quot;, main = &quot;Simulated Paths&quot;, xlab = &quot;&quot;) grid() matplot(as.Date(colnames(sim$Simulated)), t(sim$Level[1:10, ]), type = &quot;l&quot;, ylab = &quot;&quot;, main = &quot;Simulated Level&quot;, xlab = &quot;&quot;) grid() matplot(as.Date(colnames(sim$Simulated)), t(sim$Slope[1:10, ]), type = &quot;l&quot;, ylab = &quot;&quot;, main = &quot;Simulated Slope&quot;, xlab = &quot;&quot;) grid() # not much on the seasonal since gamma coefficient is zero matplot(as.Date(colnames(sim$Simulated)), t(sim$Seasonal[1:10, ]), type = &quot;l&quot;, ylab = &quot;&quot;, main = &quot;Simulated Seasonal&quot;, xlab = &quot;&quot;) grid() 4.6.6 Profiling The tsprofile function profiles an estimated model by simulating and then estimating multiple paths from the assumed DGP while leaving h values out for prediction evaluation. Each simulated path is equal to the size of the original dataset plus h additional values, and initialized with the initial state vector from the model. The resulting output contains the distribution of the MAPE, percent bias (BIAS) and mean squared log relative error (MSLRE) per horizon h. Since these matrices are of class tsmodel.distribution, they can be readily plotted with the special purpose plot function for this class from the tsmethods package. Additionally, a data.table matrix is also return with the distribution of the coefficients from each path estimation. # profiling prof &lt;- tsprofile(mod, h = 52, nsim = 1000, cores = 4, trace = 0) plot(prof, type = &quot;metrics&quot;) plot(prof, type = &quot;coef&quot;) 4.6.7 Backtesting The tsbacktest method generates an expanding window, walk forward backtest, returning a list with the estimation/horizon predictions against actuals, as well as a table of average performance metrics by horizon. b &lt;- tsbacktest(spec, h = 52, alpha = c(0.02, 0.1), trace = 0, cores = 4) head(b$metrics) ## horizon variable MAPE MSLRE BIAS n MIS[0.02] MIS[0.1] ## 1: 1 y 0.02710287 0.001165524 -0.0004851709 347 1750.719 1181.645 ## 2: 2 y 0.02695862 0.001155817 -0.0005105239 346 1694.395 1180.730 ## 3: 3 y 0.02698960 0.001148191 -0.0005009782 345 1689.606 1179.126 ## 4: 4 y 0.02712264 0.001161432 -0.0004446362 344 1709.448 1178.480 ## 5: 5 y 0.02704575 0.001164291 -0.0005618452 343 1720.270 1193.366 ## 6: 6 y 0.02686702 0.001139084 -0.0008830126 342 1643.976 1176.132 plot(b$metrics$horizon, b$metrics$MAPE*100, main = &quot;Horizon MAPE&quot;, type = &quot;l&quot;, ylab = &quot;MAPE[%]&quot;, xlab = &quot;horizon&quot;) grid() 4.6.8 Benchmarking The tsbenchmark method is used to benchmark a model for timing and accuracy and can be used as a unit testing function. bench &lt;- rbind(tsbenchmark(spec, solver = &quot;optim&quot;), tsbenchmark(spec, solver = &quot;nlminb&quot;), tsbenchmark(spec, solver = &quot;solnp&quot;)) bench ## start end spec estimate ## 1: 2020-10-29 15:46:45 2020-10-29 15:46:46 &lt;tsets.spec[5]&gt; &lt;tsets.estimate[3]&gt; ## 2: 2020-10-29 15:46:46 2020-10-29 15:46:46 &lt;tsets.spec[5]&gt; &lt;tsets.estimate[3]&gt; ## 3: 2020-10-29 15:46:46 2020-10-29 15:46:46 &lt;tsets.spec[5]&gt; &lt;tsets.estimate[3]&gt; ## solver control loglik ## 1: optim 0 -6126.568 ## 2: nlminb 0 -6125.014 ## 3: solnp 0 -6125.014 References "],["tsissm.html", "Chapter 5 tsissm package 5.1 Introduction 5.2 State Initialization 5.3 Log-Likelihood 5.4 Package Implementation 5.5 Demonstration", " Chapter 5 tsissm package 5.1 Introduction The tsissm package implements the linear (and homoscedastic) innovations state space model described in De Livera, Hyndman, and Snyder (2011) and originally proposed by Anderson and Moore (2012). It is in some ways similar to the ETS based models described in R. Hyndman et al. (2008), but with the flexibility of incorporating multiple seasonality, ARMA terms and joint maximization of the Box Cox variance stabilizing lambda parameter. The following is taken from De Livera, Hyndman, and Snyder (2011). Consider the following SEM model: \\[\\begin{equation} \\begin{array}{l} y_t^\\lambda = {\\bf{w&#39;}}{{\\bf{x}}_{t - 1}} + \\bf{c&#39;}\\bf{u}_{t - 1} + {\\varepsilon _t}, \\quad \\varepsilon_t\\sim N\\left(0,\\sigma^2\\right),\\\\ {{\\bf{x}}_t} = {\\bf{F}}{{\\bf{x}}_{t - 1}} + {\\bf{g}}{\\varepsilon _t}, \\end{array} \\tag{5.1} \\end{equation}\\] where \\(\\lambda\\) represents the Box Cox parameter, \\(\\bf{w}\\) the observation coefficient vector, \\(\\bf{x}_t\\) the unobserved state vector, \\(\\bf{c}\\) a vector of coefficients on the external regressor set \\(\\bf{u}\\). Define the state vector10 as: \\[\\begin{equation} \\bf{x}_t = {\\left( {{l_t},{b_t},s_t^{(1)},\\dots,s_t^{(T)},{d_t},{d_{t - 1}},\\dots,{d_{t - p - 1}},{\\varepsilon_t},{\\varepsilon_{t - 1}},\\dots,{\\varepsilon _{t - q - 1}}} \\right)^\\prime }, \\tag{5.2} \\end{equation}\\] where \\(\\bf{s}_t^{(i)}\\) is the row vector \\(\\left( {s_{1,t}^{(i)},s_{2,t}^{(i)},\\dots,s_{{k_i},t}^{(i)},s_{1,t}^{*(i)},s_{2,t}^{*(i)},\\dots,s_{{k_i},t}^{*(i)}} \\right)\\) in the case of trigonometric seasonality, and \\(\\left( {s_t^{(i)},s_{t - 1}^{(i)},\\dots,s_{t - ({m_i} - 1)}^{(i)}} \\right)\\) in the case of regular seasonality. Also define \\(\\bf{1}_r\\) and \\(\\bf{0}_r\\) as a vector of ones and zeros, respectively, of length \\(r\\), \\(\\bf{O}_{u,v}\\) a \\(u\\times v\\) matrix of zeros and \\(\\bf{I}_{u,v}\\) a \\(u\\times v\\) diagonal matrix of ones; let \\(\\mathbf{\\gamma} = \\left(\\mathbf{\\gamma}^{(1)},\\dots,\\mathbf{\\gamma}^{(T)} \\right)\\) be a vector of seasonal parameters with \\({\\gamma ^{(i)}} = \\left( {\\gamma _1^{(i)}{{\\bf{1}}_{{k_i}}},\\gamma _2^{(i)}{{\\bf{1}}_{{k_i}}}} \\right)\\) in the trigonometric seasonality case (with \\(k\\) harmonics), and \\({\\gamma ^{(i)}} = \\left( {{\\gamma _i},{{\\bf{0}}_{{m_i} - 1}}} \\right)\\) in the regular seasonality case; define \\(\\theta = \\left( {{\\theta_1},{\\theta_2},\\dots,{\\theta _p}} \\right)\\) and \\(\\psi = \\left( {{\\psi _1},{\\psi _2},\\dots,{\\psi_q}} \\right)\\) as the vector of AR(p) and MA(q) parameters, respectively. Define the observation transition vector \\({\\bf{w}} = {\\left( {1,\\phi ,{\\bf{a}},\\theta ,\\psi } \\right)^\\prime }\\), where \\({\\bf{a}} = \\left( {{{\\bf{a}}^{(1)}},\\dots,{{\\bf{a}}^{(T)}}} \\right)\\) with \\({{\\bf{a}}^{(i)}} = \\left( {{{\\bf{1}}_{{k_i}}},{{\\bf{0}}_{{k_i}}}} \\right)\\) for the trigonometric case and \\({{\\bf{a}}^{(i)}} = \\left( {{{\\bf{0}}_{{m_i} - 1}},1} \\right)\\) for the regular seasonality case. Define the state error adjustment vector \\({\\bf{g}} = {\\left( {\\alpha ,\\beta ,\\gamma ,1,{{\\bf{0}}_{p - 1}},1,{{\\bf{0}}_{q - 1}}} \\right)^\\prime }\\). Further, let \\({\\bf{B}} = \\gamma &#39;\\theta\\), \\({\\bf{C}} = \\gamma &#39;\\psi\\) and \\({\\bf{A}} = \\oplus _{i = 1}^T{{\\bf{A}}_i}\\), with \\[\\begin{equation} {{\\bf{A}}_i} = \\left[ {\\begin{array}{*{20}{c}} {{{\\bf{C}}^{(i)}}}&amp;{{{\\bf{S}}^{(i)}}}\\\\ { - {{\\bf{S}}^{(i)}}}&amp;{{{\\bf{C}}^{(i)}}} \\end{array}} \\right], \\tag{5.3} \\end{equation}\\] for the trigonometric case and with \\(\\bf{C}^{(i)}\\) and \\(\\bf{S}^{(i)}\\) representing the \\(k_i\\times k_i\\) diagonal matrices with elements \\(cos(\\lambda_j^{(i)})\\) and \\(sin(\\lambda_j^{(i)})\\) respectively,11 and \\[\\begin{equation} {{\\bf{A}}_i} = \\left[ {\\begin{array}{*{20}{c}} {{{\\bf{0}}_{{m_i} - 1}}}&amp;1\\\\ {{{\\bf{I}}_{{m_i} - 1}}}&amp;{{{{\\bf{0&#39;}}}_{{m_i} - 1}}} \\end{array}} \\right], \\tag{5.4} \\end{equation}\\] for the regular seasonality case, with \\(\\oplus\\) being the direct sum of matrices operator. Finally, the state transition matrix \\(\\bf{F}\\) is composed as follows: \\[\\begin{equation} {\\bf{F}} = \\left[ {\\begin{array}{*{20}{c}} 1&amp;\\phi &amp;{{{\\bf{0}}_\\tau }}&amp;{\\alpha \\theta }&amp;{\\alpha \\psi }\\\\ 0&amp;\\phi &amp;{{{\\bf{0}}_\\tau }}&amp;{\\beta \\theta }&amp;{\\beta \\psi }\\\\ {{{{\\bf{0&#39;}}}_\\tau }}&amp;{{{{\\bf{0&#39;}}}_\\tau }}&amp;{\\bf{A}}&amp;{\\bf{B}}&amp;{\\bf{C}}\\\\ 0&amp;0&amp;{{{\\bf{0}}_\\tau }}&amp;\\theta &amp;\\psi \\\\ {{{{\\bf{0&#39;}}}_{p - 1}}}&amp;{{{{\\bf{0&#39;}}}_{p - 1}}}&amp;{{{\\bf{O}}_{p - 1,\\tau }}}&amp;{{{\\bf{I}}_{p - 1,p}}}&amp;{{{\\bf{O}}_{p - 1,q}}}\\\\ 0&amp;0&amp;{{{\\bf{0}}_\\tau }}&amp;{{{\\bf{0}}_p}}&amp;{{{\\bf{0}}_q}}\\\\ {{{{\\bf{0&#39;}}}_{q - 1}}}&amp;{{{{\\bf{0&#39;}}}_{q - 1}}}&amp;{{{\\bf{O}}_{q - 1,\\tau }}}&amp;{{{\\bf{O}}_{q - 1,p}}}&amp;{{{\\bf{I}}_{q - 1,q}}} \\end{array}} \\right] \\tag{5.5} \\end{equation}\\] where \\(\\tau = 2\\sum\\limits_{i = 1}^T {{k_i}}\\) for the trigonometric case and \\(\\tau = \\sum\\limits_{i = 1}^T {{m_i}}\\) for the regular seasonality case. 5.2 State Initialization A key innovation of the De Livera, Hyndman, and Snyder (2011) paper is providing the exact initialization of the non-stationary components seed states, the exponential smoothing analogue of the De Jong and others (1991) method for augmenting the Kalman filter to handle seed states with infinite variances. The proof, based on De Livera, Hyndman, and Snyder (2011) and expanded here is as follows: Let: \\[\\begin{equation} {\\bf{D}} = {\\bf{F}} - {\\bf{gw&#39;}}. \\tag{5.6} \\end{equation}\\] We eliminate \\(\\varepsilon_t\\) in (5.1) to give: \\[\\begin{equation} {{\\bf{x}}_t} = {\\bf{D}}{{\\bf{x}}_{t - 1}} + {\\bf{g}}{y_t}. \\tag{5.7} \\end{equation}\\] Next, we proceed by backsolving the equation for the error, given a given value of \\(\\lambda\\):12 \\[\\begin{equation} \\begin{array}{l} {\\varepsilon _t} = {y_t} - {\\bf{w}}{{{\\bf{\\hat x}}}_{t - 1}},\\\\ {\\varepsilon _t} = {y_t} - {\\bf{w&#39;}}\\left( {{\\bf{D}}{{{\\bf{\\hat x}}}_{t - 2}} + {\\bf{g}}{y_{t - 1}}} \\right). \\end{array} \\tag{5.8} \\end{equation}\\] Starting with \\(t = 4\\) and working backwards: \\[\\begin{equation} \\begin{array}{l} {\\varepsilon _4} &amp;= {y_4} - {\\bf{w&#39;}}\\left( {{\\bf{D}}{{{\\bf{\\hat x}}}_2} + {\\bf{g}}{y_3}} \\right)\\\\ &amp;= {y_4} - {\\bf{w&#39;}}\\left( {{\\bf{D}}\\left( {{\\bf{D}}{{{\\bf{\\hat x}}}_1} + {\\bf{g}}{y_2}} \\right) + {\\bf{g}}{y_3}} \\right)\\\\ &amp;= {y_4} - {\\bf{w&#39;}}\\left( {{\\bf{D}}\\left( {{\\bf{D}}\\left( {{\\bf{D}}{{{\\bf{\\hat x}}}_0} + {\\bf{g}}{y_1}} \\right) + {\\bf{g}}{y_2}} \\right) + {\\bf{g}}{y_3}} \\right)\\\\ &amp;= {y_4} - {\\bf{w&#39;}}\\left( {{\\bf{D}}\\left( {{{\\bf{D}}^2}{{\\bf{x}}_0} + {\\bf{Dg}}{y_1} + {\\bf{g}}{y_2}} \\right) + {\\bf{g}}{y_3}} \\right)\\\\ &amp;= {y_4} - {\\bf{w&#39;}}\\left( {{{\\bf{D}}^3}{{\\bf{x}}_0} + {{\\bf{D}}^2}{\\bf{g}}{y_1} + {\\bf{Dg}}{y_2} + {\\bf{g}}{y_3}} \\right)\\\\ &amp;= {y_4} - {\\bf{w&#39;}}\\sum\\limits_{j = 1}^3 {{{\\bf{D}}^{j - 1}}{\\bf{g}}{y_{4 - j}} - {\\bf{w&#39;}}{{\\bf{D}}^3}{{\\bf{x}}_0}} \\\\ \\end{array} \\tag{5.9} \\end{equation}\\] and generalizing to \\(\\varepsilon_t\\): \\[\\begin{equation} \\begin{array}{l} {\\varepsilon _t} &amp;= {y_t} - {\\bf{w&#39;}}\\left( {\\sum\\limits_{j = 1}^{t - 1} {{{\\bf{D}}^{j - 1}}{\\bf{g}}{y_{t - j}}} } \\right) - {\\bf{w&#39;}}{{\\bf{D}}^{t - 1}}{{\\bf{x}}_0}\\\\ &amp;= {y_t} - {\\bf{w&#39;}}{{{\\bf{\\tilde x}}}_{t - 1}} - {{{\\bf{w&#39;}}}_{t - 1}}{{\\bf{x}}_0}\\\\ &amp;= {{\\tilde y}_t} - {{{\\bf{w&#39;}}}_{t - 1}}{{\\bf{x}}_0}, \\end{array} \\tag{5.10} \\end{equation}\\] where \\({{\\tilde y}_t} = {y_t} - {\\bf{w&#39;}}{{{\\bf{\\tilde x}}}_{t - 1}}\\), \\({{{\\bf{\\tilde x}}}_t} = {\\bf{D}}{{{\\bf{\\tilde x}}}_{t - 1}} + {\\bf{g}}{y_t}\\), \\({{{\\bf{w&#39;}}}_t} = {\\bf{D}}{{{\\bf{w&#39;}}}_{t - 1}}\\), \\({{{\\bf{\\tilde x}}}_0} = 0\\) and \\({{{\\bf{w&#39;}}}_0} = {\\bf{w&#39;}}\\), so that \\(\\bf{x}_0\\) are the coefficients from the regression of \\(\\bf{w}\\) on \\(\\boldsymbol{\\varepsilon}\\). While this approach bypasses the need to estimate the initial states by augmenting the parameter vector, which could be very costly for multiple seasonality or large seasonal periods, it still requires one full iteration for \\(i=1,\\dots,t\\) to calculate \\(\\boldsymbol{\\varepsilon}\\) and \\(\\bf{w}\\) and then one inversion to get the coefficients for every new set of parameters (i.e. for each new candidate set in the optimization). 5.3 Log-Likelihood The log-likelihood (\\(L\\)) of the model follows from the assumption that the innovations \\(\\varepsilon_t\\sim N\\left(0, \\sigma^2\\right)\\), leading to the following form for the transformed series \\(y^{\\lambda}_t\\): \\[\\begin{equation} L\\left( \\theta \\right) = - \\frac{T}{2}\\log \\left( {2\\pi {{\\sigma }^2}} \\right) - \\frac{1}{{2{{\\sigma }^2}}}\\sum\\limits_{t = 1}^T {\\varepsilon _t^2 + \\left( {\\lambda - 1} \\right)\\sum\\limits_{t = 1}^T {\\log {y_t}} }. \\tag{5.11} \\end{equation}\\] Concentrating out \\(\\sigma^2\\) with its maximum likelihood estimate, \\({\\hat \\sigma^2} = {T^{ - 1}}\\sum\\limits_{t = 1}^T {\\varepsilon_t^2}\\), eliminating constants and taking the negative for minimization in the optimization routine leads to the following form: \\[\\begin{equation} L\\left( \\theta \\right) = T\\log \\sum\\limits_{t = 1}^T {\\varepsilon _t^2} - 2\\left( {\\lambda - 1} \\right)\\sum\\limits_{t = 1}^T {\\log {y_t}}, \\tag{5.12} \\end{equation}\\] where \\(\\theta\\) is the vector of parameters being optimized. The returned value of calling method logLik on an object of class tsissm.estimate is that of equation (5.11). 5.4 Package Implementation The implementation of the model in the tsissm package differs significantly from the one provided by Hyndman et al. (2020) in the tbats and bats functions, mainly in terms of a more flexible specification object and more complete methods for working with the model, but currently lacks the automatic model selection functionality. Additionally, in order to ensure that the parameters are within the forecastability region, we constrain the characteristics roots of the matrix \\(\\bf{D}\\) in (5.2), representing the non-stationary components to be inside the unit circle, and also constrain the ARMA roots for stationarity.13 5.5 Demonstration 5.5.1 Specification The specification function defines the entry point for setting up an issm model: library(tsissm) args(issm_modelspec) ## function (y, slope = TRUE, slope_damped = FALSE, seasonal = FALSE, ## seasonal_frequency = 1, seasonal_type = c(&quot;trigonometric&quot;, ## &quot;regular&quot;), seasonal_harmonics = NULL, ar = 0, ma = 0, ## xreg = NULL, lambda = 1, sampling = NULL, ...) ## NULL The specification has options for slope, dampening, seasonality (trigonometric and regular),14 AR and MA terms and external regressors. Additionally, lambda can be fixed or estimated (by setting lambda = NA). At present, we do not offer automatic model selection, but instead leave it to the user to decide on the appropriate model. In the future we may include a function for automatic selection similar to the auto_ets function in the tsets package. 5.5.2 Estimation We showcase the functionality of the package using the electricload dataset from the tsdatasets package which represents total hourly electricity load in Greece in MW as published on ENTSO-E Transparency Platform, for the period 2016-10-17 to 2019-04-30. The series appears to have multiple seasonality with periodicity 24, \\(24\\times 7\\) and \\(24\\times 7 \\times 52\\), which we model with trigonometric terms. library(tsissm) data(&quot;electricload&quot;, package = &quot;tsdatasets&quot;) # specification spec &lt;- issm_modelspec(electricload, slope = FALSE, seasonal = TRUE, seasonal_frequency = c(24, 24*7, 24*7*52), seasonal_type = &quot;trigonometric&quot;, seasonal_harmonics = c(4, 4, 2), ar = 2, ma = 2, lambda = 0) # estimation mod &lt;- suppressMessages(estimate(spec, solver = &quot;solnp&quot;, control = list(trace = 0))) mod$opt$timing ## NULL summary(mod) ## ## ------------------------------------- ## tsissm: Summary ## ------------------------------------- ## parameters optimal lower upper ## alpha 0.106703 0.00 0.99 ## gamma24.1 0.016198 -0.01 1.00 ## gamma24.2 -0.000405 -0.01 1.00 ## gamma168.1 -0.000426 -0.01 1.00 ## gamma168.2 -0.004138 -0.01 1.00 ## gamma8736.1 0.012931 -0.01 1.00 ## gamma8736.2 -0.006961 -0.01 1.00 ## theta1 0.438848 -0.99 0.99 ## theta2 0.040952 -0.99 0.99 ## psi1 0.518389 -0.99 0.99 ## psi2 0.050841 -0.99 0.99 ## ## ------------------------------------- ## tsissm: Performance Metrics ## ---------------------------------- ## AIC : 441666.48 (n = 36) ## MAPE : 0.01827 ## BIAS : 3e-04 ## MSLRE : 0.00057 The plot method decomposes the estimated model into its components: plot(mod) but we can also extract these directly and plot them using the tsdecompose method: td &lt;- tsdecompose(mod) plot(as.zoo(td), main = &quot;ISSM Decomposition&quot;) Additional inference methods include diagnostics (tsdiagnose) as well as standard extractors for the coefficients (coef), log-likelihood (logLik) and AIC (AIC): tsdiagnose(mod, plot = TRUE) ## ## ARMA roots ## ------------------------------------------ ## Inverse AR roots: 0.5179186 0.07907106 ## Inverse MA roots: 0.3870275 0.1313619 ## ## Forecastability (D roots) ## ------------------------------------------ ## Real Eigenvalues (D): 0.995 0.995 0.998 0.998 0.99 0.99 1 1 1 1 1 1 0.491 0.491 0.959 0.959 0.699 0.699 0.858 0.858 0.859 0.387 0.131 0 0 ## ## Weighted Ljung-Box Test [scaled residuals] ## ------------------------------------------ ## Lag statistic pvalue ## Lag[1] 179 0 ## Lag[11] 897 0 ## Lag[11] 897 0 ## Lag[11] 897 0 ## ## Outlier Diagnostics (based on Rosner Test) ## ------------------------------------------ ## Outliers: 2019-01-22 14:00:00 2017-05-10 19:00:00 2017-03-02 17:00:00 2019-01-22 09:00:00 2019-01-22 05:00:00 2019-01-03 07:00:00 2017-12-25 12:00:00 2019-01-22 08:00:00 coef(mod) ## alpha gamma24.1 gamma24.2 gamma168.1 gamma168.2 ## 0.1067034287 0.0161975998 -0.0004051414 -0.0004258961 -0.0041378072 ## gamma8736.1 gamma8736.2 theta1 theta2 psi1 ## 0.0129313914 -0.0069613696 0.4388475839 0.0409523768 0.5183893890 ## psi2 ## 0.0508406536 logLik(mod) ## &#39;log Lik.&#39; -333718.6 (df=37) AIC(mod) ## [1] 441666.5 5.5.3 Prediction Similar to the other packages in the tsmodels framework, prediction builds a distribution of possible paths by simulation, outputting an object of class tsmodel.predict: p &lt;- predict(mod, h = 24*10) plot(p, n_original = 24*20, main = &quot;10-Day Hourly Forecast&quot;) The predict object also has a decomposition method: td &lt;- tsdecompose(p) par(mfrow = c(3,2),mar = c(3,3,3,3)) plot(td$Level, n_original = 24*20, main = &quot;Level[State] Predicted Distribution&quot;) plot(td$Seasonal24, n_original = 24*20, main = &quot;Seasonal24[State] Predicted Distribution&quot;) plot(td$Seasonal168, n_original = 24*20, main = &quot;Seasonal168[State] Predicted Distribution&quot;) plot(td$AR2, n_original = 24*20, main = &quot;AR2[State] Predicted Distribution&quot;) plot(td$MA2, n_original = 24*20, main = &quot;MA2[State] Predicted Distribution&quot;) 5.5.4 Simulation Simulation of an estimated object has options for changing the coefficients as well as the initial states, as well as the option for providing custom innovations or bootstrapped innovations: args(tsissm:::simulate.tsissm.estimate) ## function (object, nsim = 1, seed = NULL, h = NULL, newxreg = NULL, ## sim_dates = NULL, bootstrap = FALSE, innov = NULL, sigma_scale = 1, ## pars = coef(object), init_states = object$spec$xseed, ...) ## NULL sim &lt;- simulate(mod, nsim = 100, h = 24 * 10, bootstrap = TRUE) plot(sim) While the plot function on a simulated object provides a decomposition of the actual and state components distributions, it is useful to remind ourselves that the distribution bands represent the range of uncertainty of multiple paths. This is best illustrated by plotting these separately: matplot(as.POSIXct(colnames(sim$Simulated)), t(sim$Simulated), type = &quot;l&quot;, col = 1:100, ylab = &quot;&quot;, xlab = &quot;&quot;, main = &quot;Simulated Paths&quot;) grid() 5.5.5 Filtering Online filtering is when new data arrives and instead of re-estimating the model, we instead just filter the new data based on an existing model. In the tsissm package the tsfilter method updates an object of class tsmodel.estimate with new data as the example below illustrates. Because the class of the model is retained and only updated (both data and states) with new information, it is also possible to apply any method to that which admits that object (e.g. predict). library(tsissm) suppressMessages(library(xts)) weekends &lt;- xts(matrix(0, ncol = 1, nrow = nrow(electricload)), index(electricload)) weekends[which(weekdays(as.Date(index(weekends))) %in% c(&quot;Saturday&quot;,&quot;Sunday&quot;))] &lt;- 1 colnames(weekends) &lt;- &quot;weekend&quot; spec &lt;- issm_modelspec(electricload[1:22000], slope = FALSE, xreg = weekends[1:22000], seasonal = TRUE, seasonal_frequency = c(24, 24*7, 24*7*52), seasonal_type = &quot;trigonometric&quot;, seasonal_harmonics = c(4, 4, 2), ar = 2, ma = 2, lambda = NA) mod &lt;- estimate(spec, solver = &quot;solnp&quot;) f1 &lt;- tsfilter(mod, y = electricload[22001:22100], newxreg = weekends[22001:22100]) tail(fitted(mod)) ## Warning: timezone of object (UTC) is different than current timezone (). ## [,1] ## 2019-04-21 10:00:00 5634.844 ## 2019-04-21 11:00:00 5526.292 ## 2019-04-21 12:00:00 5034.468 ## 2019-04-21 13:00:00 4890.952 ## 2019-04-21 14:00:00 4854.412 ## 2019-04-21 15:00:00 5099.741 head(tail(fitted(f1), 100),10) ## Warning: timezone of object (UTC) is different than current timezone (). ## [,1] ## 2019-04-21 16:00:00 5430.979 ## 2019-04-21 17:00:00 5754.287 ## 2019-04-21 18:00:00 6146.832 ## 2019-04-21 19:00:00 5907.693 ## 2019-04-21 20:00:00 5332.229 ## 2019-04-21 21:00:00 4920.201 ## 2019-04-21 22:00:00 4492.700 ## 2019-04-21 23:00:00 4218.722 ## 2019-04-22 00:00:00 4241.625 ## 2019-04-22 01:00:00 4150.824 plot(f1) f2 &lt;- tsfilter(f1, y = electricload[22100:22200], newxreg = weekends[22100:22200]) tail(fitted(f1)) ## Warning: timezone of object (UTC) is different than current timezone (). ## [,1] ## 2019-04-25 14:00:00 5752.368 ## 2019-04-25 15:00:00 5831.491 ## 2019-04-25 16:00:00 6012.804 ## 2019-04-25 17:00:00 6051.963 ## 2019-04-25 18:00:00 6102.939 ## 2019-04-25 19:00:00 5724.208 head(tail(fitted(f2), 100),10) ## Warning: timezone of object (UTC) is different than current timezone (). ## [,1] ## 2019-04-25 20:00:00 5179.796 ## 2019-04-25 21:00:00 4822.841 ## 2019-04-25 22:00:00 4395.456 ## 2019-04-25 23:00:00 4184.391 ## 2019-04-26 00:00:00 4149.133 ## 2019-04-26 01:00:00 4094.189 ## 2019-04-26 02:00:00 4022.150 ## 2019-04-26 03:00:00 4235.648 ## 2019-04-26 04:00:00 4361.716 ## 2019-04-26 05:00:00 4646.646 5.5.6 Profiling The tsprofile function profiles an estimated model by simulating and then estimating multiple paths from the assumed DGP while leaving h values out for prediction evaluation. Each simulated path is equal to the size of the original dataset plus h additional values, and initialized with the initial state vector from the model. The resulting output contains the distribution of the MAPE, percent bias (BIAS) and mean squared log relative error (MSLRE) per horizon h. Since these matrices are of class tsmodel.distribution they can be readily plotted with the special purpose plot function for this class from the tsmethods package. Additionally, a data.table matrix is also return with the distribution of the coefficients from each path estimation. As of version , the tsprofile method on a tsissm.estimate object does not support newxreg. spec &lt;- issm_modelspec(electricload[1:22000], slope = FALSE, seasonal = TRUE, seasonal_frequency = c(24, 24*7, 24*7*52), seasonal_type = &quot;trigonometric&quot;, seasonal_harmonics = c(4, 4, 2), ar = 2, ma = 2, lambda = 0) mod &lt;- estimate(spec, solver = &quot;solnp&quot;) prof &lt;- tsprofile(mod, h = 24*7, nsim = 100, cores = 4, solver = &quot;solnp&quot;) plot(prof) plot(prof, type = &quot;coef&quot;) 5.5.7 Backtesting The tsbacktest method generates an expanding window walk forward backtest, returning a list with the estimation/horizon predictions against actuals as well as a table of average performance metrics by horizon. spec &lt;- issm_modelspec(electricload, slope = FALSE, seasonal = TRUE, seasonal_frequency = c(24, 24*7, 24*7*52), seasonal_type = &quot;trigonometric&quot;, seasonal_harmonics = c(4, 4, 2), ar = 2, ma = 2, lambda = NA) b &lt;- tsbacktest(spec, start = 22150, h = 24, alpha = c(0.02, 0.1), cores = 5, data_name = &quot;electricity&quot;, solver = &quot;solnp&quot;, trace = FALSE) head(b$metrics) ## horizon variable MAPE MSLRE BIAS n MIS[0.02] MIS[0.1] ## 1: 1 electricity 0.03453686 0.002008403 0.01064633 73 1509.709 818.5592 ## 2: 2 electricity 0.04542751 0.003453705 0.01379530 72 2797.289 1293.8717 ## 3: 3 electricity 0.05722549 0.005171447 0.01764414 71 3740.733 1736.8172 ## 4: 4 electricity 0.06419392 0.006124358 0.01896112 70 4443.514 1930.2262 ## 5: 5 electricity 0.06987296 0.007363092 0.02024584 69 5247.636 2305.3943 ## 6: 6 electricity 0.07558653 0.008350822 0.02229894 68 5962.675 2528.8641 plot(b$metrics$horizon, b$metrics$MAPE*100, main = &quot;Horizon MAPE&quot;, type = &quot;l&quot;, ylab = &quot;MAPE[%]&quot;, xlab = &quot;horizon&quot;) grid() The variable n in the table reports the number of h-step ahead predictions made on which the average metrics were calculated. 5.5.8 Benchmarking The tsbenchmark* method is used to benchmark a model for timing and accuracy and can be used as a unit testing function. spec &lt;- issm_modelspec(electricload, slope = FALSE, seasonal = TRUE, seasonal_frequency = c(24, 24*7, 24*7*52), seasonal_type = &quot;trigonometric&quot;, seasonal_harmonics = c(4, 4, 2), ar = 2, ma = 2, lambda = 0) bench &lt;- rbind(tsbenchmark(spec, solver = &quot;optim&quot;), tsbenchmark(spec, solver = &quot;nlminb&quot;), tsbenchmark(spec, solver = &quot;solnp&quot;)) print(bench) ## start end spec ## 1: 2020-09-10 13:31:13 2020-09-10 13:35:33 &lt;tsissm.spec[10]&gt; ## 2: 2020-09-10 13:35:33 2020-09-10 13:35:58 &lt;tsissm.spec[10]&gt; ## 3: 2020-09-10 13:35:58 2020-09-10 13:36:24 &lt;tsissm.spec[10]&gt; ## estimate solver control loglik ## 1: &lt;tsissm.estimate[4]&gt; optim 0 NaN ## 2: &lt;tsissm.estimate[4]&gt; nlminb 0 -342668.9 ## 3: &lt;tsissm.estimate[4]&gt; solnp 0 -333718.6 In the current example, the optim solver did not converge, whilst the solnp solver achieved the highest log likelihood in about 14 seconds. Because of the constraint requirements for both the \\(\\bf{D}\\) matrix as well as the ARMA roots, the problem, while convex, is non-smooth as currently constructed and hence care should be taken in checking the solution. References "],["tsvets.html", "Chapter 6 tsvets package 6.1 Introduction 6.2 Inclusion of External Regressors 6.3 Log-Likelihood 6.4 Dependence Structure 6.5 Grouping and Pooling 6.6 Homogeneous Coefficients and Aggregation 6.7 Demonstration", " Chapter 6 tsvets package 6.1 Introduction The vector exponential additive smoothing model (Vector ETS), introduced in De Silva, Hyndman, and Snyder (2010) naturally generalizes the univariate framework with a great deal of flexibility with the dynamics of the unobserved components. The dynamics of the states can be common, diagonal, grouped or fully parameterized, allowing for a rich set of patterns to be captured. The tsvets package implements a more general and flexible version based on Athanasopoulos and Silva (2012), with methods for estimation, inference, visualization, forecasting as well as aggregation. Let the vector of \\(N\\) series at time \\(t\\), \\(\\bf{y}_t\\) be represented as a linear additive combination of their unobserved level, slope and seasonal (with frequency \\(m\\)) components, \\(\\bf{l}_{t-1}\\), \\(\\bf{b}_{t-1}\\) and \\(\\bf{s}_{t-m}\\) respectively. Formally, the conditional mean of \\(\\bf{\\hat y}_t\\) is given by: \\[\\begin{equation} \\bf{\\hat y}_t = \\bf{l}_{t - 1} + \\Phi\\bf{b}_{t - 1} + \\bf{s}_{t - m}, \\tag{6.1} \\end{equation}\\] where \\(\\Phi\\) is the \\(N\\times N\\) matrix of dampening parameters. In its reduced form, the model without seasonality is equivalent to a VARIMA(1,2,2) model,15 and \\(\\Phi\\) becomes the matrix of first order autoregressive coefficients. The 1 step ahead forecast errors, \\(\\boldsymbol{\\varepsilon}_t\\) follow a multivariate normal distribution: \\[\\begin{equation} {\\boldsymbol{\\varepsilon} _t} = {\\bf{y}_t} - {{\\bf{\\hat y}}_t},\\quad {\\boldsymbol{\\varepsilon} _t} \\sim {\\bf{N}}\\left( {{\\bf{0}},\\Sigma } \\right). \\tag{6.2} \\end{equation}\\] The state equations have the following dynamics: \\[\\begin{equation} \\begin{array}{l} {\\bf{l}_t} = {\\bf{l}_{t - 1}} + \\Phi {\\bf{b}_{t - 1}} + {\\bf{A}}{\\boldsymbol{\\varepsilon}_t},\\\\ {\\bf{b}_t} = \\Phi {\\bf{b}_{t - 1}} + {\\bf{B}}{\\boldsymbol{\\varepsilon}_t},\\\\ {\\bf{s}_t} = {\\bf{s}_{t - m}} + {G_1}K{\\boldsymbol{\\varepsilon}_t},\\\\ {\\bf{s}_{t - m}} = {\\bf{s}_{t - i}} + {G_2}K{\\boldsymbol{\\varepsilon}_t}, \\end{array} \\tag{6.3} \\end{equation}\\] where the matrices \\(\\mathbf{A}\\), \\(\\mathbf{B}\\) and \\(\\mathbf{K}\\) represent the adjustment of the vector components to the errors, and can be diagonal, fully parameterized or scalar (common adjustment). In vector innovations state space form, the system can be written as follows: \\[\\begin{equation} \\begin{array}{l} {\\bf{y}_t} = H{\\bf{x}_{t - 1}} + {\\boldsymbol{\\varepsilon}_t},\\\\ {\\bf{x}_t} = F{\\bf{x}_{t - 1}} + GA{\\boldsymbol{\\varepsilon}_t} \\end{array} \\tag{6.4} \\end{equation}\\] where the matrices \\(H\\), \\(F\\), \\(G\\) and \\(A\\) are composed as follows: \\[\\begin{equation} \\mathop H\\limits_{\\left( {m + 2} \\right)N \\times N} = \\left[ {\\begin{array}{*{20}{c}} {{I_N}}\\\\ {{I_N}}\\\\ {{0_{N \\times N}}}\\\\ \\vdots \\\\ {{0_{N \\times N}}}\\\\ {{I_N}} \\end{array}} \\right],\\quad \\mathop A\\limits_{\\left( {m + 2} \\right)N \\times N} = \\left[ {\\begin{array}{*{20}{c}} {\\bf{A}}\\\\ {\\bf{B}}\\\\ {\\bf{K}}\\\\ \\vdots \\\\ \\vdots \\\\ {\\bf{K}} \\end{array}} \\right], \\tag{6.5} \\end{equation}\\] \\[\\begin{equation} \\mathop F\\limits_{\\left( {m + 2} \\right)N \\times \\left( {m + 2} \\right)N} = \\left[ {\\begin{array}{*{20}{c}} {{I_N}}&amp;{{I_N}}&amp;{{0_{N \\times mN}}}\\\\ {{0_{N \\times N}}}&amp;{{\\Phi _N}}&amp;{{0_{N \\times mN}}}\\\\ {{0_{mN \\times N}}}&amp;{{0_{mN \\times N}}}&amp;{\\tilde{F} \\otimes {I_N}} \\end{array}} \\right],\\mathop {\\tilde{F}}\\limits_{m \\times m} = \\left[ {\\begin{array}{*{20}{c}} 0&amp;0&amp;0&amp; \\cdots &amp;0&amp;1\\\\ 1&amp;0&amp;0&amp; \\cdots &amp;0&amp;0\\\\ 0&amp;1&amp;0&amp; \\cdots &amp;0&amp;0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0&amp;0&amp;0&amp; \\cdots &amp;1&amp;0 \\end{array}} \\right], \\tag{6.6} \\end{equation}\\] \\[\\begin{equation} \\mathop G\\limits_{\\left( {m + 2} \\right)N \\times \\left( {m + 2} \\right)N} = \\left[ {\\begin{array}{*{20}{c}} {{I_N}}&amp;{{0_{N \\times N}}}&amp;{{0_{N \\times mN}}}\\\\ {{0_{N \\times N}}}&amp;{{I_N}}&amp;{{0_{N \\times mN}}}\\\\ {{0_{mN \\times N}}}&amp;{{0_{mN \\times N}}}&amp;{\\tilde{G} \\otimes {I_N}} \\end{array}} \\right], \\tag{6.7} \\end{equation}\\] \\[\\begin{equation} \\mathop {\\tilde{G}}\\limits_{m \\times m} = \\left[ {\\begin{array}{*{20}{c}} {\\frac{{m - 1}}{m}}&amp;0&amp; \\cdots &amp;0&amp;0\\\\ 0&amp;{ - \\frac{1}{m}}&amp; \\cdots &amp;0&amp;0\\\\ 0&amp;0&amp;{ - \\frac{1}{m}}&amp;0&amp;0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0&amp;0&amp;0&amp; \\cdots &amp;{ - \\frac{1}{m}} \\end{array}} \\right], \\tag{6.8} \\end{equation}\\] \\[\\begin{equation} \\mathop {{\\bf{x}_t}}\\limits_{\\left( {m + 2} \\right)N \\times N} = \\left[ {\\begin{array}{*{20}{c}} {{\\bf{l}_t}}\\\\ {{\\bf{b}_t}}\\\\ {{\\bf{s}_t}}\\\\ \\vdots \\\\ {{\\bf{s}_{t - m + 2}}}\\\\ {{\\bf{s}_{t - m + 1}}} \\end{array}} \\right]. \\tag{6.9} \\end{equation}\\] The values in the seasonal matrix \\(\\tilde{G}\\) represent normalization terms which ensure that the seasonal component adds to zero throughout the updating process without becoming contaminated by the level component. 6.2 Inclusion of External Regressors We augment the model with the ability to include external regressors (\\(z_t\\)) such that: \\[\\begin{equation} \\begin{array}{l} {\\bf{y}_t} = H{\\bf{x}_{t - 1}} + \\mathbf{W}\\bf{z}_t + {\\boldsymbol{\\varepsilon}_t},\\\\ {\\bf{x}_t} = F{\\bf{x}_{t - 1}} + GA{\\boldsymbol{\\varepsilon}_t}, \\end{array} \\tag{6.10} \\end{equation}\\] where the \\(W\\) is the matrix of coefficients which are time invariant. The xreg_include argument in the vets_modelspec function is a design matrix which allows one to define which coefficients should be set to zero, which should be estimated individually as well as which should be pooled (see the documentation for more details). The index on \\(\\bf{z}\\) is \\(t\\) and not \\(t-1\\) and it is left to the user to pre-lag any regressors passed to the function. 6.3 Log-Likelihood The log-likelihood (\\(L\\)) of the model follows from the assumption that the innovations \\(\\boldsymbol{\\varepsilon_t}\\sim N\\left(\\bf{0}, \\Sigma\\right)\\), leading to the following function: \\[\\begin{equation} L\\left(\\boldsymbol{\\theta} \\right) = \\frac{T}{2}\\left( {N\\log \\left( {2\\pi } \\right) + N\\log \\left| \\Sigma \\right|} \\right) - \\frac{1}{2}\\sum\\limits_{t = 1}^T {\\boldsymbol{\\varepsilon_t}{\\Sigma ^{ - 1}}\\boldsymbol{\\varepsilon_t}}, \\tag{6.11} \\end{equation}\\] where \\(\\boldsymbol{\\theta}\\) is the vector of parameters being optimized. We also concentrate out the parameters of the covariance matrix by using its ML estimator \\[\\begin{equation} \\hat \\Sigma = \\frac{1}{T}\\sum\\limits_{t = 1}^T {{\\boldsymbol{\\varepsilon}_t}{{\\boldsymbol{\\varepsilon}&#39;}_t}}. \\tag{6.12} \\end{equation}\\] Therefore, the vector ETS log-likelihood is proportional to \\[\\begin{equation} L\\left(\\boldsymbol{\\theta} \\right) \\propto \\sum\\limits_{t = 1}^T {{{\\boldsymbol{\\varepsilon}&#39;}_t}{{\\hat \\Sigma }^{ - 1}}{\\boldsymbol{\\varepsilon}_t}}, \\tag{6.13} \\end{equation}\\] which requires looping through each \\(t\\) to calculate the quadratic form which is expensive. Instead, we can make use of the following relationship, assuming positive-definite or positive-semi-definite \\(\\hat \\Sigma\\), \\({\\hat\\Sigma ^{-1}} = Q\\Lambda^{-1} Q&#39;\\), where \\(\\Lambda\\) is the diagonal matrix of eigenvalues of \\(\\hat\\Sigma\\). We then have \\({\\varepsilon _t} = Q{\\bf{u}_t}\\) where \\(\\bf{u}_t\\) are the projections onto the eigenvectors \\(Q\\). The negative of the log-likelihood can be represented as: \\[\\begin{equation} L\\left( \\boldsymbol{\\theta} \\right) = \\frac{1}{2}T\\left( {N\\log 2\\pi - \\log \\left| {\\hat \\Sigma } \\right|} \\right) + \\frac{1}{2}{{{\\bf{1&#39;}}}_T}\\left( {{{\\left( {\\varepsilon Q} \\right)}^2}\\frac{1}{\\lambda }} \\right), \\tag{6.14} \\end{equation}\\] where we have used the relation \\(Q^{-1} = Q&#39;\\) due to \\(\\hat{\\Sigma}\\) being symmetric (and thus \\(Q\\) is orthogonal), and \\(1 / \\lambda\\) is a vector containing the reporicals of the eigenvalues. Additionally, we constrain the \\(N+1\\) largest eigenvalues (\\(\\lambda_s\\)) to be less than 1 to ensure invertibility of the system, such that \\[\\begin{equation} {\\lambda _s}\\left( D \\right) &lt; 1,D = F - GAH. \\tag{6.15} \\end{equation}\\] This is added as a soft barrier constraint. The diagonal elements of the level, slope, dampening and seasonal matrices are bounded between 0 and 1, while the off diagonal elements are allowed to vary between -1 and 1.16 Finally, the initial seed values for each of the states are approximated using the heuristic approach from the univariate ETS model as described in Section 4.5.2. 6.4 Dependence Structure While Equation (5.10) assumes a full covariance matrix, allowing contemporaneous associations among the residuals, we also offer 3 additional estimators for the dependence structure: diagonal covariance, equicorrelation and shrinkage covariance based on the estimator of Ledoit and Wolf (2004). 6.4.1 Diagonal Covariance The diagonal covariance matrix is the one used by Athanasopoulos and Silva (2012), and leads to the fastest estimation. In this case, the log-likelihood is greatly simplified and equal to \\[\\begin{equation} \\frac{T}{2}\\left( {N\\log \\left( {2\\pi } \\right) + N\\log \\left| \\Sigma \\right|} \\right) - \\frac{1}{2}\\sum\\limits_{t = 1}^T {\\sum\\limits_{i = 1}^N {\\varepsilon _{it}^2/\\sigma _i^2} }. \\tag{6.16} \\end{equation}\\] 6.4.2 Equicorrelation The equicorrelation covariance assumes that the correlation across all series is set to some common value \\(\\rho\\). The correlation matrix \\(\\bf{R}\\) can be calculated as \\[\\begin{equation} {\\bf{R}} = \\rho {\\bf{11&#39;}} + \\left( {1 - \\rho } \\right){\\bf{I}}, \\tag{6.17} \\end{equation}\\] which is guaranteed to be positive definite as long as \\(-\\frac{1}{{N - 1}}&lt;\\rho&lt; 1\\). The covariance is then equal to: \\[\\begin{equation} {\\Sigma} = {\\bf{DRD&#39;}}, \\tag{6.18} \\end{equation}\\] where \\({\\bf{D}} = diag\\left( {{\\hat \\sigma _1},\\dots,{\\hat \\sigma _n}} \\right)\\). Some of the advantages of assuming equicorrelation are discussed in Clements, Scott, and Silvennoinen (2015). 6.4.3 Shrinkage Covariance The shrinkage estimator of Ledoit and Wolf (2004) follows from the observation that the eigenvalues of the estimated correlations tend to be more dispersed than the eigenvalues of the true data generating process. The shrinkage estimator of the covariance is based on a convex combination of the sample covariance \\(\\hat\\Sigma\\) and a target covariance set to a multiple of the identity matrix. It is this combination weight \\(\\rho\\) which we estimate in the case of the shrinkage estimator \\[\\begin{equation} {\\Sigma} = \\left( {1 - \\rho } \\right)\\Sigma + \\frac{\\rho }{n}tr\\left( \\Sigma \\right){\\bf{I}}. \\tag{6.18} \\end{equation}\\] 6.5 Grouping and Pooling The tsvets package allows both global pooling of state component coefficients as well as group-wise pooling. For instance, if we had a large-dimensional system composed of series which have some common grouping structure (e.g. geographical, feature or statistical based), we could impose that these groups have common dynamics for some or all of the components. We provide an example of this in the demonstration section. 6.6 Homogeneous Coefficients and Aggregation When a model is estimated with all components pooled (i.e. common coefficients), then we can aggregate the model to obtain an aggregated representation in closed form, following Section 17.1.2 of R. Hyndman et al. (2008). When this is not the case, we can still obtain an aggregated series from the estimation and prediction objects. This functionality is implemented via the tsaggregate method and we provide an example in the demonstration section. 6.7 Demonstration 6.7.1 Specification The specification function defines the entry point for setting up a vets model: suppressWarnings(suppressPackageStartupMessages(library(tsvets))) args(vets_modelspec) ## function (y, level = c(&quot;constant&quot;, &quot;diagonal&quot;, &quot;common&quot;, &quot;full&quot;, ## &quot;grouped&quot;), slope = c(&quot;none&quot;, &quot;constant&quot;, &quot;common&quot;, &quot;diagonal&quot;, ## &quot;full&quot;, &quot;grouped&quot;), damped = c(&quot;none&quot;, &quot;common&quot;, &quot;diagonal&quot;, ## &quot;full&quot;, &quot;grouped&quot;), seasonal = c(&quot;none&quot;, &quot;common&quot;, &quot;diagonal&quot;, ## &quot;full&quot;, &quot;grouped&quot;), group = NULL, xreg = NULL, xreg_include = NULL, ## frequency = 1, lambda = NULL, lambda_lower = 0, lambda_upper = 1.5, ## dependence = c(&quot;diagonal&quot;, &quot;full&quot;, &quot;equicorrelation&quot;, &quot;shrinkage&quot;), ## cores = 1) ## NULL The specification has options for how the components of level, slope, dampening, seasonality are structured as well as the type of dependence to use. We also allow multicore processing since we require the initial state vectors to be calculated through calls to the tsets package, which can be done in parallel. The lambda argument can be set either to NA, in which case the multivariate version of the Box Cox transformation is used which targets a transformation to multivariate normality based on Velilla (1993) using the powerTransform function from the car package of Fox, Weisberg, and Price (2020), a vector of length equal to the number of series of a single number to apply to all series, and NULL in which case no transformation is performed. The next sections provide fully worked examples with methods showcasing the functionality of the package under different assumptions. 6.7.2 Example: Australian Retail Sales We use a subset of the Australian retail dataset from package tsdatasets representing the monthly retail turnover in $Million AUD across different regions for the news vendor category, with common level, constant slope and diagonal seasonal and dependence structure. suppressWarnings(suppressPackageStartupMessages(library(tsvets))) suppressMessages(library(tsmethods)) suppressMessages(library(tsaux)) suppressMessages(library(xts)) data(&quot;austretail&quot;, package = &quot;tsdatasets&quot;) y &lt;- na.omit(austretail[,grepl(&quot;NEWS&quot;, colnames(austretail))]) spec &lt;- vets_modelspec(y, level = &quot;common&quot;, slope = &quot;constant&quot;, damped = &quot;none&quot;, seasonal = &quot;diagonal&quot;, lambda = NA, dependence = &quot;diagonal&quot;, frequency = 12, cores = 3) mod &lt;- estimate(spec, solver = &quot;solnp&quot;, control = list(trace = 0)) The joint estimation of the 8 series takes about 17.25301 seconds. The summary object prints the full matrices for each component using the Matrix package of Bates and Maechler (2019). The summary method also take an optional weights argument which is used to calculate the weighted Accuracy Criteria, and when this is NULL, an equal weight vector is used instead (and hence equivalent to the Mean Criteria). Similar to other packages, there is a diagnostics method tsdiagnose which prints the eiganvalues of the \\(D\\) matrix as well as the output from a multivariate Normality Test and Multivariate Outliers based on the mvn function of the MVN package of Korkmaz, Goksuluk, and Zararsiz (2019). summary(mod) ## ## Vector ETS ## ----------------------------------- ## Level : common ## Slope : constant ## Seasonal : diagonal ## Dependence : diagonal ## No. Series : 8 ## No. TimePoints : 369 ## ## Parameter Matrices ## ## Level Matrix ## 8 x 8 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 0.7228698 . . . . . . ## [2,] . 0.7228698 . . . . . ## [3,] . . 0.7228698 . . . . ## [4,] . . . 0.7228698 . . . ## [5,] . . . . 0.7228698 . . ## [6,] . . . . . 0.7228698 . ## [7,] . . . . . . 0.7228698 ## [8,] . . . . . . . ## [,8] ## [1,] . ## [2,] . ## [3,] . ## [4,] . ## [5,] . ## [6,] . ## [7,] . ## [8,] 0.7228698 ## ## Slope Matrix ## 8 x 8 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 0 . . . . . . . ## [2,] . 0 . . . . . . ## [3,] . . 0 . . . . . ## [4,] . . . 0 . . . . ## [5,] . . . . 0 . . . ## [6,] . . . . . 0 . . ## [7,] . . . . . . 0 . ## [8,] . . . . . . . 0 ## ## Seasonal Matrix ## 8 x 8 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.2856528 . . . . . ## [2,] . 2.394974e-09 . . . . ## [3,] . . 3.846852e-09 . . . ## [4,] . . . 5.733621e-09 . . ## [5,] . . . . 3.585116e-09 . ## [6,] . . . . . 6.533005e-09 ## [7,] . . . . . . ## [8,] . . . . . . ## [,7] [,8] ## [1,] . . ## [2,] . . ## [3,] . . ## [4,] . . ## [5,] . . ## [6,] . . ## [7,] 0.1511026 . ## [8,] . 2.545877e-09 ## ## Correlation Matrix ## 8 x 8 Matrix of class &quot;dsyMatrix&quot; ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS SA.NEWS T.NEWS ## ACF.NEWS 1.00000000 0.08053171 0.03288053 0.17037946 0.05281494 0.094956667 ## NSW.NEWS 0.08053171 1.00000000 0.20606452 0.15796191 0.11770804 0.172207588 ## NT.NEWS 0.03288053 0.20606452 1.00000000 0.11509632 0.09843775 0.127150095 ## Q.NEWS 0.17037946 0.15796191 0.11509632 1.00000000 0.13848200 0.059622612 ## SA.NEWS 0.05281494 0.11770804 0.09843775 0.13848200 1.00000000 0.058926740 ## T.NEWS 0.09495667 0.17220759 0.12715009 0.05962261 0.05892674 1.000000000 ## V.NEWS 0.20519203 0.23060264 0.11493382 0.10415261 0.14049607 0.072776172 ## WA.NEWS -0.02841268 0.20572327 0.19391119 0.17141803 0.12027857 -0.000681685 ## V.NEWS WA.NEWS ## ACF.NEWS 0.20519203 -0.028412685 ## NSW.NEWS 0.23060264 0.205723265 ## NT.NEWS 0.11493382 0.193911189 ## Q.NEWS 0.10415261 0.171418031 ## SA.NEWS 0.14049607 0.120278569 ## T.NEWS 0.07277617 -0.000681685 ## V.NEWS 1.00000000 0.031399075 ## WA.NEWS 0.03139907 1.000000000 ## ## Information Criteria ## AIC BIC AICc ## 1278.63 2003.45 1289.06 ## ## Accuracy Criteria ## Mean Weighted ## MAPE 0.0618 0.0618 ## MSLRE 0.0068 0.0068 tsdiagnose(mod) ## Real Eigenvalues (D): 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0.866 0.866 0.866 0.866 0.866 0.866 0.866 0.866 0.866 0.866 0.866 0.866 0.5 0.5 0.5 0.5 0 0 0.5 0.5 0 0 0.5 0.5 0.5 0.5 0.5 0.5 0.866 0.866 1 0.866 0.866 1 0 0 0 0 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.866 0.866 1 0.866 0.866 1 0 0 0 0 0.5 0.5 0.866 0.866 1 0.5 0.5 0.866 0.866 1 0.867 0.867 0.869 0.869 0.505 0.505 0.01 0.01 0.511 0.511 0.484 0.484 0.846 0.846 0.978 0.023 0.023 0.465 0.465 0.822 0.822 0.952 0.277 0.277 0.277 0.277 0.277 0.277 0.163 0.021 ## ## Multivariate Normality Tests ## Test E df p value MVN ## 1 Doornik-Hansen 216.8132 16 3.099293e-37 NO ## ## Univariate Normality Tests ## Test Variable Statistic p value Normality ## 1 Shapiro-Francia ACF.NEWS 0.9850 0.0011 NO ## 2 Shapiro-Francia NSW.NEWS 0.9943 0.1602 YES ## 3 Shapiro-Francia NT.NEWS 0.9720 &lt;0.001 NO ## 4 Shapiro-Francia Q.NEWS 0.9860 0.0018 NO ## 5 Shapiro-Francia SA.NEWS 0.9845 9e-04 NO ## 6 Shapiro-Francia T.NEWS 0.9867 0.0026 NO ## 7 Shapiro-Francia V.NEWS 0.9688 &lt;0.001 NO ## 8 Shapiro-Francia WA.NEWS 0.9738 &lt;0.001 NO ## ## Multivariate Outliers (Mahalanobis Distance) ## Observation Mahalanobis Distance Outlier ## 21 1989-12-31 25.080 TRUE ## 24 1990-03-31 24.942 TRUE ## 33 1990-12-31 19.529 TRUE ## 54 1992-09-30 23.035 TRUE ## 63 1993-06-30 21.246 TRUE ## 66 1993-09-30 20.317 TRUE ## 69 1993-12-31 58.753 TRUE ## 72 1994-03-31 31.115 TRUE ## 105 1996-12-31 47.117 TRUE ## 106 1997-01-31 24.857 TRUE ## 121 1998-04-30 37.653 TRUE ## 124 1998-07-31 19.277 TRUE ## 142 2000-01-31 40.444 TRUE ## 148 2000-07-31 19.625 TRUE ## 157 2001-04-30 26.903 TRUE ## 166 2002-01-31 26.156 TRUE ## 172 2002-07-31 46.330 TRUE ## 184 2003-07-31 28.908 TRUE ## 226 2007-01-31 23.302 TRUE ## 235 2007-10-31 79.396 TRUE ## 238 2008-01-31 22.707 TRUE ## 241 2008-04-30 31.688 TRUE ## 251 2009-02-28 22.112 TRUE ## 256 2009-07-31 23.650 TRUE ## 259 2009-10-31 19.600 TRUE ## 261 2009-12-31 20.838 TRUE ## 262 2010-01-31 67.995 TRUE ## 263 2010-02-28 48.986 TRUE ## 265 2010-04-30 30.044 TRUE ## 268 2010-07-31 22.728 TRUE ## 274 2011-01-31 90.180 TRUE ## 285 2011-12-31 22.557 TRUE ## 286 2012-01-31 49.671 TRUE ## 287 2012-02-29 19.882 TRUE ## 295 2012-10-31 29.557 TRUE ## 301 2013-04-30 51.556 TRUE ## 302 2013-05-31 19.664 TRUE ## 307 2013-10-31 19.951 TRUE ## 309 2013-12-31 28.466 TRUE ## 310 2014-01-31 38.202 TRUE Additional methods are similar to what is available in other packages. Coefficients: coef(mod) ## Level[Common] Seasonal[ACF.NEWS] Seasonal[NSW.NEWS] Seasonal[NT.NEWS] ## 7.228698e-01 2.856528e-01 2.394974e-09 3.846852e-09 ## Seasonal[Q.NEWS] Seasonal[SA.NEWS] Seasonal[T.NEWS] Seasonal[V.NEWS] ## 5.733621e-09 3.585116e-09 6.533005e-09 1.511026e-01 ## Seasonal[WA.NEWS] ## 2.545877e-09 Loglikelihood and AIC: logLik(mod) ## &#39;log Lik.&#39; 1036.628 (df=121) AIC(mod) ## [1] 1278.628 Performance metrics with optional weights option: tsmetrics(mod, weights = runif(8)) ## N no_pars LogLik AIC BIC AICc MAPE MSLRE ## 1 2952 121 1036.628 1278.628 2003.447 1289.061 0.06177398 0.00682494 ## WAPE WSLRE ## 1 0.0626271 0.00706332 tscor(mod) ## 8 x 8 Matrix of class &quot;dsyMatrix&quot; ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS SA.NEWS T.NEWS ## ACF.NEWS 1.00000000 0.08053171 0.03288053 0.17037946 0.05281494 0.094956667 ## NSW.NEWS 0.08053171 1.00000000 0.20606452 0.15796191 0.11770804 0.172207588 ## NT.NEWS 0.03288053 0.20606452 1.00000000 0.11509632 0.09843775 0.127150095 ## Q.NEWS 0.17037946 0.15796191 0.11509632 1.00000000 0.13848200 0.059622612 ## SA.NEWS 0.05281494 0.11770804 0.09843775 0.13848200 1.00000000 0.058926740 ## T.NEWS 0.09495667 0.17220759 0.12715009 0.05962261 0.05892674 1.000000000 ## V.NEWS 0.20519203 0.23060264 0.11493382 0.10415261 0.14049607 0.072776172 ## WA.NEWS -0.02841268 0.20572327 0.19391119 0.17141803 0.12027857 -0.000681685 ## V.NEWS WA.NEWS ## ACF.NEWS 0.20519203 -0.028412685 ## NSW.NEWS 0.23060264 0.205723265 ## NT.NEWS 0.11493382 0.193911189 ## Q.NEWS 0.10415261 0.171418031 ## SA.NEWS 0.14049607 0.120278569 ## T.NEWS 0.07277617 -0.000681685 ## V.NEWS 1.00000000 0.031399075 ## WA.NEWS 0.03139907 1.000000000 Correlation and covariance matrices: tscov(mod) ## 8 x 8 Matrix of class &quot;dsyMatrix&quot; ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS SA.NEWS ## ACF.NEWS 0.0115176893 0.0005204026 0.0006261006 0.0011599528 0.0004533595 ## NSW.NEWS 0.0005204026 0.0036255980 0.0022014849 0.0006033684 0.0005668906 ## NT.NEWS 0.0006261006 0.0022014849 0.0314807680 0.0012954614 0.0013969719 ## Q.NEWS 0.0011599528 0.0006033684 0.0012954614 0.0040242112 0.0007026467 ## SA.NEWS 0.0004533595 0.0005668906 0.0013969719 0.0007026467 0.0063974462 ## T.NEWS 0.0026750756 0.0027218866 0.0059219846 0.0009928401 0.0012372107 ## V.NEWS 0.0016124595 0.0010167162 0.0014931937 0.0004837892 0.0008228357 ## WA.NEWS -0.0012068439 0.0049026331 0.0136170079 0.0043038096 0.0038075686 ## T.NEWS V.NEWS WA.NEWS ## ACF.NEWS 2.675076e-03 0.0016124595 -1.206844e-03 ## NSW.NEWS 2.721887e-03 0.0010167162 4.902633e-03 ## NT.NEWS 5.921985e-03 0.0014931937 1.361701e-02 ## Q.NEWS 9.928401e-04 0.0004837892 4.303810e-03 ## SA.NEWS 1.237211e-03 0.0008228357 3.807569e-03 ## T.NEWS 6.890581e-02 0.0013988231 -7.082194e-05 ## V.NEWS 1.398823e-03 0.0053615617 9.099524e-04 ## WA.NEWS -7.082194e-05 0.0009099524 1.566435e-01 tscor(mod) ## 8 x 8 Matrix of class &quot;dsyMatrix&quot; ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS SA.NEWS T.NEWS ## ACF.NEWS 1.00000000 0.08053171 0.03288053 0.17037946 0.05281494 0.094956667 ## NSW.NEWS 0.08053171 1.00000000 0.20606452 0.15796191 0.11770804 0.172207588 ## NT.NEWS 0.03288053 0.20606452 1.00000000 0.11509632 0.09843775 0.127150095 ## Q.NEWS 0.17037946 0.15796191 0.11509632 1.00000000 0.13848200 0.059622612 ## SA.NEWS 0.05281494 0.11770804 0.09843775 0.13848200 1.00000000 0.058926740 ## T.NEWS 0.09495667 0.17220759 0.12715009 0.05962261 0.05892674 1.000000000 ## V.NEWS 0.20519203 0.23060264 0.11493382 0.10415261 0.14049607 0.072776172 ## WA.NEWS -0.02841268 0.20572327 0.19391119 0.17141803 0.12027857 -0.000681685 ## V.NEWS WA.NEWS ## ACF.NEWS 0.20519203 -0.028412685 ## NSW.NEWS 0.23060264 0.205723265 ## NT.NEWS 0.11493382 0.193911189 ## Q.NEWS 0.10415261 0.171418031 ## SA.NEWS 0.14049607 0.120278569 ## T.NEWS 0.07277617 -0.000681685 ## V.NEWS 1.00000000 0.031399075 ## WA.NEWS 0.03139907 1.000000000 Note that, even though we impose a diagonal covariance matrix structure, we still return the full covariance matrix in tscov. Fitted and residuals: head(fitted(mod)[,1:4], 4) ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS ## 1988-04-30 5.083113 73.64487 1.826402 32.42629 ## 1988-05-31 5.676505 77.54209 1.989449 33.63407 ## 1988-06-30 5.075896 74.71800 2.035401 34.26673 ## 1988-07-31 4.934340 80.55047 2.505502 36.89036 head(residuals(mod)[,1:4], 4) ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS ## 1988-04-30 0.01688730 -0.6448678 -0.02640171 -0.6262864 ## 1988-05-31 -0.07650468 2.7579096 0.01055136 3.0659271 ## 1988-06-30 -0.07589608 0.5819960 0.36459863 0.4332729 ## 1988-07-31 -0.03433964 -6.3504739 -0.10550229 2.2096386 head(residuals(mod, raw = TRUE)[,1:4], 4) ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS ## 1988-04-30 0.003685518 -0.008795015 -0.020380197 -0.01950311 ## 1988-05-31 -0.015177745 0.034948730 0.007813705 0.08723713 ## 1988-06-30 -0.016728761 0.007759055 0.258018253 0.01256486 ## 1988-07-31 -0.007742658 -0.082119842 -0.071413488 0.05817216 where argument raw denotes the non backtransformed residuals (if a Box Cox calculation was used). States decomposition: tsx &lt;- tsdecompose(mod) head(tsx$Level[,1:4], 4) ## Level[ACF.NEWS] Level[NSW.NEWS] Level[NT.NEWS] Level[Q.NEWS] ## 1988-04-30 1.788054 4.364053 0.8567770 3.546527 ## 1988-05-31 1.776977 4.389549 0.8604635 3.610767 ## 1988-06-30 1.764779 4.395390 1.0450153 3.621029 ## 1988-07-31 1.759077 4.336260 0.9914309 3.664259 head(tsx$Slope[,1:4], 4) ## Slope[ACF.NEWS] Slope[NSW.NEWS] Slope[NT.NEWS] Slope[Q.NEWS] ## 1988-04-30 -0.0001052891 0.0002321085 -0.001961778 0.001179101 ## 1988-05-31 -0.0001052891 0.0002321085 -0.001961778 0.001179101 ## 1988-06-30 -0.0001052891 0.0002321085 -0.001961778 0.001179101 ## 1988-07-31 -0.0001052891 0.0002321085 -0.001961778 0.001179101 head(tsx$Seasonal[,1:4], 4) ## Seasonal[ACF.NEWS] Seasonal[NSW.NEWS] Seasonal[NT.NEWS] ## 1988-04-30 0.049802193 -0.013464513 -0.01415806 ## 1988-05-31 -0.063814594 -0.076059814 0.01605518 ## 1988-06-30 -0.083010706 -0.006737797 0.16093509 ## 1988-07-31 -0.003458054 0.020157339 0.13897920 ## Seasonal[Q.NEWS] ## 1988-04-30 -0.032166012 ## 1988-05-31 -0.077770996 ## 1988-06-30 -0.014257373 ## 1988-07-31 0.009366054 Plot methods exist outputting the fitted values, the states and the residuals, with an argument for the series to output, with a maximum number per call of 10 series: plot(mod) plot(mod, type = &quot;states&quot;) plot(mod, type = &quot;residuals&quot;) The predict function will output a list which includes data.table of each series prediction object and state decomposition: p &lt;- predict(mod, h = 12) p$prediction_table ## series Level Slope Seasonal X ## 1: ACF.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 2: NSW.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 3: NT.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 4: Q.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 5: SA.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 6: T.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 7: V.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 8: WA.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## Predicted ## 1: &lt;tsmodel.predict[2]&gt; ## 2: &lt;tsmodel.predict[2]&gt; ## 3: &lt;tsmodel.predict[2]&gt; ## 4: &lt;tsmodel.predict[2]&gt; ## 5: &lt;tsmodel.predict[2]&gt; ## 6: &lt;tsmodel.predict[2]&gt; ## 7: &lt;tsmodel.predict[2]&gt; ## 8: &lt;tsmodel.predict[2]&gt; The plot method on the predicted object can output 1 series at a time using the series argument: plot(p, series = 2) The simulation method also returns a list with a data.table object with each series simulation object and state decomposition. Additionally, there are optional arguments for the initial state to use (init_states) and parameter vector (pars). s &lt;- simulate(mod, h = 12*8, nsim = 100, init_states = mod$spec$vets_env$States[,1]) s$simulation_table ## series Level Slope Seasonal ## 1: ACF.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 2: NSW.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 3: NT.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 4: Q.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 5: SA.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 6: T.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 7: V.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 8: WA.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## Simulated ## 1: &lt;tsmodel.predict[2]&gt; ## 2: &lt;tsmodel.predict[2]&gt; ## 3: &lt;tsmodel.predict[2]&gt; ## 4: &lt;tsmodel.predict[2]&gt; ## 5: &lt;tsmodel.predict[2]&gt; ## 6: &lt;tsmodel.predict[2]&gt; ## 7: &lt;tsmodel.predict[2]&gt; ## 8: &lt;tsmodel.predict[2]&gt; par(mfrow = c(4,1),mar = c(3,3,3,3)) plot(s$simulation_table[series == &quot;ACF.NEWS&quot;]$Simulated[[1]], main = &quot;Simulated Series&quot;) plot(s$simulation_table[series == &quot;ACF.NEWS&quot;]$Level[[1]], main = &quot;Simulated Level&quot;) plot(s$simulation_table[series == &quot;ACF.NEWS&quot;]$Slope[[1]], main = &quot;Simulated Slope&quot;) plot(s$simulation_table[series == &quot;ACF.NEWS&quot;]$Seasonal[[1]], main = &quot;Simulated Seasonal&quot;) The tsbacktest method has similar arguments to other backtest implementations in the tsmodels framework: b &lt;- tsbacktest(spec, h = 12, cores = 5, solver = &quot;solnp&quot;, trace = 0) The returned object is a list of 2 data.tables with the predictions for each series by estimation date and horizon and the summary metrics table: head(b$prediction) ## series estimation_date horizon size forecast_dates forecast actual ## 1: ACF.NEWS 2003-07-31 1 184 2003-08-31 11.19785 11.2 ## 2: ACF.NEWS 2003-07-31 2 184 2003-09-30 10.62864 10.1 ## 3: ACF.NEWS 2003-07-31 3 184 2003-10-31 10.54963 9.7 ## 4: ACF.NEWS 2003-07-31 4 184 2003-11-30 11.16726 9.7 ## 5: ACF.NEWS 2003-07-31 5 184 2003-12-31 15.53520 14.2 ## 6: ACF.NEWS 2003-07-31 6 184 2004-01-31 10.11977 8.6 head(b$metrics) ## series horizon MAPE MSLRE BIAS n ## 1: ACF.NEWS 1 0.08370057 0.01153021 0.01604933 185 ## 2: ACF.NEWS 2 0.09818586 0.01473627 0.02540813 184 ## 3: ACF.NEWS 3 0.10826144 0.01744959 0.03383216 183 ## 4: ACF.NEWS 4 0.11925661 0.02091545 0.04322381 182 ## 5: ACF.NEWS 5 0.11687057 0.02065710 0.05132596 181 ## 6: ACF.NEWS 6 0.12608157 0.02290307 0.05894074 180 6.7.3 Example: Australian Retail Sales Grouped Dynamics In this example, we show how grouping works on 99 series, where grouping is by sector. suppressWarnings(suppressPackageStartupMessages(library(tsvets))) suppressMessages(library(tsmethods)) suppressMessages(library(tsaux)) suppressMessages(library(xts)) suppressMessages(library(data.table)) data(&quot;austretail&quot;, package = &quot;tsdatasets&quot;) y &lt;- austretail[&quot;2000/&quot;] include &lt;- sapply(1:ncol(y), function(i) all(!is.na(y[,i]))) y &lt;- y[,include] groups &lt;- sapply(1:ncol(y), function(i) strsplit(colnames(y[,i]), &quot;\\\\.&quot;)[[1]][2]) groups_index &lt;- sort.int(groups, index.return = TRUE) y &lt;- y[,groups_index$ix] groups &lt;- groups[groups_index$ix] groups &lt;- rle(groups)$lengths groups &lt;- unlist(sapply(1:length(groups), function(i) rep(i, groups[i]))) spec &lt;- vets_modelspec(y[,1:99], level = &quot;grouped&quot;, slope = &quot;grouped&quot;, damped = &quot;none&quot;, group = groups[1:99], seasonal = &quot;grouped&quot;, lambda = 0.5, dependence = &quot;diagonal&quot;, frequency = 12, cores = 3) mod &lt;- estimate(spec, solver = &quot;solnp&quot;, control = list(trace = 0)) The joint estimation of the 99 series takes about 21.2180385 minutes. We avoid the summary method for such a large object, and instead print out the coefficients and the performance metrics: cf &lt;- coef(mod) print(data.table::data.table(Coefficient = names(cf), Value = round(cf,4))) ## Coefficient Value ## 1: Level[Group = 1] 0.4481 ## 2: Level[Group = 2] 0.5108 ## 3: Level[Group = 3] 0.7151 ## 4: Level[Group = 4] 0.6882 ## 5: Level[Group = 5] 0.1667 ## 6: Level[Group = 6] 0.5208 ## 7: Level[Group = 7] 0.5494 ## 8: Level[Group = 8] 0.4183 ## 9: Level[Group = 9] 0.5011 ## 10: Level[Group = 10] 0.5867 ## 11: Level[Group = 11] 0.6780 ## 12: Level[Group = 12] 0.4934 ## 13: Level[Group = 13] 0.7206 ## 14: Slope[Group = 1] 0.0000 ## 15: Slope[Group = 2] 0.0000 ## 16: Slope[Group = 3] 0.0000 ## 17: Slope[Group = 4] 0.0000 ## 18: Slope[Group = 5] 0.0073 ## 19: Slope[Group = 6] 0.0000 ## 20: Slope[Group = 7] 0.0000 ## 21: Slope[Group = 8] 0.0000 ## 22: Slope[Group = 9] 0.0000 ## 23: Slope[Group = 10] 0.0056 ## 24: Slope[Group = 11] 0.0000 ## 25: Slope[Group = 12] 0.0000 ## 26: Slope[Group = 13] 0.0000 ## 27: Seasonal[Group = 1] 0.3418 ## 28: Seasonal[Group = 2] 0.1879 ## 29: Seasonal[Group = 3] 0.0000 ## 30: Seasonal[Group = 4] 0.0000 ## 31: Seasonal[Group = 5] 0.1334 ## 32: Seasonal[Group = 6] 0.2645 ## 33: Seasonal[Group = 7] 0.0000 ## 34: Seasonal[Group = 8] 0.3150 ## 35: Seasonal[Group = 9] 0.3885 ## 36: Seasonal[Group = 10] 0.0245 ## 37: Seasonal[Group = 11] 0.0986 ## 38: Seasonal[Group = 12] 0.2692 ## 39: Seasonal[Group = 13] 0.0000 ## Coefficient Value tsmetrics(mod) ## N no_pars LogLik AIC BIC AICc MAPE MSLRE ## 1 22572 1425 -7608.464 -4758.464 6676.399 -4566.272 0.04053744 0.003140806 ## WAPE WSLRE ## 1 0.04053744 0.003140806 plot(mod, series = (1:99)[groups == 4]) plot(mod, series = (1:99)[groups == 4], type = &quot;states&quot;) plot(mod, series = (1:99)[groups == 4], type = &quot;residuals&quot;) 6.7.4 Example: Australian Retail Sales Filtering This example highlights the use of the tsfilter method for online filtering as already discussed in Sections 4.6.4 and 5.5.5. suppressWarnings(suppressPackageStartupMessages(library(tsvets))) suppressMessages(library(tsmethods)) suppressMessages(library(tsaux)) suppressMessages(library(xts)) data(&quot;austretail&quot;, package = &quot;tsdatasets&quot;) y &lt;- na.omit(austretail[,grepl(&quot;NEWS&quot;, colnames(austretail))]) y_train &lt;- y[1:(369 - 24)] y_test &lt;- y[(369 - 24 + 1):369] spec &lt;- vets_modelspec(y_train, level = &quot;common&quot;, slope = &quot;diagonal&quot;, damped = &quot;none&quot;, seasonal = &quot;common&quot;, lambda = 0.5, dependence = &quot;diagonal&quot;, frequency = 12, cores = 3) mod &lt;- estimate(spec, solver = &quot;solnp&quot;, control = list(trace=0)) filt &lt;- tsfilter(mod, y = y_test) tail(fitted(mod)) ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS SA.NEWS T.NEWS V.NEWS ## 2016-07-31 6.274802 85.48932 1.563079 51.79396 12.263420 12.41307 47.63892 ## 2016-08-31 5.822149 87.91930 1.518185 54.59900 10.952638 12.97214 47.66849 ## 2016-09-30 4.902060 81.27387 1.394669 51.86889 9.940536 13.71905 45.84747 ## 2016-10-31 4.791340 83.27202 1.319689 55.07791 9.960856 12.42058 49.48286 ## 2016-11-30 5.547110 86.93624 1.357323 54.95785 10.368361 12.07245 50.89754 ## 2016-12-31 8.184714 116.27849 1.858933 72.01647 15.622337 14.69098 76.20899 ## WA.NEWS ## 2016-07-31 40.75597 ## 2016-08-31 42.52752 ## 2016-09-30 39.24565 ## 2016-10-31 40.45785 ## 2016-11-30 41.79129 ## 2016-12-31 55.58457 head(tail(fitted(filt), 24 + 1), 5) ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS SA.NEWS T.NEWS V.NEWS ## 2016-12-31 8.184714 116.27849 1.858933 72.01647 15.622337 14.69098 76.20899 ## 2017-01-31 5.136206 92.07694 1.239746 57.96093 8.790409 12.48530 47.68475 ## 2017-02-28 7.150197 94.62242 1.098936 52.06098 9.564016 13.50578 47.71322 ## 2017-03-31 5.671754 98.44473 1.105181 54.21631 11.124735 11.63927 52.55806 ## 2017-04-30 4.718625 82.17765 1.158383 51.48419 8.319084 10.94583 47.45608 ## WA.NEWS ## 2016-12-31 55.58457 ## 2017-01-31 48.38623 ## 2017-02-28 40.52982 ## 2017-03-31 40.46546 ## 2017-04-30 37.86586 6.7.5 Example: Australian Retail Sales Aggregation This demonstrates the ability to aggregate a model estimated with homogeneous coefficients. We continue with the retail sales data, which being a set of flow variables, can be aggregated. suppressWarnings(suppressPackageStartupMessages(library(tsvets))) suppressMessages(library(tsmethods)) suppressMessages(library(tsaux)) suppressMessages(library(xts)) data(&quot;austretail&quot;, package = &quot;tsdatasets&quot;) y &lt;- na.omit(austretail[,grepl(&quot;NEWS&quot;, colnames(austretail))]) We use a vector of ones as weights, to denote summation to the total for the News category. weights &lt;- rep(1, 8) Additionally, we also include, purely for expositional purposes, a matrix of 5 randomly generated regressors: xreg &lt;- xts(matrix(rnorm(nrow(y) * 5), nrow = nrow(y), ncol = 5), index(y)) xreg_include &lt;- matrix(2, ncol = 5, nrow = ncol(y)) xreg_include[2,1] &lt;- 1 Setting the include matrix to the value of 2 means pooling, and we also set regressor 1 for series 2 to a value of 1 in order to denote that we want this to be estimated seperately (non-pooled). spec &lt;- vets_modelspec(y, level = &quot;common&quot;, slope = &quot;common&quot;, seasonal = &quot;common&quot;, dependence = &quot;diagonal&quot;, frequency = 12, cores = 2, xreg = xreg, xreg_include = xreg_include) mod &lt;- estimate(spec, solver = &quot;solnp&quot;, control = list(trace = 0)) We can extract specific matrices from the estimate object using the as yet unexported function p_matrix: tsvets:::p_matrix(mod)$X ## 8 x 5 Matrix of class &quot;dgeMatrix&quot; ## x1 x2 x3 x4 x5 ## ACF.NEWS -0.00591057 -0.007903383 0.01301432 -0.03547232 0.009989615 ## NSW.NEWS -0.02314656 -0.007903383 0.01301432 -0.03547232 0.009989615 ## NT.NEWS -0.00591057 -0.007903383 0.01301432 -0.03547232 0.009989615 ## Q.NEWS -0.00591057 -0.007903383 0.01301432 -0.03547232 0.009989615 ## SA.NEWS -0.00591057 -0.007903383 0.01301432 -0.03547232 0.009989615 ## T.NEWS -0.00591057 -0.007903383 0.01301432 -0.03547232 0.009989615 ## V.NEWS -0.00591057 -0.007903383 0.01301432 -0.03547232 0.009989615 ## WA.NEWS -0.00591057 -0.007903383 0.01301432 -0.03547232 0.009989615 where we can observe that the coefficients for each regressor have been pooled, with the exception of x1 for the NSW.NEWS series. We can now proceed to aggregate the model using the tsaggregate method on the estimated object: mod_aggregate &lt;- tsaggregate(mod, weights = weights, return_model = TRUE) summary(mod_aggregate) ## ## Vector ETS ## ----------------------------------- ## Level : common ## Slope : common ## Seasonal : common ## Dependence : diagonal ## No. Series : 1 ## No. TimePoints : 369 ## ## Parameter Matrices ## ## Level Matrix ## 1 x 1 Matrix of class &quot;dsyMatrix&quot; ## [,1] ## [1,] 0.6228533 ## ## Slope Matrix ## 1 x 1 Matrix of class &quot;dsyMatrix&quot; ## [,1] ## [1,] 5.993693e-09 ## ## Seasonal Matrix ## 1 x 1 Matrix of class &quot;dsyMatrix&quot; ## [,1] ## [1,] 0.1288974 ## ## Regressor Matrix ## 1 x 5 Matrix of class &quot;dgeMatrix&quot; ## x1 x2 x3 x4 x5 ## aggregate -0.06452055 -0.06322707 0.1041146 -0.2837786 0.07991692 ## ## Covariance Matrix ## 1 x 1 Matrix of class &quot;dsyMatrix&quot; ## aggregate ## aggregate 222.4449 ## ## Accuracy Criteria ## Mean Weighted ## MAPE 0.0352 0.0352 ## MSLRE 0.0021 0.0021 plot(mod_aggregate) suppressWarnings(plot(mod_aggregate, type = &quot;states&quot;)) We can then proceed with prediction as well as simulation, both of which allow for the outputs to be aggregated. Alternatively, since we now have an aggregated model object, we can also predict or simulate directly from that object. newxreg &lt;- xts(matrix(rnorm(24 * 5), nrow = 24, ncol = 5), future_dates(tail(index(y),1),&quot;months&quot;,24)) p &lt;- predict(mod, newxreg = newxreg) p_agg_series &lt;- tsaggregate(p, weights = weights) p_agg_model &lt;- predict(mod_aggregate, newxreg = newxreg) plot(p_agg_series$distribution, gradient_color = &quot;whitesmoke&quot;, interval_color = &quot;grey&quot;, main = &quot;Agggregated Model&quot;) plot(p_agg_model$prediction_table[1]$Predicted[[1]]$distribution, gradient_color = &quot;whitesmoke&quot;, add = TRUE, median_color = 2, interval_color = &quot;grey10&quot;) As we can observe, the output of the 2 predictions is identical (subject to some randomness from the simulation of the predictive distribution). We do the same for the simulation method: s &lt;- simulate(mod, nsim = 100, h = 24, newxreg = newxreg, seed = 700) s_agg_series &lt;- tsaggregate(s, weights = weights) s_agg_model &lt;- simulate(mod_aggregate, nsim = 100, h = 24, newxreg = newxreg, seed = 700) plot(s_agg_series$distribution, gradient_color = &quot;whitesmoke&quot;, interval_color = &quot;grey&quot;) plot(s_agg_model$simulation_table[1]$Simulated[[1]]$distribution, gradient_color = &quot;whitesmoke&quot;, add = TRUE, median_color = 2, interval_color = &quot;grey10&quot;) which is again almost identical, but we can achieve exact replication is we use the bootstrap residuals method: s &lt;- simulate(mod, nsim = 100, h = 24, newxreg = newxreg, seed = 700, bootstrap = TRUE) s_agg_series &lt;- tsaggregate(s, weights = weights) s_agg_model &lt;- simulate(mod_aggregate, nsim = 100, h = 24, newxreg = newxreg, seed = 700, bootstrap = TRUE) plot(s_agg_series$distribution, gradient_color = &quot;whitesmoke&quot;, interval_color = &quot;grey&quot;) plot(s_agg_model$simulation_table[1]$Simulated[[1]]$distribution, gradient_color = &quot;whitesmoke&quot;, add = TRUE, median_color = 2, interval_color = &quot;grey10&quot;) 6.7.6 Example: Groupwise Pooling It is also possible to have group wise pooling. We saw above that we could specify pooling using the number of series times number of regressors matrix, xreg_include, with values 0, 1 or 2+ (0 = no beta, 1 = individual beta and 2+ = grouped beta). Group pooling can be achieved by setting values of 2+. For instance 2 series sharing one pooled estimate, and 3 other series sharing another grouped estimate would have values of (2,2,3,3,3). In the following example we have 8 series and 2 regressors, with different combinations of pooling. xreg &lt;- xts(matrix(rnorm(nrow(y)*2), ncol = 2, nrow = nrow(y)), index(y)) colnames(xreg) &lt;- c(&quot;B1&quot;,&quot;B2&quot;) xreg_include &lt;- matrix(1, ncol = 2, nrow = ncol(y)) rownames(xreg_include) &lt;- colnames(y) colnames(xreg_include) &lt;- colnames(xreg) # individual [1,1], grouped 2:3, 5:6 and 7:8 xreg_include[c(2:3),1] &lt;- 2 xreg_include[c(5:6),1] &lt;- 3 xreg_include[c(7:8),1] &lt;- 4 # individual 1, zero for 2:3, group for 4:8 xreg_include[c(2:3),2] &lt;- 0 xreg_include[c(4:8),2] &lt;- 3 xreg_include ## B1 B2 ## ACF.NEWS 1 1 ## NSW.NEWS 2 0 ## NT.NEWS 2 0 ## Q.NEWS 1 3 ## SA.NEWS 3 3 ## T.NEWS 3 3 ## V.NEWS 4 3 ## WA.NEWS 4 3 spec &lt;- vets_modelspec(y, level = &quot;grouped&quot;, slope = &quot;grouped&quot;, group = c(1,1,2,2,3,3,4,4), damped = &quot;none&quot;, seasonal = &quot;grouped&quot;, xreg = xreg, xreg_include = xreg_include, lambda = 0, dependence = &quot;diagonal&quot;, frequency = 12, cores = 1) mod &lt;- estimate(spec, solver = &quot;solnp&quot;, control = list(trace=1)) ## ## Iter: 1 fn: -3271.4734 Pars: 0.5692498822857 0.7581471970313 0.7395568229416 0.7202628217574 0.0000000003531 0.0000000139420 0.0000000166027 0.0000000049353 0.2508654873774 0.0000000133883 0.0000000200549 0.1085075558381 -0.0002694220937 0.0005076727876 -0.0036440545749 -0.0035342324501 -0.0048241742999 -0.0005296077285 -0.0011640802922 ## Iter: 2 fn: -3271.4734 Pars: 5.693e-01 7.582e-01 7.396e-01 7.203e-01 6.347e-11 1.334e-08 1.586e-08 4.612e-09 2.509e-01 1.288e-08 1.942e-08 1.085e-01 -2.693e-04 5.076e-04 -3.644e-03 -3.534e-03 -4.824e-03 -5.291e-04 -1.164e-03 ## solnp--&gt; Completed in 2 iterations Note that the Box Cox parameter lambda should be the same for the grouped series else the coefficients should not be pooled (as they have different scales). 6.7.7 Example: Price Volume Aggregation This final example shows how to jointly model a series based on price-volume relationship with a target of generating a model for Sales: \\(Sales = Price\\times Volume\\) While this is a multiplicative model, if we were to model the series using a log transformation we would be able to aggregate the model in logs before exponentiating to recover the Sales as the example will demonstrate. data(&quot;priceunits&quot;, package = &quot;tsdatasets&quot;) spec &lt;- vets_modelspec(priceunits, level = &quot;common&quot;, slope = &quot;common&quot;, frequency = 12, lambda = rep(0,2), dependence = &quot;full&quot;) mod &lt;- estimate(spec, solver = &quot;optim&quot;, control = list(trace = 0)) summary(mod) ## ## Vector ETS ## ----------------------------------- ## Level : common ## Slope : common ## Seasonal : none ## Dependence : full ## No. Series : 2 ## No. TimePoints : 100 ## ## Parameter Matrices ## ## Level Matrix ## 2 x 2 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] [,2] ## [1,] 0.4012655 . ## [2,] . 0.4012655 ## ## Slope Matrix ## 2 x 2 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] [,2] ## [1,] 0.07645917 . ## [2,] . 0.07645917 ## ## Correlation Matrix ## 2 x 2 Matrix of class &quot;dsyMatrix&quot; ## price units ## price 1.0000000 -0.6009338 ## units -0.6009338 1.0000000 ## ## Information Criteria ## AIC BIC AICc ## 162.76 182.55 163.2 ## ## Accuracy Criteria ## Mean Weighted ## MAPE 0.0508 0.0508 ## MSLRE 0.0042 0.0042 plot(mod) plot(mod, type = &quot;states&quot;) weights &lt;- c(1,1) mod_aggregate &lt;- tsaggregate(mod, weights = weights, return_model = TRUE) summary(mod_aggregate) ## ## Vector ETS ## ----------------------------------- ## Level : common ## Slope : common ## Seasonal : none ## Dependence : diagonal ## No. Series : 1 ## No. TimePoints : 100 ## ## Parameter Matrices ## ## Level Matrix ## 1 x 1 Matrix of class &quot;dsyMatrix&quot; ## [,1] ## [1,] 0.4012655 ## ## Slope Matrix ## 1 x 1 Matrix of class &quot;dsyMatrix&quot; ## [,1] ## [1,] 0.07645917 ## ## Covariance Matrix ## 1 x 1 Matrix of class &quot;dsyMatrix&quot; ## aggregate ## aggregate 0.003891895 ## ## Accuracy Criteria ## Mean Weighted ## MAPE 0.0496 0.0496 ## MSLRE 0.0039 0.0039 p &lt;- predict(mod, h = 24) p_agg_model &lt;- predict(mod_aggregate, h = 24) p_agg_series &lt;- tsaggregate(p, weights = weights) plot(p_agg_model$prediction_table[1]$Predicted[[1]]$distribution, gradient_color = &quot;whitesmoke&quot;, interval_color = &quot;grey&quot;) plot(p_agg_series$distribution, gradient_color = &quot;whitesmoke&quot;, add = TRUE, median_color = 2, interval_color = &quot;grey10&quot;) In the presence of a non-homogeneous model, or a homogeneous model with Box Cox parameter not equal to either 0 (log) or 1 (no transform), then the tsaggregate method will still work but the return_model arguments needs to be set to FALSE and the resulting output will just be the aggregated series. References "],["tsforeign.html", "Chapter 7 tsforeign package 7.1 Introduction 7.2 ARIMA model 7.3 bsts model", " Chapter 7 tsforeign package 7.1 Introduction The tsforeign package provides custom wrappers for the auto.arima function from the forecast package of Hyndman et al. (2020) package and the bsts method from bsts package of Scott (2020), with methods for estimation, some diagnostics and prediction. This package may be extended in future to provide wrappers for other interesting models. For the bsts model, we have also made use of the tsconvert method from the tsmethods package to convert the output of the model to one conforming to the required inputs of the dlm package, for which we provide an example in the demonstration section. Since both of those packages have their own vignettes and documentation, we proceed here directly into a demonstration of the functionality. 7.2 ARIMA model The entry point for the ARIMA model is the arima_modelspec function suppressPackageStartupMessages(library(tsforeign)) library(xts) library(tsaux) args(arima_modelspec) ## function (y, xreg = NULL, frequency = NULL, seasonal = FALSE, ## seasonal_type = &quot;regular&quot;, lambda = NULL, seasonal_harmonics = NULL, ## lambda_lower = 0, lambda_upper = 1.5, ...) ## NULL which allows for both regular and trigonometric seasonality. For the demonstration example, well use the AirPassengers dataset after first converting it into an xts object: air &lt;- AirPassengers dt &lt;- future_dates(as.Date(&quot;1948-12-31&quot;), &quot;months&quot;, length(air)) air &lt;- xts(as.numeric(air), dt) spec &lt;- arima_modelspec(y = air, frequency = 12, seasonal_type = &quot;regular&quot;, seasonal = TRUE, lambda = NA) mod &lt;- estimate(spec) summary(mod) ## ARIMA(0,1,1)(0,1,1)[12] ## ------------------------ ## Estimate Std.Error t value Pr(&gt;|t|) ## ma1 -0.4018 0.08964 -4.482 7.381e-06 ## sma1 -0.5569 0.07310 -7.619 2.554e-14 ## ## sigma : 0.037 ## ## AIC BIC AICc ## -483.31 -474.69 -483.13 ## ## MAPE MASE MSLRE BIAS ## 0.0262 0.2297 0.0012 0 tsmetrics(mod) ## n no.pars LogLik AIC BIC AICc MAPE MASE ## 1 131 2 244.6574 -483.3147 -474.6891 -483.1258 0.02623633 0.2297063 ## MSLRE BIAS ## 1 0.001228422 4.059721e-05 plot(mod) The predict function generates a predictive distribution via simulation and returns an object of class tsmodel.predict: p &lt;- predict(mod, h = 12) plot(p) Additionally, there is also a backtest method: b &lt;- tsbacktest(spec, h = 12, alpha = c(0.02, 0.1), cores = 5, trace = F) b$metrics ## horizon variable MAPE MSLRE BIAS n MIS[0.02] MIS[0.1] ## 1: 1 y 0.02360639 0.0009540715 0.003575389 72 77.33794 58.29969 ## 2: 2 y 0.02841019 0.0013679751 0.006115174 71 78.93216 63.52667 ## 3: 3 y 0.03292009 0.0018302968 0.008294287 70 94.00607 75.56274 ## 4: 4 y 0.03723548 0.0023008820 0.009624260 69 117.06930 86.25187 ## 5: 5 y 0.03849679 0.0024650809 0.011548920 68 121.40149 90.67120 ## 6: 6 y 0.04179470 0.0027917000 0.013225996 67 130.73042 96.69803 ## 7: 7 y 0.04415812 0.0031168561 0.015763263 66 140.20947 101.03404 ## 8: 8 y 0.04552676 0.0032082037 0.018640612 65 143.08042 105.56979 ## 9: 9 y 0.04702747 0.0034102771 0.020720570 64 144.59635 112.34580 ## 10: 10 y 0.04910762 0.0037459150 0.023908958 63 151.21872 118.60027 ## 11: 11 y 0.05027115 0.0039222860 0.026035828 62 164.99230 124.75698 ## 12: 12 y 0.05283781 0.0042096114 0.027740267 61 175.68014 130.57722 7.3 bsts model The bsts package of (Scott 2020) provides functions for fitting Bayesian structural time series models. The entry point in our wrapper is the bsts_modelspec function: args(bsts_modelspec) ## function (y, xreg = NULL, frequency = NULL, differences = 0, ## level = TRUE, slope = TRUE, damped = FALSE, seasonal = FALSE, ## seasonal_frequency = 4, ar = FALSE, ar_max = 1, cycle = FALSE, ## cycle_frequency = NULL, cycle_names = NULL, seasonal_type = &quot;regular&quot;, ## lambda = NULL, lambda_lower = 0, lambda_upper = 1, seasonal_harmonics = NULL, ## distribution = &quot;gaussian&quot;, ...) ## NULL which provides a rich set of options such as degree of differencing,17 a sparse AR component, sparse regressors, regular or trigonometric seasonal components (including multiple seasonal), cyclical components and the Box Cox transformation. Once an object is estimated, and all required components have been generated, any additional methods on the estimated component are performed directly by custom written functions in the tsforeign package. For the demonstration well use the priceunits dataset from the tsdatasets package. Once a series is estimated, the resulting MCMC draws are converted to an mcmc object from the coda package of Plummer et al. (2006) in order to provide a nice summary report. data(&quot;priceunits&quot;, package = &quot;tsdatasets&quot;) spec &lt;- bsts_modelspec(y = priceunits[1:80,1], frequency = 12, differences = 0, level = T, slope = T, damped = T, seasonal = T, seasonal_frequency = 12, ar = T, ar_max = 3, lambda = 0) mod &lt;- estimate(spec, n_iter = 2000, trace = FALSE) summary(mod) ## ## Iterations = 1:2000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 2000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE Prob[Include] ## obs.sigma 0.011903 0.007074 1.582e-04 0.0009229 1.0000 ## level.sigma 0.002567 0.002615 5.847e-05 0.0008459 1.0000 ## slope.sigma 0.032559 0.006015 1.345e-04 0.0003716 1.0000 ## slope.ar -0.344280 0.249709 5.584e-03 0.0208361 1.0000 ## seasonal12.sigma 0.003536 0.002417 5.406e-05 0.0005651 1.0000 ## ar1 0.052963 0.264738 5.920e-03 0.0243442 0.7050 ## ar2 0.042145 0.137206 3.068e-03 0.0111412 0.4130 ## ar3 0.001784 0.052422 1.172e-03 0.0027041 0.2235 ## ar3.sigma 0.028941 0.007514 1.680e-04 0.0004122 1.0000 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## obs.sigma 0.0050408 0.0078381 0.010473 0.014597 0.023692 ## level.sigma 0.0002324 0.0007809 0.001673 0.003381 0.010600 ## slope.sigma 0.0212699 0.0284784 0.032374 0.036300 0.044927 ## slope.ar -0.7485836 -0.5243106 -0.377527 -0.186675 0.221050 ## seasonal12.sigma 0.0007215 0.0017083 0.002734 0.004942 0.009573 ## ar1 -0.4246558 -0.0678791 0.000000 0.170936 0.694972 ## ar2 -0.1624886 0.0000000 0.000000 0.036982 0.433815 ## ar3 -0.1167698 0.0000000 0.000000 0.000000 0.123909 ## ar3.sigma 0.0207477 0.0253989 0.028291 0.031557 0.039288 ## ## ## Harvey&#39;s Goodness of Fit Statistic: 0.2121123 ## ## MAPE MASE MSLRE BIAS ## 0.0372 0.4709 0.0023 -4e-04 tsmetrics(mod) ## n no.pars MAPE MASE MSLRE BIAS ## 1 80 9 0.03717513 0.4708893 0.002293538 -0.0004315992 plot(mod) The decomposition of the model into its fitted components is done via the tsdecompose method. However, note that the bsts routine returns the smoothed component states, not the filtered ones. tde &lt;- tsdecompose(mod) # distribution objects of state components str(tde) ## List of 5 ## $ Level : &#39;tsmodel.distribution&#39; num [1:1980, 1:80] 1.49 1.55 1.64 1.66 1.74 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:80] &quot;1992-08-31&quot; &quot;1992-09-30&quot; &quot;1992-10-31&quot; &quot;1992-11-30&quot; ... ## ..- attr(*, &quot;date_class&quot;)= chr &quot;Date&quot; ## $ Slope : &#39;tsmodel.distribution&#39; num [1:1980, 1:80] 0.1074 0.0504 -0.0603 -0.0624 -0.1576 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:80] &quot;1992-08-31&quot; &quot;1992-09-30&quot; &quot;1992-10-31&quot; &quot;1992-11-30&quot; ... ## ..- attr(*, &quot;date_class&quot;)= chr &quot;Date&quot; ## $ AR : &#39;tsmodel.distribution&#39; num [1:1980, 1:80] 0.1362 0.0717 -0.0306 -0.0477 -0.1201 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:80] &quot;1992-08-31&quot; &quot;1992-09-30&quot; &quot;1992-10-31&quot; &quot;1992-11-30&quot; ... ## ..- attr(*, &quot;date_class&quot;)= chr &quot;Date&quot; ## $ X : NULL ## $ Seasonal12: &#39;tsmodel.distribution&#39; num [1:1980, 1:80] -0.0291 -0.0668 -0.0496 -0.0337 -0.0415 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:80] &quot;1992-08-31&quot; &quot;1992-09-30&quot; &quot;1992-10-31&quot; &quot;1992-11-30&quot; ... ## ..- attr(*, &quot;date_class&quot;)= chr &quot;Date&quot; Once bsts model is estimated, we can convert it to a dlm object18 using either the mean values of the parameters and initial states, or a specific draw using the tsconvert method. library(dlm) dlm_model &lt;- tsconvert(mod, to = &quot;dlm&quot;, draw = &quot;mean&quot;, burn = bsts::SuggestBurn(0.1, mod$model)) str(dlm_model) ## List of 6 ## $ m0: num [1:17] 1.5697 0.01657 -0.00452 -0.04076 -0.02871 ... ## $ C0: num [1:17, 1:17] 1e+07 0e+00 0e+00 0e+00 0e+00 0e+00 0e+00 0e+00 0e+00 0e+00 ... ## $ FF: num [1, 1:17] 1 0 0 1 0 0 0 0 0 0 ... ## $ V : num [1, 1] 0.0115 ## $ GG: num [1:17, 1:17] 1 0 0 0 0 0 0 0 0 0 ... ## $ W : num [1:17, 1:17] 0.00258 0 0 0 0 ... ## - attr(*, &quot;class&quot;)= chr &quot;dlm&quot; filtered_dlm &lt;- dlmFilter(log(priceunits[1:80,1]), dlm_model) smoothed_dlm &lt;- dlmSmooth(filtered_dlm) smoothed_series &lt;- xts((dlm_model$FF %*% t(smoothed_dlm$s))[1,-1], index(priceunits)[1:80]) par(mfrow = c(2,1), mar = c(3,3,3,3)) plot(as.zoo(filtered_dlm$f), type = &quot;l&quot;, main = &quot;Filtered Series&quot;, ylab = &quot;&quot;) lines(as.zoo(fitted(mod, raw = TRUE)), col = 2) grid() legend(&quot;topright&quot;, c(&quot;DLM&quot;,&quot;BSTS&quot;), col = 1:2, lty = 1, bty = &quot;n&quot;) plot(as.zoo(smoothed_series), type = &quot;l&quot;, main = &quot;Smoothed Series&quot;, ylab = &quot;&quot;) lines(as.zoo(fitted(mod, raw = TRUE, type = &quot;smoothed&quot;)), col = 2) grid() legend(&quot;topright&quot;, c(&quot;DLM&quot;,&quot;BSTS&quot;), col = 1:2, lty = 1, bty = &quot;n&quot;) We compare the predictive distribution of the predicted object from calling the predict method of the tsforeign package and that of the bsts package. Note that there are a lot more arguments which can be passed to the predict routine, including user overrides for the last state means and the posterior means of the parameters (see the documentation for more details). p1 &lt;- predict(mod, h = 12) # BSTS method p2 &lt;- bsts::predict.bsts(mod$model, horizon = 12) # convert predictive distribution to tsmodel.distribution for comparison p2d &lt;- p2$distribution colnames(p2d) &lt;- colnames(p1$distribution) class(p2d) &lt;- &quot;tsmodel.distribution&quot; plot(log(p1$distribution), main = &quot;tsforeign native predict c++ routine vs bsts routine&quot;) plot(p2d, add = TRUE, median_color = 2, interval_color = 2, median_type = 2) A predicted object can also be decomposed into its structural components, something we are able to do because of the custom predict routine since bsts does not return this information. td &lt;- tsdecompose(p1) par(mfrow = c(2,2), mar = c(3,3,3,3)) plot(td$Level, main = &quot;Level&quot;) plot(td$Slope, main = &quot;Slope&quot;) plot(td$Seasonal12, main = &quot;Seasonal&quot;) plot(td$AR, main = &quot;AR&quot;) References "],["other.html", "Chapter 8 Other packages", " Chapter 8 Other packages The tsaux package contains a set of functions used across all the packages of our framework. The tsdatasets packages contains some sample datasets we use for benchmarking and examples. This may be expanded in the future. "],["team.html", "Chapter 9 The tsmodels team", " Chapter 9 The tsmodels team The core team is comprised of a group of friends, and at some point ex-colleagues, with an interest in time series forecasting: Alexios Galanos (aut,cre) Keith OHara (ctb) James Nesbit (ctb) Professor Diego J. Pedregal (ctb and advisor) Professor Eric Zivot (academic sponsor and advisor) Professor Doug Martin (advisor) "],["references.html", "References", " References "]]
