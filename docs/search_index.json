[["index.html", "A Time Series Framework Chapter 1 Installation", " A Time Series Framework The tsmodels team 2022-10-13 Chapter 1 Installation Until such time as the packages make it to CRAN, please use the following: devtools::install_github(&quot;tsmodels/tsmethods&quot;) devtools::install_github(&quot;tsmodels/tsdatasets&quot;) devtools::install_github(&quot;tsmodels/tsaux&quot;) devtools::install_github(&quot;tsmodels/tsetsad&quot;) devtools::install_github(&quot;tsmodels/tsets&quot;) devtools::install_github(&quot;tsmodels/tsissmad&quot;) devtools::install_github(&quot;tsmodels/tsissm&quot;) devtools::install_github(&quot;tsmodels/tsvetsad&quot;) devtools::install_github(&quot;tsmodels/tsvets&quot;) devtools::install_github(&quot;tsmodels/tsforeign&quot;) devtools::install_github(&quot;tsmodels/tscausal&quot;) devtools::install_github(&quot;tsmodels/tsdistributions&quot;) The first 2 are required to be installed before any of the other packages. Also note, that additional dependencies may be required such as Rcpp and RcppArmadillo, and bsts for tsforeign. "],["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction The tsmodels framework provides a set of probabilistic time series forecasting models with common calling conventions and methods. Our objective was to provide a set of models which offer fast estimation, explainable decomposition and probabilistic predictions which can also be ensembled. Many of the models we have so far released are based on R. Hyndman et al. (2008), some of which are already implemented in the forecast package, but we have chosen to re-write the models from scratch based on our own design choices. Some of these choices include exclusive use of xts objects to represent time series data, data.table for certain outputs, probabilistic distributions for predictions via simulation as well as a feature rich set of methods to work with these models. Additionally, all of the our models benefit from automatic handling of missing data via the prediction step, and all models implement automatic differentiation during model estimation, making use of the TMB package. Some methods and features are still experimental and may change over time. Examples include the arguments and outputs as we work to unify and clean the codebase to be as consistent as possible across packages, and methods such as the empirical calibration of the predictive distribution. We would be remiss if we were not to mention that alternative implementations exist within the single source of error (SEM) framework in R, including the forecast package of R. Hyndman et al. (2022) and smooth package of Svetunkov (2020), with their own design decisions. We make no representation as to what is likely to be more useful for a particular user, simply that our representation and framework is the right one for us. The set of packages we have released and plan to release may include bugs, and we encourage users to submit bug reports through the github issues system. A summary of the currently available packages is given below: Package Description tsmethods Time Series S3 Methods, plotting functionality and ensembling tsaux Auxilliary functions used by all packages tsdatasets Datasets for benchmarking and examples tsets ETS Models tsissm Linear Innovations State Space Models with Multiple Seasonality tsvets Vector Innovations Linear State Space Models tsforeign Wrapper for other models [bsts (BSTS package), auto.arima (forecast package)] tscausal Time Series Casual Inference tsdistributions Time Series Parameterized Distributions The following packages may be released in the future as time allows: Package Description tspyramid Hierarchical Probabilistic Reconcilitation using Tree Stuctures tsssm Linear State Space Models tslifecycle Models for Life Cycle Prediction tsfactor Factor Models tsgarch GARCH models (reimplementation of rugarch and rmgarch See the News section for updates. References "],["tsmethods.html", "Chapter 3 tsmethods package 3.1 Introduction 3.2 Ensembling Forecast Distributions 3.3 Demonstration", " Chapter 3 tsmethods package 3.1 Introduction The tsmethods package provides a set of common methods for use in other packages in our framework. Additionally, it also exports plot methods for objects of class tsmodel.predict, which is the output class from all calls to the predict method, as well as for objects of class tsmodel.distribution, which is a subclass within tsmodel.predict providing the simulated or MCMC based forecast distribution.1 Finally, the package also includes the methods for ensembling distributions. Table 3.1 below provides the currently exported methods for each of the packages which have been released. We have tried to work with existing S3 methods in the stats package where possible including: summary, coef, logLik, AIC, fitted, residuals, predict and simulate. However, we have also created custom methods such as estimate (for model estimation from a specification object), tsdecompose (for time series decomposition of structural time series type models), tsfilter for online filtering,2 tsbacktest (for automatic backtesting) as well as a number of other methods documented in their individual packages. TABLE 3.1: Implemented Methods methods tsets tsissm tsvets tsforeign tsdistributions estimate ✓ ✓ ✓ ✓ 1 summary ✓ ✓ ✓ ✓ 1 coef ✓ ✓ ✓ 1 logLik ✓ ✓ ✓ 1 AIC ✓ ✓ ✓ 0 fitted ✓ ✓ ✓ ✓ 0 residuals ✓ ✓ ✓ ✓ 0 plot ✓ ✓ ✓ ✓ 1 tsmetrics ✓ ✓ ✓ ✓ 0 tsspec ✓ ✓ ✓ 0 tsdiagnose ✓ ✓ ✓ 1 tsdecompose ✓ ✓ ✓ ✓ 0 tsfilter ✓ ✓ ✓ 0 tsprofile ✓ ✓ 0 predict ✓ ✓ ✓ ✓ 0 tsbacktest ✓ ✓ ✓ ✓ 0 simulate ✓ ✓ ✓ 0 tsbenchmark ✓ ✓ ✓ 0 tsreport ✓ 0 tsmoments ✓ ✓ 1 tscalibrate ✓ ✓ 0 tscor ✓ 0 tscov ✓ 0 tsaggregate ✓ 0 tsconvert ✓ 0 tsequation 0 3.2 Ensembling Forecast Distributions Ensembling in the tsmodels framework proceeds as follows: (Separate) models are estimated for either the same series or a set of different series. The residuals of the estimated series are collected and the residual correlation is calculated. For very large-dimensional systems (where \\(N\\) &gt; \\(T\\)) it is possible instead to use a factor model to extract the correlations, but we leave this for a separate discussion. The estimated correlations are then used to generate correlated samples on the unit hypercube (using for instance a copula). These samples are then passed to the innov argument in the predict method of each model, at which time they are transformed to normal random variables with variance equal to the estimated model variance.3 A key reason for using uniform variates and transforming them back into normal variates within the predict method is the possible presence of the Box Cox transformation, which requires that the variance be on the transformed scale rather than the final series scale. It was therefore decided that this approach was more general and less prone to user error.4 Note that the predict function is no limited to taking innovations as quantiles but can also directly take in standardized variates and an argument is available to denote the type which is passed. The predictive distribution, which is generated by simulation, will then be infused with the cross-sectional correlation of the residuals passed to the predict method. Note: This approach works well for models with a single source of error such as ARIMA and ETS, but NOT for multiple source of error models such as BSTS. The predictive distributions are then passed to the ensemble_modelspec function for validation, followed by calling the tsensemble method on the resulting object with a vector of user specified weights. The next section provides a demonstration of this approach. 3.3 Demonstration Step 1: Estimation suppressMessages(library(tsmethods)) suppressWarnings(suppressMessages(library(tsets))) suppressMessages(library(xts)) suppressMessages(library(copula)) suppressWarnings(suppressMessages(library(viridis))) data(austretail, package = &quot;tsdatasets&quot;) y1 &lt;- austretail[,&quot;SA.DS&quot;] y2 &lt;- austretail[,&quot;ACF.DS&quot;] spec1 &lt;- ets_modelspec(y1, model = &quot;MMM&quot;, frequency = 12) spec2 &lt;- ets_modelspec(y2, model = &quot;MMM&quot;, frequency = 12) mod1 &lt;- estimate(spec1) mod2 &lt;- estimate(spec2) Step 2: Residual Correlation and Copula Construction/Sampling C &lt;- cor(residuals(mod1), residuals(mod2)) cop &lt;- normalCopula(as.numeric(C), dim = 2, dispstr = &quot;un&quot;) set.seed(100) U &lt;- rCopula(5000 * 12, cop) cor(U) ## [,1] [,2] ## [1,] 1.0000000 0.5958815 ## [2,] 0.5958815 1.0000000 Step 3: Prediction p1.joint &lt;- predict(mod1, h = 12, nsim = 5000, innov = U[,1], innov_type = &quot;q&quot;) p2.joint &lt;- predict(mod2, h = 12, nsim = 5000, innov = U[,2]) p1.indep &lt;- predict(mod1, h = 12, nsim = 5000) p2.indep &lt;- predict(mod2, h = 12, nsim = 5000) par(mfrow = c(2,2), mar = c(3,3,3,3)) plot(p1.joint, main = &quot;Correlated Predictions[y1]&quot;, n_original = 52) plot(p2.joint, main = &quot;Correlated Predictions[y2]&quot;, n_original = 52) plot(p1.indep, main = &quot;Independent Predictions[y1]&quot;, n_original = 52) plot(p2.indep, main = &quot;Independent Predictions[y2]&quot;, n_original = 52) We can also plot the distributions directly, since there is a plot method for both tsmodel.predict and tsmodel.distribution, and we can also overlay one on top of another by using the add = TRUE argument. plot(p1.joint$distribution, gradient_color = &quot;whitesmoke&quot;, median_col = &quot;grey&quot;, median_width = 4, interval_color = 3, interval_quantiles = c(0.01, 0.99), main = &quot;SA.DS 12 Month Prediction&quot;) plot(p1.indep$distribution, add = TRUE, median_col = 2, median_width = 1, gradient_color = &quot;whitesmoke&quot;, interval_color = 2, interval_quantiles = c(0.01, 0.99), interval_width = 0.5) Evaluation of Predictive Distributions on Forecast Growth Rates p1.growth.joint &lt;- tsgrowth(p1.joint, d = 1, type = &#39;diff&#39;) p2.growth.joint &lt;- tsgrowth(p2.joint, d = 1, type = &#39;diff&#39;) p1.growth.indep &lt;- tsgrowth(p1.indep, d = 1, type = &#39;diff&#39;) p2.growth.indep &lt;- tsgrowth(p2.indep, d = 1, type = &#39;diff&#39;) D.joint &lt;- cor(p1.growth.joint$distribution, p2.growth.joint$distribution) D.indep &lt;- cor(p1.growth.indep$distribution, p2.growth.indep$distribution) print(round(data.frame(Correlated = diag(D.joint), Independent = diag(D.indep)), 3)) ## Correlated Independent ## 2019-01-31 0.608 -0.021 ## 2019-02-28 0.608 -0.005 ## 2019-03-31 0.605 0.018 ## 2019-04-30 0.614 0.013 ## 2019-05-31 0.606 0.004 ## 2019-06-30 0.610 0.002 ## 2019-07-31 0.613 0.004 ## 2019-08-31 0.617 0.002 ## 2019-09-30 0.632 0.000 ## 2019-10-31 0.616 -0.003 ## 2019-11-30 0.620 -0.009 ## 2019-12-31 0.610 -0.017 par(mfrow = c(2,1), mar = c(3,3,3,3)) image(as.matrix(D.joint), col = rev(grey(seq(0, 1, length = 25))), zlim = c(0, 1), main = &quot;Correlated&quot;) contour(cor(p1.joint$distribution, p2.joint$distribution), add = TRUE, drawlabels = TRUE) image(D.indep, col = rev(grey(seq(0, 1, length = 25))), zlim = c(0, 1), main = &quot;Independent&quot;) contour(D.indep, add = TRUE, drawlabels = TRUE) Step 4: Forecast Ensembling par(mfrow = c(1,1), mar = c(3,3,3,3)) spec.joint &lt;- ensemble_modelspec(p1.joint, p2.joint) ensemble.joint &lt;- tsensemble(spec.joint, weights = c(0.5, 0.5)) plot(ensemble.joint, main = &quot;Ensemble Weighted Forecast&quot;, n_original = 52) The current choice of using base R plotting may change in the future to use either ggplot2 or plotly.↩︎ We have avoided using stats::filter as this is a function rather than a method.↩︎ The samples can be transformed to any other distribution using the quantile method of the distribution.↩︎ Additionally the marginal distributions of the individual series may not all be the same, so this approach provides some greater degree of flexibility.↩︎ "],["tsets.html", "Chapter 4 tsets package 4.1 Introduction 4.2 Taxonomy of Models 4.3 Extensions 4.4 Some Encompasing Alternatives 4.5 Package Implementation 4.6 Demonstration", " Chapter 4 tsets package 4.1 Introduction Exponential smoothing was proposed by Robert G. Brown, originally in Brown (1959) and later in Brown (1962), where he developed the general exponential smoothing methodology in the context of inventory management, production planning and control. Independently, Charles C. Holt developed a similar method for exponential smoothing of additive trends and an entirely different method for smoothing seasonal data in Holt (1957). This approach gained popularity following Winters (1960), which tested Holt’s methods with empirical data, from whence the now popular Holt-Winters forecasting system came to prominence. More recently, R. J. Hyndman et al. (2002) and Taylor (2003) formalized the framework and provided a taxonomy of the various models under different assumptions on the type of Error (E), Trend (T) and Seasonality (S) components. In their most basic form, exponential smoothing methods are weighted sums of past observations, with the weights decaying exponentially with older observations. They form a simpler alternative to the more complex structural time series models (see Harvey (1990) and West and Harrison (2006)), by adopting the innovations formulation of the state space representation with all sources of error perfectly correlated.5 The Single Source of Errors model is observationally equivalent to the Multiple Source of Errors model under non-restrictive assumptions, and the interested reader is referred to Casals, Sotoca, and Jerez (1999) for a proof of this. Formally, the general linear innovations state space model can be written as: \\[\\begin{equation} \\begin{array}{l} {y_t} &amp;= {\\bf{w&#39;}}{{\\bf{x}}_{t - 1}} + {\\varepsilon _t} ,\\\\ {{\\bf{x}}_t} &amp;= {\\bf{F}}{{\\bf{x}}_{t - 1}} + {\\bf{g}}{\\varepsilon _t}, \\end{array} \\tag{4.1} \\end{equation}\\] where \\(y_t\\) is the observed value at time \\(t\\), \\(\\mathbf{x}_t\\) the vector of state variables (which may include information about the level, slope, seasonal patterns and exogenous regressors), \\(\\mathbf{w}\\) is the observation matrix and \\(\\mathbf{F}\\) the state transition matrix. An innovations state space model can be reduced to an equivalent ARIMA model with the help of the lag operator \\(L\\). The state equation can be rewritten as 6: \\[\\begin{equation} \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){\\mathbf{x}_t} = {\\bf{g}}{\\varepsilon _t}. \\tag{4.2} \\end{equation}\\] Since \\({\\bf{I}} - {\\bf{F}}L\\) may not have an inverse, both sides are multiplied by its adjugate7 \\(adj{\\left( {{\\bf{I}} - {\\bf{F}}L} \\right)}\\) to obtain: \\[\\begin{equation} \\det\\left( {{\\bf{I}} - {\\bf{F}}L} \\right){\\mathbf{x}_t} = adj{\\left( {{\\bf{I}} - {\\bf{F}}L} \\right)}{\\bf{g}}{\\varepsilon _t}. \\tag{4.3} \\end{equation}\\] Applying the \\(\\det\\left( {{\\bf{I}} - {\\bf{F}}L} \\right)\\) to the observation equation: \\[\\begin{equation} \\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){y_t} = {\\bf{w&#39;}}\\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){{\\mathbf{x}}_{t - 1}} + \\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){\\varepsilon _t}, \\tag{4.4} \\end{equation}\\] and finally replacing \\(\\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){{\\mathbf{x}}_{t - 1}}\\) with the state equation formula we obtain: \\[\\begin{equation} \\det \\left({\\bf{I}} - {\\bf{F}}L \\right){y_t} = {\\bf{w&#39;}}\\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){\\bf{g}}{\\varepsilon _{t - 1}} + \\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){\\varepsilon _t}. \\tag{4.5} \\end{equation}\\] Defining \\(\\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right)\\) as \\(\\eta\\left(L\\right)\\) and \\({\\bf{w&#39;}}\\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){\\bf{g}}{L} + \\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right)\\) as \\(\\theta\\left(L\\right)\\) we obtain the typical ARIMA representation8 \\(\\eta \\left( L \\right){y_t} = \\theta \\left( L \\right){\\varepsilon _t}\\), where \\(\\eta\\left(L\\right)\\) and \\(\\theta\\left(L\\right)\\) are polynomials in the lag operator \\(L\\) and may include powers of \\(L\\) related to the seasonal period \\(m\\). To obtain the ARIMA representation, set \\(\\eta \\left( L \\right){y_t} = \\phi \\left( L \\right)\\delta \\left( L \\right){y_t}\\), where \\(\\delta\\left(L\\right)\\) contains all unit roots of the polynomial. Typical examples include the damped local trend model,9 which can be represented by an ARIMA(1,1,2) model and the local linear trend model, which can be represented as an ARIMA(0,2,2) model. On the other hand, an ARIMA(2,0,2) model with complex roots, which gives rise to cyclical behavior, cannot be represented by an exponential smoothing model. 4.2 Taxonomy of Models Table 4.1 presents the taxonomy proposed by Pegels (1969) and Gardner Jr (1985) for exponential smoothing models. In addition, for each of the 12 model combinations presented, it is possible to have either additive or multiplicative errors, giving rise to the ETS formulation of Error, Trend and Seasonal. For instance, the MAM model corresponds to Multiplicative Error, Additive Trend and Multiplicative Seasonality. TABLE 4.1: Exponential Smoothing Model Taxonomy N A M (none) (additive) (multiplicative) N (none) NN NA NM A (additive) AN AA AM M (multiplicative) MN MA MM D (damped) DN DA DM Following Ord, Koehler, and Snyder (1997) and R. J. Hyndman et al. (2002), we present a state space formulation the ETS modelling system. \\[\\begin{equation} \\begin{array}{l} {y_t} = h\\left( {{\\mathbf{x}_{t - 1}}} \\right) + k\\left( {{\\mathbf{x}_{t - 1}}} \\right){\\varepsilon _t},\\\\ {\\mathbf{x}_t} = f\\left( {{\\mathbf{x}_{t - 1}}} \\right) + g\\left( {{\\mathbf{x}_{t - 1}}} \\right){\\varepsilon _t}, \\end{array} \\tag{4.6} \\end{equation}\\] where \\({\\varepsilon _t} \\sim N\\left( {0,{\\sigma ^2}} \\right)\\) and \\({\\mathbf{x}_t} = \\left\\{ {{l_t},{b_t},{s_{t - 1}},...,{s_{t - \\left( {m - 1} \\right)}}} \\right\\}\\). Defining \\({e_t} = k\\left( {{x_{t - 1}}} \\right){\\varepsilon _t}\\) and \\({\\mu _t} = h\\left( {{x_{t - 1}}} \\right)\\), then \\({y_t} = {\\mu _t} + {e_t}\\). When errors are additive, then \\(y_t=\\mu_t + \\varepsilon_t\\) and \\(k\\left( {{x_{t - 1}}} \\right)=1\\), whilst when errors are multiplicative, then \\(y_t=\\mu_t\\left(1+\\varepsilon_t\\right)\\) and therefore \\(k\\left( {{x_{t - 1}}} \\right)=\\mu_t\\) so that \\(\\varepsilon_t=\\frac{\\left(y_t-\\mu_t\\right)}{\\mu_t}\\) represents a relative error. For illustration, we show below the additive error with additive trend and seasonality: \\[\\begin{equation} \\begin{array}{l} {{\\hat y}_t} = {l_{t - 1}} + \\phi {b_{t - 1}} + {s_{t - m}},\\\\ {\\varepsilon _t} = {y_t} - {{\\hat y}_t},\\\\ {l_t} = {l_{t - 1}} + \\phi {b_{t - 1}} + \\alpha {\\varepsilon _t},\\\\ {b_t} = \\phi {b_{t - 1}} + \\beta {\\varepsilon _t},\\\\ {s_t} = {s_{t - m}} + \\gamma {\\varepsilon _t}, \\end{array} \\tag{4.7} \\end{equation}\\] with \\(\\phi\\) representing the damping parameter which is equal to one when there is no damping. The \\(h\\)-step ahead forecast is then given as: \\[\\begin{equation} {y_{t + h}} = \\left\\{ {\\begin{array}{*{20}{c}} {{l_t} + h{b_t} + {s_{t - m + h_m^ + }}},&amp;{\\phi \\in \\left\\{ \\emptyset \\right\\}},\\\\ {{l_t} + {\\phi^h}{b_t} + {s_{t - m + h_m^ + }}},&amp;{\\phi \\in [0,1]}, \\end{array}} \\right. \\tag{4.8} \\end{equation}\\] where \\(h_m^ + = \\left[ {\\left( {h - 1} \\right)\\bmod m} \\right] + 1\\). 4.3 Extensions A number of extensions have been suggested and too numerous to outline here. One interesting model proposed by Koehler, Snyder, and Ord (2001) is the decomposition of the MAM model to include power terms as follows: \\[\\begin{equation} \\begin{array}{l} {y_t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right){s_{t - m}} + {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^\\theta }{s^\\delta }_{t - m}{\\varepsilon _t},\\\\ {l_t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right) + \\alpha {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^\\theta }{s_{t - m}^{\\delta - 1}}{\\varepsilon _t},\\\\ {b_t} = \\phi {b_{t - 1}} + \\beta {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^\\theta }{s_{t - m}^{\\delta - 1}}{\\varepsilon _t},\\\\ {s_t} = {s_{t - m}} + \\gamma {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^{\\theta - 1}}{s_{t - m}^\\delta}{\\varepsilon _t}. \\end{array} \\tag{4.9} \\end{equation}\\] When \\(\\theta=1\\) and \\(\\delta=1\\), this reduces to the standard MAM model. When \\(\\theta=0\\) and \\(\\delta=0\\) this is reduces to the AAM model or the AAN model if there is no seasonal term. The exponents can be thought as controlling the degree of heteroscedasticity in the data, since the unscaled residuals \\(\\epsilon_t\\) are distributed as: \\[\\begin{equation} {\\epsilon_t} = {y_t} - \\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right){s_{t - m}},\\quad {\\epsilon_t} | \\mathcal{F}_{t-1} \\sim N\\left( {0,{{\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)}^{2\\theta} }{s_{t-m}^{2\\delta}} \\sigma^2 } \\right), \\tag{4.10} \\end{equation}\\] which is an appealing alternative to Box Cox and related transformations. For instance, as R. Hyndman et al. (2008) 4.4.5 notes, a value of \\(\\theta=1/3\\) would produce a variance proportional to the \\(2/3\\) power of the mean, similar to the cube root transformation. Normalized seasonality, discussed in Roberts (1982) and McKenzie (1986), can be used to de-seasonalize the data, by acting as a filter, and is required for implementing the Wiener-Kolmogorov (WK) filter (smoother). This is discussed in more detail in Chapter 8 of R. Hyndman et al. (2008), and is implemented as an option in the tsets package. Another avenue of interest is in the multivariate generalization of the model presented in De Silva, Hyndman, and Snyder (2010). It has the ability to incorporate common levels, trends or seasonality and is implemented in the tsvets package. 4.4 Some Encompasing Alternatives The general state space representation (see Harvey (1990)), based on the multiple sources of error (MSOE) state space model, provides a more general implementation of the unobserved components model, albeit requiring the use of the Kalman filter for estimation. The model is flexible enough to incorporate many types of additive models, including cyclical behavior and regressors, although the nonlinear (multiplicative) variations require the extended Kalman filter for estimation. The bsts package of Steven L. Scott and Varian (2015) provides fast and efficient computation of Bayesian Unobserved Components, with the option of a spike and slab prior for regressor regularization, for which we provide a wrapper in the tsforeign package. 4.5 Package Implementation The tsets package implements 4 families of models, whose equations are given in Tables 4.2 and 4.3. These are the full equations assuming all variables (trend, damped, seasonal and regressors) enter the model, but any and all combinations of the variables are allowed. Methods implemented include Quasi-ML estimation, prediction, simulation, plotting and post-estimation diagnostics. TABLE 4.2: AAA and MMM Model Equations Equation AAA MMM Observation \\({y_t} = {l_{t - 1}} + \\phi{b_{t - 1}} + {s_{t - m}} + \\bf{x}_{t - 1}\\bf{w} + \\varepsilon_t\\) \\({y_t} = {l_{t - 1}}{b_{t-1}^\\phi}\\left(1.0 + \\bf{x}_{t-1}\\bf{w}\\right){s_{t - m}}\\left(1 + \\varepsilon _t\\right)\\) Mean \\({\\mu _t} = {l_{t - 1}} + \\phi{b_{t - 1}} + {s_{t - m}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}\\) \\({\\mu_t} = {l_{t - 1}}{b^\\phi_{t - 1}}\\left( {{{1.0 + \\bf{x}}_{t-1}}{\\bf{w}}} \\right){s_{t - m}}\\) Distribution \\({\\hat y_t}\\left| {{\\Im _{t - 1}} \\sim N\\left( {{\\mu _t},\\sigma } \\right)} \\right.\\) \\({{\\hat y}_t}\\left| {{\\Im _{t - 1}} \\sim N\\left( {{\\mu _t},{l_{t - 1}}{b_{t-1}^\\phi}\\left( {{{\\bf{x}}_{t-1}}{\\bf{w}}} \\right){s_{t - m}}\\sigma } \\right)} \\right.\\) Error \\({\\varepsilon _t} = {y_t} - {\\mu _t}\\) \\({\\varepsilon _t} = \\frac{{{y_t} - {\\mu _t}}}{{{l_{t - 1}}{b}_{t - 1}^\\phi \\left( {{{1.0 + \\bf{x}}_{t-1}}{\\bf{w}}} \\right){s_{t - m}}}}\\) Level[State] \\({l_t} = {l_{t - 1}} + \\phi {b_{t - 1}} + \\alpha {\\varepsilon _t}\\) \\({l_t} = {l_{t - 1}}{b_{t-1}^\\phi}\\left( {1 + \\alpha {\\varepsilon _t}} \\right)\\) Trend[State] \\({b_t} = \\phi {b_{t - 1}} + \\beta {\\varepsilon _t}\\) \\({b_t} = {b_{t-1}^\\phi}\\left( {1 + \\beta {\\varepsilon _t}} \\right)\\) Seasonal[State] \\({s_t} = {s_{t - m}} + \\gamma {\\varepsilon _t}\\) \\({s_t} = {s_{t - m}}\\left( {1 + \\gamma {\\varepsilon _t}} \\right)\\) TABLE 4.3: MAM and powerMAM Model Equations Equation MAM powerMAM Observation \\({y_t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right){s_{t - m}}\\left( {1 + {\\varepsilon _t}} \\right)\\) \\({y_t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right){s_{t - m}} + {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right)^\\theta }s_{t - m}^\\delta {\\varepsilon _t}\\) Mean \\({\\mu _t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right){s_{t - m}}\\) \\({\\mu _t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right){s_{t - m}}\\) Distribution \\({\\hat y_t}\\left| {{\\Im _{t - 1}} \\sim N\\left( {{\\mu _t},\\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right){s_{t - m}}\\sigma } \\right)} \\right.\\) \\({\\hat y_t}\\left| {{\\Im _{t - 1}} \\sim N\\left( {{\\mu _t},{{\\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right)}^\\theta }s_{t - m}^\\delta \\sigma } \\right)} \\right.\\) Error \\({\\varepsilon _t} = \\frac{{{y_t} - {\\mu _t}}}{{\\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right){s_{t - m}}}}\\) \\({\\varepsilon _t} = \\frac{{{y_t} - {\\mu _t}}}{{{{\\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right)}^\\theta }s_{t - m}^\\delta }}\\) Level[State] \\({l_t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)\\left( {1 + \\alpha {\\varepsilon _t}} \\right)\\) \\({l_t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right) + \\alpha {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^\\theta }s_{t - m}^{\\delta - 1}{\\varepsilon _t}\\) Trend[State] \\({b_t} = \\phi {b_{t - 1}} + \\beta \\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right){\\varepsilon _t}\\) \\({b_t} = \\phi {b_{t - 1}} + \\beta {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^\\theta }s_{t - m}^{\\delta - 1}{\\varepsilon _t}\\) Seasonal[State] \\({s_t} = {s_{t - m}}\\left( {1 + \\gamma {\\varepsilon _t}} \\right)\\) \\({s_t} = {s_{t - m}} + \\gamma {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^{\\theta - 1}}s_{t - m}^\\delta {\\varepsilon _t}\\) 4.5.1 Constraints Variables in the additive ETS type models need to be constrained in order to achieve stability and forecastability, and the interested reader should consult Section 10.2 of R. J. Hyndman, Akram, and Archibald (2008) for more details. These conditions become increasingly complex depending on the components included in the model. In our implementation we have opted for the following simple conditions: \\(\\alpha \\in \\left[ {0,1} \\right]\\), \\(\\beta \\in \\left[ {0,\\alpha } \\right]\\), \\(\\gamma \\in \\left[ {0,1 - \\alpha } \\right]\\), \\(\\phi \\in \\left[ {0.5,1} \\right]\\), \\(\\theta \\in \\left[ {0,1} \\right]\\), \\(\\delta \\in \\left[ {0,1} \\right]\\), \\(\\sigma \\in {{\\bf{R}}_+ }\\). For multiplicative models, we simply impose that \\(\\max\\left(\\alpha,\\beta,\\gamma\\right)&lt;1\\) and \\(\\varepsilon_t&gt;-1\\), although we do not impose the last condition for estimation (only for simulation). 4.5.2 Initialization To obtain a reasonable set of parameters for the initialization conditions of the states as well as the parameters, we obtain values for \\(l_0\\), \\(b_0\\) and \\(s_0\\) using the heuristic approach described in Section 5.2 of R. Hyndman et al. (2008). There is also an option for estimating the initial states for the seasonal component. 4.5.3 Transformations Variance stabilizing transformations form an important part of the pre-processing of the outcome variable in order to achieve certain statistical properties which help reduce misspecification of the model. At present, we implement the Box-Cox and logit transformations with an option for automatic tuning of the parameter \\(\\lambda\\) (for Box-Cox) using the method of Guerrero (1993), from the tsaux package. 4.6 Demonstration 4.6.1 The Specification Object The entry specification function is called ets_modelspec suppressWarnings(suppressMessages(library(tsets))) args(ets_modelspec) ## function (y, model = &quot;AAN&quot;, damped = FALSE, power = FALSE, xreg = NULL, ## frequency = NULL, transformation = &quot;box-cox&quot;, lambda = NULL, ## normalized_seasonality = TRUE, fixed_pars = NULL, scale = FALSE, ## seasonal_init = &quot;fixed&quot;, lower = 0, upper = 1, sampling = NULL, ## xreg_init = TRUE, ...) ## NULL This requires passing in an xts vector y, followed by a number of options described below: model: The ETS model type. damped: Whether to dampen the trend. power: The power MAM model (only applicable to the MAM). xreg: An xts matrix of regressors. frequency: The seasonal frequency of y, only needed if using a seasonal model. transformation: The name of the transformation lambda: The Box Box transformation parameter. If NA, will estimate it. normalized_seasonality: Whether to impose the normalized approach of McKenzie (1986). fixed_pars: An optional named vector of fixed parameters. scale: Whether to pre-scale the data prior to estimation (will rescale back after estimation). seasonal_init: Whether to “estimate” or use the heuristic (“fixed”) values for the initial states. lower: the lower bound of the transformation (used for the Box-Cox estimation or the logit bounds). upper: the upper bound of the transformation (used for the Box-Cox estimation or the logit bounds). sampling: An optional string denoting the sampling frequency of the data (will try to discover it if NULL). xreg_init: An optional vector of initial parameters for the regressors. The xreg_init argument is particularly useful for initializing the parameters for the regressors during estimation which may sometime be difficult due to different scales. The function auto_regressors from the tsaux package will always return these values which can be then passed to this argument. 4.6.2 Estimation For illustration, we use the gas dataset from the tsdatasets package, representing weekly US finished motor gasoline products supplied (in thousands of barrels per day) from February 1991 to May 2005. data(gas, package = &quot;tsdatasets&quot;) spec &lt;- ets_modelspec(gas[1:(NROW(gas) - 52)], model = &quot;AAA&quot;, frequency = 52, lambda = NA) str(spec, max.level = 1) ## List of 5 ## $ target :List of 6 ## $ model :List of 15 ## $ seasonal :List of 1 ## $ transform:List of 7 ## $ xreg :List of 3 ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;tsets.spec&quot; &quot;tsmodel.spec&quot; The specification object returns a list which inherits the tsmodel.spec class and has a number of slots. Common across the tsmodels framework will be the target slot, which has the target variable with details on its sampling frequency among others, the transform slot, which contains the Box Cox transformation and its inverse, and the xreg slot, which contains any optionally including external regressors. Estimation by default will use the nlminb solver with autodiff on. There is also an option of using the hessian by setting use_hessian to TRUE, in which case the standard errors and p-values if available are also generated. mod &lt;- estimate(spec) summary(mod) ## ## ETS Model [ AAA ] ## ## Parameter Description Est[Value] Std. Error t value Pr(&gt;|t|) ## ---------- --------------------- ----------- ----------- --------- --------- ## alpha State[Level-coef] 0.0348 NaN NaN NaN ## beta State[Slope-coef] 0.0001 NaN NaN NaN ## gamma State[Seasonal-coef] 0.0000 NaN NaN NaN ## l0 State[Level-init] 7108.5605 48.1694 147.5741 0 ## b0 State[Slope-init] 2.7619 0.4735 5.8331 0 ## ## ## LogLik AIC BIC AICc ## --------- --------- -------- --------- ## -6123.43 12360.86 12619.7 12371.27 ## ## ## MAPE MASE MSLRE BIAS ## ------ ------- ------- ------ ## 0.026 0.6328 0.0011 9e-04 A number of methods exist for post-estimation inference which we illustrate below. logLik(mod) ## &#39;log Lik.&#39; -6123.43 (df=6) AIC(mod) ## [1] 12360.86 coef(mod) ## alpha beta gamma l0 b0 ## 0.03476947 0.00010000 0.00000000 7108.56050025 2.76190798 ## s0 s1 s2 s3 s4 ## -511.18996952 -456.41526245 -461.98649919 -139.91350053 -54.69925501 ## s5 s6 s7 s8 s9 ## 130.45382240 30.67198353 44.64905141 -183.07147849 -118.14143955 ## s10 s11 s12 s13 s14 ## -36.61803477 106.86088082 64.97841126 60.58207265 133.10219815 ## s15 s16 s17 s18 s19 ## 59.11091891 -32.12207765 -112.14202044 31.88357988 78.64699634 ## s20 s21 s22 s23 s24 ## 166.54294243 315.25674137 247.26950318 357.09149148 240.91993671 ## s25 s26 s27 s28 s29 ## 403.59736605 216.67098944 346.01213061 229.17309895 342.80928619 ## s30 s31 s32 s33 s34 ## 354.71663251 245.57277925 80.87671647 80.08645793 70.46072950 ## s35 s36 s37 s38 s39 ## 165.45430465 70.35842296 -50.38183625 -130.95815688 111.89388805 ## s40 s41 s42 s43 s44 ## -47.23340484 -120.85562760 -187.81733374 -108.53261498 -107.25369102 ## s45 s46 s47 s48 s49 ## 4.20810004 -259.16694666 -266.68011313 -211.33469029 -269.16731225 ## s50 sigma ## -497.75901659 261.46393288 head(residuals(mod, raw = TRUE)) ## residuals ## 1991-02-01 -68.26357 ## 1991-02-08 -185.24816 ## 1991-02-15 -261.22191 ## 1991-02-22 328.94254 ## 1991-03-01 21.31185 ## 1991-03-08 82.27015 head(residuals(mod, raw = FALSE)) ## residuals ## 1991-02-01 -68.30331 ## 1991-02-08 -185.35576 ## 1991-02-15 -261.37412 ## 1991-02-22 329.13531 ## 1991-03-01 21.32429 ## 1991-03-08 82.31824 head(fitted(mod)) ## fitted ## 1991-02-01 6689.303 ## 1991-02-08 6618.356 ## 1991-02-15 6843.374 ## 1991-02-22 6894.865 ## 1991-03-01 6853.676 ## 1991-03-08 6864.682 plot(mod) tsdiagnose(mod) ## ## Weighted Ljung-Box Test [scaled residuals] ## ------------------------------------------ ## Lag statistic pvalue ## &lt;char&gt; &lt;num&gt; &lt;num&gt; ## [1] 9.01 0.00268 ## [2] 9.39 0.00276 ## [3] 10.77 0.00197 ## [4] 11.48 0.00221 ## ## Parameter Bounds and Conditions ## ------------------------------------------ ## coef value &gt;lb &lt;ub condition condition_pass ## &lt;char&gt; &lt;num&gt; &lt;lgcl&gt; &lt;lgcl&gt; &lt;char&gt; &lt;lgcl&gt; ## alpha 0.0348 TRUE TRUE NA NA ## beta 0.0001 TRUE TRUE &lt; alpha TRUE ## gamma 0.0000 TRUE TRUE &lt; (1 - alpha) TRUE ## phi NA TRUE TRUE NA NA ## theta NA TRUE TRUE NA NA ## delta NA TRUE TRUE NA NA ## ## Outlier Diagnostics (based on Rosner Test) ## ------------------------------------------ ## Outliers: 1998-03-27 tsmetrics(mod) ## n no.pars LogLik AIC BIC AICc MAPE MASE ## 1 693 56 -6123.43 12360.86 12619.7 12371.27 0.02604335 0.6328421 ## MSLRE BIAS ## 1 0.00108786 0.0008551045 tsd_mod &lt;- tsdecompose(mod) tsd_mod &lt;- do.call(cbind, lapply(1:length(tsd_mod), function(i) tsd_mod[[i]])) head(tsd_mod) ## Level Slope Seasonal Irregular ## 1991-02-01 7108.561 2.761908 -426.4712 -68.26357 ## 1991-02-08 7108.949 2.755082 -497.7590 -185.24816 ## 1991-02-15 7105.263 2.736557 -269.1673 -261.22191 ## 1991-02-22 7098.917 2.710435 -211.3347 328.94254 ## 1991-03-01 7113.065 2.743329 -266.6801 21.31185 ## 1991-03-08 7116.549 2.745460 -259.1669 82.27015 The tsdecompose function also has an argument for simplifying the output to return components named Trend, Seasonal, X and Irregular. Automatic selection can be also be carried out by using the auto_ets function. 4.6.3 Prediction All prediction objects in the tsmodels framework are of class tsmodel.predict, with slots for the original series and the forecast distribution (the latter being of class tsmodel.distribution). Some prediction objects will contain additional slots, usually the original specification object as well as the state component predicted decomposition (also of class tsmodel.distribution). p &lt;- predict(mod, h = 52, nsim = 5000) class(p) ## [1] &quot;tsets.predict&quot; &quot;tsmodel.predict&quot; str(p, max.level = 1) ## List of 6 ## $ distribution : &#39;tsets.distribution&#39; num [1:5000, 1:52] 9357 8700 8935 9116 9279 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## ..- attr(*, &quot;date_class&quot;)= chr &quot;Date&quot; ## $ original_series:&#39;zoo&#39; series from 1991-02-01 to 2004-05-07 ## Data: num [1:693] 6621 6433 6582 7224 6875 ... ## Index: Date[1:693], format: &quot;1991-02-01&quot; &quot;1991-02-08&quot; ... ## $ h : num 52 ## $ spec :List of 5 ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;tsets.spec&quot; &quot;tsmodel.spec&quot; ## $ decomposition :List of 5 ## $ mean :&#39;zoo&#39; series from 2004-05-14 to 2005-05-06 ## Data: Named num [1:52] 9143 9156 9161 9323 9435 ... ## ..- attr(*, &quot;names&quot;)= chr [1:52] &quot;2004-05-14&quot; &quot;2004-05-21&quot; &quot;2004-05-28&quot; &quot;2004-06-04&quot; ... ## Index: Date[1:52], format: &quot;2004-05-14&quot; &quot;2004-05-21&quot; ... ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;tsets.predict&quot; &quot;tsmodel.predict&quot; plot(p, n_original = 52*4) p_decomp &lt;- tsdecompose(p) par(mfrow = c(2,2), mar = c(3,3,3,3)) plot(p_decomp$Level, main = &quot;Level[State] Predicted Distribution&quot;) plot(p_decomp$Slope, main = &quot;Slope[State] Predicted Distribution&quot;) plot(p_decomp$Seasonal, main = &quot;Seasonal[State] Predicted Distribution&quot;) plot(p_decomp$Error, main = &quot;Simulated Error Distribution&quot;) Since we left 52 points for out-of-sample testing, we are able to evaluate the prediction using the tsmetrics method on a predicted object. This method also takes the original series as an input in order to calculate MASE in the presence of seasonality. The alpha parameter is the coverage rate for calculation of the Mean Interval Score of Gneiting and Raftery (2007). Note that the tsdecompose method has an additional parameter for simplifying the state output which effectively means that the Level and Slope are aggregated into one components (Trend). This is probably more useful in a package such as tsissm which may include multiple seasonality and aggregating them sometimes make sense. tsmetrics(p, tail(gas, 52), original_series = spec$target$y_orig, alpha = 0.05) ## h MAPE MASE MSLRE BIAS MIS CRPS ## 1 52 0.01447461 0.4016108 0.000388943 0.003547343 1074.333 104.6745 tsd_predict &lt;- tsdecompose(p, simplify = TRUE) par(mfrow = c(2,2), mar = c(3,3,3,3)) plot(tsd_predict$Trend, main = &quot;Trend[State]&quot;) plot(tsd_predict$Seasonal, main = &quot;Seasonal[State]&quot;) plot(tsd_predict$Irregular, main = &quot;Irregular[State]&quot;) 4.6.4 Filtering Online filtering is when new data arrives and instead of re-estimating the model, we instead just filter the new data based on an existing model. In the tsets package the tsfilter method updates an object of class tsmodel.estimate with new data as the example below illustrates. Because the class of the model is retained and only updated (both data and states) with new information, it is also possible to apply any method to that which admits that object (e.g. predict). mod_filter &lt;- tsfilter(mod, y = gas[(NROW(gas) - 52 + 1)]) tail(fitted(mod),1) ## fitted ## 2004-05-07 9239.359 tail(fitted(mod_filter),2) ## fitted ## 2004-05-07 9239.359 ## 2004-05-14 9143.025 mod_filter &lt;- tsfilter(mod_filter, y = gas[(NROW(gas) - 52 + 2)]) tail(fitted(mod),1) ## fitted ## 2004-05-07 9239.359 tail(fitted(mod_filter),3) ## fitted ## 2004-05-07 9239.359 ## 2004-05-14 9143.025 ## 2004-05-21 9155.941 head(predict(mod, h = 12)$mean) ## 2004-05-14 2004-05-21 2004-05-28 2004-06-04 2004-06-11 2004-06-18 ## 9158.698 9170.661 9153.782 9329.027 9439.320 9448.866 head(predict(mod_filter, h = 12)$mean) ## 2004-05-28 2004-06-04 2004-06-11 2004-06-18 2004-06-25 2004-07-02 ## 9156.777 9321.691 9441.089 9417.546 9320.598 9447.503 4.6.5 Simulation An estimated object can also be simulated from, with the parameters and initial states overridden by passing them as named values in the pars argument. The default is to initialize the states from the seed states used in the estimated object, with h equal to the length of the original series (default for NULL h). Innovations for the simulation can either be parametric (normal for additive or truncated normal for multiplicative error models), based on the estimated residuals (bootstrap argument) or a user supplied set of uniform random numbers (which are then translated into normal or truncated normal using standard deviation equal to the model sigma and optionally scaled by sigma_scale). sim &lt;- simulate(mod, nsim = 10, h = 52*2) plot(sim) par(mar = c(3,3,3,3), mfrow = c(4, 1)) matplot(as.Date(colnames(sim$Simulated)), t(sim$Simulated[1:10, ]), type = &quot;l&quot;, ylab = &quot;&quot;, main = &quot;Simulated Paths&quot;, xlab = &quot;&quot;) grid() matplot(as.Date(colnames(sim$Simulated)), t(sim$Level[1:10, ]), type = &quot;l&quot;, ylab = &quot;&quot;, main = &quot;Simulated Level&quot;, xlab = &quot;&quot;) grid() matplot(as.Date(colnames(sim$Simulated)), t(sim$Slope[1:10, ]), type = &quot;l&quot;, ylab = &quot;&quot;, main = &quot;Simulated Slope&quot;, xlab = &quot;&quot;) grid() # not much on the seasonal since gamma coefficient is zero matplot(as.Date(colnames(sim$Simulated)), t(sim$Seasonal[1:10, ]), type = &quot;l&quot;, ylab = &quot;&quot;, main = &quot;Simulated Seasonal&quot;, xlab = &quot;&quot;) grid() 4.6.6 Profiling The tsprofile function profiles an estimated model by simulating and then estimating multiple paths from the assumed DGP while leaving h values out for prediction evaluation. Each simulated path is equal to the size of the original dataset plus h additional values, and initialized with the initial state vector from the model. The resulting output contains the distribution of the MAPE, percent bias (BIAS) and mean squared log relative error (MSLRE) per horizon h. Since these matrices are of class tsmodel.distribution, they can be readily plotted with the special purpose plot function for this class from the tsmethods package. Additionally, a data.table matrix is also return with the distribution of the coefficients from each path estimation. # profiling prof &lt;- tsprofile(mod, h = 52, nsim = 1000, trace = 0) ## ## Attaching package: &#39;data.table&#39; ## The following objects are masked from &#39;package:xts&#39;: ## ## first, last ## The following objects are masked from &#39;package:zoo&#39;: ## ## yearmon, yearqtr ## Warning: UNRELIABLE VALUE: Future (&#39;&lt;none&gt;&#39;) unexpectedly generated random ## numbers without specifying argument &#39;seed&#39;. There is a risk that those random ## numbers are not statistically sound and the overall results might be invalid. ## To fix this, specify &#39;seed=TRUE&#39;. This ensures that proper, parallel-safe random ## numbers are produced via the L&#39;Ecuyer-CMRG method. To disable this check, use ## &#39;seed=NULL&#39;, or set option &#39;future.rng.onMisuse&#39; to &quot;ignore&quot;. plot(prof, type = &quot;metrics&quot;) plot(prof, type = &quot;coef&quot;) 4.6.7 Backtesting The tsbacktest method generates an expanding window, walk forward backtest, returning a list with the estimation/horizon predictions against actuals, as well as a table of average performance metrics by horizon. The following code snipper shows how to set up parallel execution using the future package which is used across all the packages in the repository. suppressMessages(library(future)) plan(list( tweak(sequential), tweak(multisession, workers = 5) )) b &lt;- tsbacktest(spec, h = 52, alpha = c(0.02, 0.1), trace = FALSE) head(b$metrics) ## horizon variable MAPE MSLRE BIAS n MIS[0.02] ## &lt;int&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;int&gt; &lt;num&gt; ## 1: 1 y 0.02719382 0.001181631 4.852284e-05 347 1731.006 ## 2: 2 y 0.02703144 0.001171728 4.016646e-05 346 1751.405 ## 3: 3 y 0.02697888 0.001165600 8.026391e-05 345 1680.371 ## 4: 4 y 0.02710193 0.001175067 8.357437e-06 344 1731.333 ## 5: 5 y 0.02722021 0.001179874 -8.818227e-05 343 1722.267 ## 6: 6 y 0.02679243 0.001150047 -3.196561e-04 342 1691.321 ## MIS[0.1] ## &lt;num&gt; ## 1: 1199.827 ## 2: 1194.232 ## 3: 1186.844 ## 4: 1187.321 ## 5: 1201.019 ## 6: 1182.579 plot(b$metrics$horizon, b$metrics$MAPE*100, main = &quot;Horizon MAPE&quot;, type = &quot;l&quot;, ylab = &quot;MAPE[%]&quot;, xlab = &quot;horizon&quot;) grid() plan(sequential) 4.6.8 Benchmarking The tsbenchmark method is used to benchmark a model for timing and accuracy and can be used as a unit testing function. bench &lt;- rbind(tsbenchmark(spec, solver = &quot;optim&quot;), tsbenchmark(spec, solver = &quot;nlminb&quot;), tsbenchmark(spec, solver = &quot;nlminb&quot;, autodiff = TRUE), tsbenchmark(spec, solver = &quot;solnp&quot;)) bench ## start end spec estimate ## &lt;POSc&gt; &lt;POSc&gt; &lt;list&gt; &lt;list&gt; ## 1: 2022-10-13 15:02:39 2022-10-13 15:02:39 &lt;tsets.spec[5]&gt; &lt;tsets.estimate[4]&gt; ## 2: 2022-10-13 15:02:39 2022-10-13 15:02:39 &lt;tsets.spec[5]&gt; &lt;tsets.estimate[4]&gt; ## 3: 2022-10-13 15:02:39 2022-10-13 15:02:39 &lt;tsets.spec[5]&gt; &lt;tsets.estimate[4]&gt; ## 4: 2022-10-13 15:02:39 2022-10-13 15:02:39 &lt;tsets.spec[5]&gt; &lt;tsets.estimate[4]&gt; ## solver control loglik ## &lt;char&gt; &lt;list&gt; &lt;num&gt; ## 1: optim 0 -6125.443 ## 2: nlminb 0 -6123.430 ## 3: nlminb 0 -6123.430 ## 4: solnp 0 -6123.430 References "],["tsissm.html", "Chapter 5 tsissm package 5.1 Introduction 5.2 State Initialization 5.3 Log-Likelihood 5.4 Package Implementation 5.5 Demonstration", " Chapter 5 tsissm package 5.1 Introduction The tsissm package implements the linear (and homoscedastic) innovations state space model described in De Livera, Hyndman, and Snyder (2011) and originally proposed by Anderson and Moore (2012). It is in some ways similar to the ETS based models described in R. Hyndman et al. (2008), but with the flexibility of incorporating multiple seasonality, ARMA terms and joint maximization of the Box Cox variance stabilizing lambda parameter. The following is taken from De Livera, Hyndman, and Snyder (2011). Consider the following SEM model: \\[\\begin{equation} \\begin{array}{l} y_t^\\lambda = {\\bf{w&#39;}}{{\\bf{x}}_{t - 1}} + \\bf{c&#39;}\\bf{u}_{t - 1} + {\\varepsilon _t}, \\quad \\varepsilon_t\\sim N\\left(0,\\sigma^2\\right),\\\\ {{\\bf{x}}_t} = {\\bf{F}}{{\\bf{x}}_{t - 1}} + {\\bf{g}}{\\varepsilon _t}, \\end{array} \\tag{5.1} \\end{equation}\\] where \\(\\lambda\\) represents the Box Cox parameter, \\(\\bf{w}\\) the observation coefficient vector, \\(\\bf{x}_t\\) the unobserved state vector, \\(\\bf{c}\\) a vector of coefficients on the external regressor set \\(\\bf{u}\\). Define the state vector10 as: \\[\\begin{equation} \\bf{x}_t = {\\left( {{l_t},{b_t},s_t^{(1)},\\dots,s_t^{(T)},{d_t},{d_{t - 1}},\\dots,{d_{t - p - 1}},{\\varepsilon_t},{\\varepsilon_{t - 1}},\\dots,{\\varepsilon _{t - q - 1}}} \\right)^\\prime }, \\tag{5.2} \\end{equation}\\] where \\(\\bf{s}_t^{(i)}\\) is the row vector \\(\\left( {s_{1,t}^{(i)},s_{2,t}^{(i)},\\dots,s_{{k_i},t}^{(i)},s_{1,t}^{*(i)},s_{2,t}^{*(i)},\\dots,s_{{k_i},t}^{*(i)}} \\right)\\) in the case of trigonometric seasonality, and \\(\\left( {s_t^{(i)},s_{t - 1}^{(i)},\\dots,s_{t - ({m_i} - 1)}^{(i)}} \\right)\\) in the case of regular seasonality. Also define \\(\\bf{1}_r\\) and \\(\\bf{0}_r\\) as a vector of ones and zeros, respectively, of length \\(r\\), \\(\\bf{O}_{u,v}\\) a \\(u\\times v\\) matrix of zeros and \\(\\bf{I}_{u,v}\\) a \\(u\\times v\\) diagonal matrix of ones; let \\(\\mathbf{\\gamma} = \\left(\\mathbf{\\gamma}^{(1)},\\dots,\\mathbf{\\gamma}^{(T)} \\right)\\) be a vector of seasonal parameters with \\({\\gamma ^{(i)}} = \\left( {\\gamma _1^{(i)}{{\\bf{1}}_{{k_i}}},\\gamma _2^{(i)}{{\\bf{1}}_{{k_i}}}} \\right)\\) in the trigonometric seasonality case (with \\(k\\) harmonics), and \\({\\gamma ^{(i)}} = \\left( {{\\gamma _i},{{\\bf{0}}_{{m_i} - 1}}} \\right)\\) in the regular seasonality case; define \\(\\theta = \\left( {{\\theta_1},{\\theta_2},\\dots,{\\theta _p}} \\right)\\) and \\(\\psi = \\left( {{\\psi _1},{\\psi _2},\\dots,{\\psi_q}} \\right)\\) as the vector of AR(p) and MA(q) parameters, respectively. Define the observation transition vector \\({\\bf{w}} = {\\left( {1,\\phi ,{\\bf{a}},\\theta ,\\psi } \\right)^\\prime }\\), where \\({\\bf{a}} = \\left( {{{\\bf{a}}^{(1)}},\\dots,{{\\bf{a}}^{(T)}}} \\right)\\) with \\({{\\bf{a}}^{(i)}} = \\left( {{{\\bf{1}}_{{k_i}}},{{\\bf{0}}_{{k_i}}}} \\right)\\) for the trigonometric case and \\({{\\bf{a}}^{(i)}} = \\left( {{{\\bf{0}}_{{m_i} - 1}},1} \\right)\\) for the regular seasonality case. Define the state error adjustment vector \\({\\bf{g}} = {\\left( {\\alpha ,\\beta ,\\gamma ,1,{{\\bf{0}}_{p - 1}},1,{{\\bf{0}}_{q - 1}}} \\right)^\\prime }\\). Further, let \\({\\bf{B}} = \\gamma &#39;\\theta\\), \\({\\bf{C}} = \\gamma &#39;\\psi\\) and \\({\\bf{A}} = \\oplus _{i = 1}^T{{\\bf{A}}_i}\\), with \\[\\begin{equation} {{\\bf{A}}_i} = \\left[ {\\begin{array}{*{20}{c}} {{{\\bf{C}}^{(i)}}}&amp;{{{\\bf{S}}^{(i)}}}\\\\ { - {{\\bf{S}}^{(i)}}}&amp;{{{\\bf{C}}^{(i)}}} \\end{array}} \\right], \\tag{5.3} \\end{equation}\\] for the trigonometric case and with \\(\\bf{C}^{(i)}\\) and \\(\\bf{S}^{(i)}\\) representing the \\(k_i\\times k_i\\) diagonal matrices with elements \\(cos(\\lambda_j^{(i)})\\) and \\(sin(\\lambda_j^{(i)})\\) respectively,11 and \\[\\begin{equation} {{\\bf{A}}_i} = \\left[ {\\begin{array}{*{20}{c}} {{{\\bf{0}}_{{m_i} - 1}}}&amp;1\\\\ {{{\\bf{I}}_{{m_i} - 1}}}&amp;{{{{\\bf{0&#39;}}}_{{m_i} - 1}}} \\end{array}} \\right], \\tag{5.4} \\end{equation}\\] for the regular seasonality case, with \\(\\oplus\\) being the direct sum of matrices operator. Finally, the state transition matrix \\(\\bf{F}\\) is composed as follows: \\[\\begin{equation} {\\bf{F}} = \\left[ {\\begin{array}{*{20}{c}} 1&amp;\\phi &amp;{{{\\bf{0}}_\\tau }}&amp;{\\alpha \\theta }&amp;{\\alpha \\psi }\\\\ 0&amp;\\phi &amp;{{{\\bf{0}}_\\tau }}&amp;{\\beta \\theta }&amp;{\\beta \\psi }\\\\ {{{{\\bf{0&#39;}}}_\\tau }}&amp;{{{{\\bf{0&#39;}}}_\\tau }}&amp;{\\bf{A}}&amp;{\\bf{B}}&amp;{\\bf{C}}\\\\ 0&amp;0&amp;{{{\\bf{0}}_\\tau }}&amp;\\theta &amp;\\psi \\\\ {{{{\\bf{0&#39;}}}_{p - 1}}}&amp;{{{{\\bf{0&#39;}}}_{p - 1}}}&amp;{{{\\bf{O}}_{p - 1,\\tau }}}&amp;{{{\\bf{I}}_{p - 1,p}}}&amp;{{{\\bf{O}}_{p - 1,q}}}\\\\ 0&amp;0&amp;{{{\\bf{0}}_\\tau }}&amp;{{{\\bf{0}}_p}}&amp;{{{\\bf{0}}_q}}\\\\ {{{{\\bf{0&#39;}}}_{q - 1}}}&amp;{{{{\\bf{0&#39;}}}_{q - 1}}}&amp;{{{\\bf{O}}_{q - 1,\\tau }}}&amp;{{{\\bf{O}}_{q - 1,p}}}&amp;{{{\\bf{I}}_{q - 1,q}}} \\end{array}} \\right] \\tag{5.5} \\end{equation}\\] where \\(\\tau = 2\\sum\\limits_{i = 1}^T {{k_i}}\\) for the trigonometric case and \\(\\tau = \\sum\\limits_{i = 1}^T {{m_i}}\\) for the regular seasonality case. 5.2 State Initialization A key innovation of the De Livera, Hyndman, and Snyder (2011) paper is providing the exact initialization of the non-stationary component’s seed states, the exponential smoothing analogue of the De Jong et al. (1991) method for augmenting the Kalman filter to handle seed states with infinite variances. The proof, based on De Livera, Hyndman, and Snyder (2011) and expanded here is as follows: Let: \\[\\begin{equation} {\\bf{D}} = {\\bf{F}} - {\\bf{gw&#39;}}. \\tag{5.6} \\end{equation}\\] We eliminate \\(\\varepsilon_t\\) in (5.1) to give: \\[\\begin{equation} {{\\bf{x}}_t} = {\\bf{D}}{{\\bf{x}}_{t - 1}} + {\\bf{g}}{y_t}. \\tag{5.7} \\end{equation}\\] Next, we proceed by backsolving the equation for the error, given a given value of \\(\\lambda\\):12 \\[\\begin{equation} \\begin{array}{l} {\\varepsilon _t} = {y_t} - {\\bf{w}}{{{\\bf{\\hat x}}}_{t - 1}},\\\\ {\\varepsilon _t} = {y_t} - {\\bf{w&#39;}}\\left( {{\\bf{D}}{{{\\bf{\\hat x}}}_{t - 2}} + {\\bf{g}}{y_{t - 1}}} \\right). \\end{array} \\tag{5.8} \\end{equation}\\] Starting with \\(t = 4\\) and working backwards: \\[\\begin{equation} \\begin{array}{l} {\\varepsilon _4} &amp;= {y_4} - {\\bf{w&#39;}}\\left( {{\\bf{D}}{{{\\bf{\\hat x}}}_2} + {\\bf{g}}{y_3}} \\right)\\\\ &amp;= {y_4} - {\\bf{w&#39;}}\\left( {{\\bf{D}}\\left( {{\\bf{D}}{{{\\bf{\\hat x}}}_1} + {\\bf{g}}{y_2}} \\right) + {\\bf{g}}{y_3}} \\right)\\\\ &amp;= {y_4} - {\\bf{w&#39;}}\\left( {{\\bf{D}}\\left( {{\\bf{D}}\\left( {{\\bf{D}}{{{\\bf{\\hat x}}}_0} + {\\bf{g}}{y_1}} \\right) + {\\bf{g}}{y_2}} \\right) + {\\bf{g}}{y_3}} \\right)\\\\ &amp;= {y_4} - {\\bf{w&#39;}}\\left( {{\\bf{D}}\\left( {{{\\bf{D}}^2}{{\\bf{x}}_0} + {\\bf{Dg}}{y_1} + {\\bf{g}}{y_2}} \\right) + {\\bf{g}}{y_3}} \\right)\\\\ &amp;= {y_4} - {\\bf{w&#39;}}\\left( {{{\\bf{D}}^3}{{\\bf{x}}_0} + {{\\bf{D}}^2}{\\bf{g}}{y_1} + {\\bf{Dg}}{y_2} + {\\bf{g}}{y_3}} \\right)\\\\ &amp;= {y_4} - {\\bf{w&#39;}}\\sum\\limits_{j = 1}^3 {{{\\bf{D}}^{j - 1}}{\\bf{g}}{y_{4 - j}} - {\\bf{w&#39;}}{{\\bf{D}}^3}{{\\bf{x}}_0}} \\\\ \\end{array} \\tag{5.9} \\end{equation}\\] and generalizing to \\(\\varepsilon_t\\): \\[\\begin{equation} \\begin{array}{l} {\\varepsilon _t} &amp;= {y_t} - {\\bf{w&#39;}}\\left( {\\sum\\limits_{j = 1}^{t - 1} {{{\\bf{D}}^{j - 1}}{\\bf{g}}{y_{t - j}}} } \\right) - {\\bf{w&#39;}}{{\\bf{D}}^{t - 1}}{{\\bf{x}}_0}\\\\ &amp;= {y_t} - {\\bf{w&#39;}}{{{\\bf{\\tilde x}}}_{t - 1}} - {{{\\bf{w&#39;}}}_{t - 1}}{{\\bf{x}}_0}\\\\ &amp;= {{\\tilde y}_t} - {{{\\bf{w&#39;}}}_{t - 1}}{{\\bf{x}}_0}, \\end{array} \\tag{5.10} \\end{equation}\\] where \\({{\\tilde y}_t} = {y_t} - {\\bf{w&#39;}}{{{\\bf{\\tilde x}}}_{t - 1}}\\), \\({{{\\bf{\\tilde x}}}_t} = {\\bf{D}}{{{\\bf{\\tilde x}}}_{t - 1}} + {\\bf{g}}{y_t}\\), \\({{{\\bf{w&#39;}}}_t} = {\\bf{D}}{{{\\bf{w&#39;}}}_{t - 1}}\\), \\({{{\\bf{\\tilde x}}}_0} = 0\\) and \\({{{\\bf{w&#39;}}}_0} = {\\bf{w&#39;}}\\), so that \\(\\bf{x}_0\\) are the coefficients from the regression of \\(\\bf{w}\\) on \\(\\boldsymbol{\\varepsilon}\\). While this approach bypasses the need to estimate the initial states by augmenting the parameter vector, which could be very costly for multiple seasonality or large seasonal periods, it still requires one full iteration for \\(i=1,\\dots,t\\) to calculate \\(\\boldsymbol{\\varepsilon}\\) and \\(\\bf{w}\\) and then one inversion to get the coefficients for every new set of parameters (i.e. for each new candidate set in the optimization). 5.3 Log-Likelihood The log-likelihood (\\(L\\)) of the model follows from the assumption that the innovations \\(\\varepsilon_t\\sim N\\left(0, \\sigma^2\\right)\\), leading to the following form for the transformed series \\(y^{\\lambda}_t\\): \\[\\begin{equation} L\\left( \\theta \\right) = - \\frac{T}{2}\\log \\left( {2\\pi {{\\sigma }^2}} \\right) - \\frac{1}{{2{{\\sigma }^2}}}\\sum\\limits_{t = 1}^T {\\varepsilon _t^2 + \\left( {\\lambda - 1} \\right)\\sum\\limits_{t = 1}^T {\\log {y_t}} }. \\tag{5.11} \\end{equation}\\] Concentrating out \\(\\sigma^2\\) with its maximum likelihood estimate, \\({\\hat \\sigma^2} = {T^{ - 1}}\\sum\\limits_{t = 1}^T {\\varepsilon_t^2}\\), eliminating constants and taking the negative for minimization in the optimization routine leads to the following form: \\[\\begin{equation} L\\left( \\theta \\right) = T\\log \\sum\\limits_{t = 1}^T {\\varepsilon _t^2} - 2\\left( {\\lambda - 1} \\right)\\sum\\limits_{t = 1}^T {\\log {y_t}}, \\tag{5.12} \\end{equation}\\] where \\(\\theta\\) is the vector of parameters being optimized. The returned value of calling method logLik on an object of class tsissm.estimate is that of equation (5.11). 5.4 Package Implementation The implementation of the model in the tsissm package differs significantly from the one provided by R. Hyndman et al. (2022) in the tbats and bats functions, mainly in terms of a more flexible specification object and more complete methods for working with the model. Additionally, in order to ensure that the parameters are within the forecastability region, we constrain the characteristics roots of the matrix \\(\\bf{D}\\) in (5.2), representing the non-stationary components to be inside the unit circle, and also constrain the ARMA roots for stationarity.13 5.5 Demonstration 5.5.1 Specification The specification function defines the entry point for setting up an issm model: library(tsissm) args(issm_modelspec) ## function (y, slope = TRUE, slope_damped = FALSE, seasonal = FALSE, ## seasonal_frequency = 1, seasonal_type = c(&quot;trigonometric&quot;, ## &quot;regular&quot;), seasonal_harmonics = NULL, ar = 0, ma = 0, ## xreg = NULL, transformation = &quot;box-cox&quot;, lambda = 1, lower = 0, ## upper = 1, sampling = NULL, ...) ## NULL The specification has options for slope, dampening, seasonality (trigonometric and regular),14 AR and MA terms and external regressors. Additionally, lambda can be fixed or estimated (by setting lambda = NA). Automatic selection model selection and ranking based on AIC is available via the auto_issm function. 5.5.2 Estimation We showcase the functionality of the package using the electricload dataset from the tsdatasets package which represents total hourly electricity load in Greece in MW as published on ENTSO-E Transparency Platform, for the period 2016-10-17 to 2019-04-30. The series appears to have multiple seasonality with periodicity 24, \\(24\\times 7\\) and \\(24\\times 7 \\times 52\\), which we model with trigonometric terms. library(tsissm) data(&quot;electricload&quot;, package = &quot;tsdatasets&quot;) # specification spec &lt;- issm_modelspec(electricload, slope = T, seasonal = TRUE, seasonal_frequency = c(24, 4500), seasonal_type = &quot;trigonometric&quot;, seasonal_harmonics = c(6, 6), ar = 2, ma = 2, lambda = 0.25) # estimation mod &lt;- suppressMessages(estimate(spec, solver = &quot;nlminb&quot;)) mod$opt$elapsed ## Time difference of 2.284488 mins summary(mod) ## ISSM Model: AAA(24{6}/4500{6})+ARMA(2,2) ## ## Parameter Est[Value] Lower Upper Std. Error t value Pr(&gt;|t|) ## ------------ ----------- ------ ------ ----------- --------- --------- ## alpha 0.2961 0.00 0.99 0.1381 2.1436 0.0321 ## beta 0.0000 0.00 0.99 NaN NaN NaN ## gamma24.1 0.0345 -0.01 0.99 0.0009 38.5761 0.0000 ## gamma24.2 0.0147 -0.01 0.99 0.0006 24.9130 0.0000 ## gamma4500.1 0.1805 -0.01 0.99 0.0231 7.8308 0.0000 ## gamma4500.2 -0.0100 -0.01 0.99 NaN NaN NaN ## theta1 -0.3438 -0.99 0.99 0.0104 -33.1414 0.0000 ## theta2 -0.4971 -0.99 0.99 0.0096 -51.5499 0.0000 ## psi1 -0.1066 -0.99 0.99 0.0072 -14.7263 0.0000 ## psi2 0.7020 -0.99 0.99 0.0066 105.7564 0.0000 ## lambda 0.2500 0.25 0.25 NaN NaN NaN ## ## tsissm: Performance Metrics ## ---------------------------------- ## AIC : 437349.96 (n = 40) ## MAPE : 0.01662 ## BIAS : 0.00023 ## MSLRE : 0.00048 The plot method decomposes the estimated model into it’s components: plot(mod) but we can also extract these directly and plot them using the tsdecompose method: td &lt;- tsdecompose(mod) plot(as.zoo(td), main = &quot;ISSM Decomposition&quot;) Additional inference methods include diagnostics (tsdiagnose) as well as standard extractors for the coefficients (coef), log-likelihood (logLik) and AIC (AIC): tsdiagnose(mod, plot = TRUE) ## ## ARMA roots ## ------------------------------------------ ## Inverse AR roots: 0.7050694 0.7050694 ## Inverse MA roots: 0.8378323 0.8378323 ## ## Forecastability (D roots) ## ------------------------------------------ ## Real Eigenvalues (D): 1 1 1 1 1 1 1 1 1 1 1 1 1 0.964 0.964 0.862 0.862 0.701 0.701 0.492 0.492 0.248 0.248 0.015 0.015 0.053 0.053 0.496 0 0 ## ## Weighted Ljung-Box Test [scaled residuals] ## ------------------------------------------ ## Lag statistic pvalue ## &lt;char&gt; &lt;num&gt; &lt;num&gt; ## Lag[1] 31.8 1.7e-08 ## Lag[11] 280.3 0.0e+00 ## Lag[11] 280.3 0.0e+00 ## Lag[11] 280.3 0.0e+00 ## ## Outlier Diagnostics (based on Rosner Test) ## ------------------------------------------ ## Outliers: 2017-05-10 19:00:00 2019-01-22 14:00:00 2019-01-22 08:00:00 2019-01-22 17:00:00 2019-01-22 09:00:00 2019-01-22 05:00:00 2019-01-12 07:00:00 2017-03-02 17:00:00 2016-11-29 10:00:00 2016-11-08 17:00:00 coef(mod) ## alpha beta gamma24.1 gamma24.2 gamma4500.1 ## 2.960931e-01 1.000000e-12 3.447394e-02 1.469827e-02 1.805444e-01 ## gamma4500.2 theta1 theta2 psi1 psi2 ## -1.000000e-02 -3.437839e-01 -4.971229e-01 -1.066188e-01 7.019629e-01 logLik(mod) ## &#39;log Lik.&#39; -138954.3 (df=41) AIC(mod) ## [1] 437350 5.5.3 Prediction Similar to the other packages in the tsmodels framework, prediction builds a distribution of possible paths by simulation, outputting an object of class tsmodel.predict: p &lt;- predict(mod, h = 24*10) plot(p, n_original = 24*20, main = &quot;10-Day Hourly Forecast&quot;) The predict object also has a decomposition method: td &lt;- tsdecompose(p) par(mfrow = c(3,2),mar = c(3,3,3,3)) plot(td$Level, n_original = 24*20, main = &quot;Level[State] Predicted Distribution&quot;) plot(td$Seasonal24, n_original = 24*20, main = &quot;Seasonal24[State] Predicted Distribution&quot;) plot(td$Seasonal4500, n_original = 24*20, main = &quot;Seasonal4500[State] Predicted Distribution&quot;) plot(td$AR2, n_original = 24*20, main = &quot;AR2[State] Predicted Distribution&quot;) plot(td$MA2, n_original = 24*20, main = &quot;MA2[State] Predicted Distribution&quot;) 5.5.4 Simulation Simulation of an estimated object has options for changing the coefficients as well as the initial states, as well as the option for providing custom innovations or bootstrapped innovations: args(tsissm:::simulate.tsissm.estimate) ## function (object, nsim = 1, seed = NULL, h = NULL, newxreg = NULL, ## sim_dates = NULL, bootstrap = FALSE, innov = NULL, sigma_scale = 1, ## pars = coef(object), init_states = object$spec$xseed, ...) ## NULL sim &lt;- simulate(mod, nsim = 100, h = 24 * 10, bootstrap = TRUE) plot(sim) While the plot function on a simulated object provides a decomposition of the actual and state components distributions, it is useful to remind ourselves that the distribution bands represent the range of uncertainty of multiple paths. This is best illustrated by plotting these separately: matplot(as.POSIXct(colnames(sim$Simulated)), t(sim$Simulated), type = &quot;l&quot;, col = 1:100, ylab = &quot;&quot;, xlab = &quot;&quot;, main = &quot;Simulated Paths&quot;) grid() 5.5.5 Filtering Online filtering is when new data arrives and instead of re-estimating the model, we instead just filter the new data based on an existing model. In the tsissm package the tsfilter method updates an object of class tsmodel.estimate with new data as the example below illustrates. Because the class of the model is retained and only updated (both data and states) with new information, it is also possible to apply any method to that which admits that object (e.g. predict). library(tsissm) suppressMessages(library(xts)) weekends &lt;- xts(matrix(0, ncol = 1, nrow = nrow(electricload)), index(electricload)) weekends[which(weekdays(as.Date(index(weekends))) %in% c(&quot;Saturday&quot;,&quot;Sunday&quot;))] &lt;- 1 colnames(weekends) &lt;- &quot;weekend&quot; spec &lt;- issm_modelspec(electricload[1:22000], slope = TRUE, seasonal = TRUE, seasonal_frequency = c(24, 4500), xreg = weekends[1:22000], seasonal_type = &quot;trigonometric&quot;, seasonal_harmonics = c(6, 6), ar = 2, ma = 2, lambda = 0.25) mod &lt;- estimate(spec, solver = &quot;nlminb&quot;) f1 &lt;- tsfilter(mod, y = electricload[22001:22100], newxreg = weekends[22001:22100]) tail(fitted(mod)) ## [,1] ## 2019-04-21 10:00:00 5602.398 ## 2019-04-21 11:00:00 5505.524 ## 2019-04-21 12:00:00 5089.799 ## 2019-04-21 13:00:00 4911.070 ## 2019-04-21 14:00:00 4831.108 ## 2019-04-21 15:00:00 5042.946 head(tail(fitted(f1), 100),10) ## [,1] ## 2019-04-21 16:00:00 5446.961 ## 2019-04-21 17:00:00 5795.905 ## 2019-04-21 18:00:00 6158.819 ## 2019-04-21 19:00:00 5908.761 ## 2019-04-21 20:00:00 5284.804 ## 2019-04-21 21:00:00 4888.972 ## 2019-04-21 22:00:00 4499.164 ## 2019-04-21 23:00:00 4216.602 ## 2019-04-22 00:00:00 4185.699 ## 2019-04-22 01:00:00 4105.256 plot(f1) f2 &lt;- tsfilter(f1, y = electricload[22100:22200], newxreg = weekends[22100:22200]) tail(fitted(f1)) ## [,1] ## 2019-04-25 14:00:00 5741.715 ## 2019-04-25 15:00:00 5759.517 ## 2019-04-25 16:00:00 5970.261 ## 2019-04-25 17:00:00 6054.623 ## 2019-04-25 18:00:00 6082.171 ## 2019-04-25 19:00:00 5672.456 head(tail(fitted(f2), 100),10) ## [,1] ## 2019-04-25 20:00:00 5079.625 ## 2019-04-25 21:00:00 4757.516 ## 2019-04-25 22:00:00 4393.659 ## 2019-04-25 23:00:00 4187.004 ## 2019-04-26 00:00:00 4145.853 ## 2019-04-26 01:00:00 4059.955 ## 2019-04-26 02:00:00 3973.271 ## 2019-04-26 03:00:00 4221.322 ## 2019-04-26 04:00:00 4393.471 ## 2019-04-26 05:00:00 4598.862 5.5.6 Profiling The tsprofile function profiles an estimated model by simulating and then estimating multiple paths from the assumed DGP while leaving h values out for prediction evaluation. Each simulated path is equal to the size of the original dataset plus h additional values, and initialized with the initial state vector from the model. The resulting output contains the distribution of the MAPE, percent bias (BIAS) and mean squared log relative error (MSLRE) per horizon h. Since these matrices are of class tsmodel.distribution they can be readily plotted with the special purpose plot function for this class from the tsmethods package. Additionally, a data.table matrix is also returned with the distribution of the coefficients from each path estimation. As of version 0.2.0, the tsprofile method on a tsissm.estimate object does not support newxreg. spec &lt;- issm_modelspec(electricload[1:(24*24)], slope = T, seasonal = TRUE, seasonal_frequency = c(24), seasonal_type = &quot;trigonometric&quot;, seasonal_harmonics = c(6), ar = 2, ma = 2, lambda = 0.25) mod &lt;- estimate(spec, solver = &quot;nlminb&quot;) prof &lt;- tsprofile(mod, h = 24*7, nsim = 100, solver = &quot;nlminb&quot;) ## Warning: UNRELIABLE VALUE: Future (&#39;&lt;none&gt;&#39;) unexpectedly generated random ## numbers without specifying argument &#39;seed&#39;. There is a risk that those random ## numbers are not statistically sound and the overall results might be invalid. ## To fix this, specify &#39;seed=TRUE&#39;. This ensures that proper, parallel-safe random ## numbers are produced via the L&#39;Ecuyer-CMRG method. To disable this check, use ## &#39;seed=NULL&#39;, or set option &#39;future.rng.onMisuse&#39; to &quot;ignore&quot;. plot(prof) plot(prof, type = &quot;coef&quot;) 5.5.7 Backtesting The tsbacktest method generates an expanding window walk forward backtest, returning a list with the estimation/horizon predictions against actuals as well as a table of average performance metrics by horizon. Optionally, instead of re-estimating every one period, the option to re-estimate every n-periods is available via the estimate_every option. suppressMessages(library(future)) plan(list( tweak(sequential), tweak(multisession, workers = 5) )) spec &lt;- issm_modelspec(electricload[1:(24*24)], slope = T, seasonal = TRUE, seasonal_frequency = c(24), seasonal_type = &quot;trigonometric&quot;, seasonal_harmonics = c(6), ar = 2, ma = 2, lambda = 0.25) b &lt;- tsbacktest(spec, start = 24*12, h = 24, estimate_every = 24, alpha = c(0.02, 0.1), data_name = &quot;electricity&quot;, solver = &quot;nlminb&quot;, autodiff = TRUE, trace = FALSE) ## Warning: UNRELIABLE VALUE: Future (&#39;&lt;none&gt;&#39;) unexpectedly generated random ## numbers without specifying argument &#39;seed&#39;. There is a risk that those random ## numbers are not statistically sound and the overall results might be invalid. ## To fix this, specify &#39;seed=TRUE&#39;. This ensures that proper, parallel-safe random ## numbers are produced via the L&#39;Ecuyer-CMRG method. To disable this check, use ## &#39;seed=NULL&#39;, or set option &#39;future.rng.onMisuse&#39; to &quot;ignore&quot;. head(b$metrics) ## horizon variable MAPE MSLRE BIAS n MIS[0.02] ## &lt;int&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;int&gt; &lt;num&gt; ## 1: 1 y 0.04190210 0.003309235 -0.0236839237 12 5998.228 ## 2: 2 y 0.04867266 0.004278126 -0.0137657649 12 6205.653 ## 3: 3 y 0.06539931 0.007050628 0.0008472191 12 7406.479 ## 4: 4 y 0.07501513 0.008072229 -0.0090672231 12 5777.668 ## 5: 5 y 0.07535560 0.009468918 -0.0222899079 12 7845.072 ## 6: 6 y 0.09600599 0.013778439 -0.0254790898 12 12528.816 ## MIS[0.1] ## &lt;num&gt; ## 1: 1774.305 ## 2: 2028.161 ## 3: 2361.633 ## 4: 2365.149 ## 5: 3083.159 ## 6: 4400.748 plot(b$metrics$horizon, b$metrics$MAPE*100, main = &quot;Horizon MAPE&quot;, type = &quot;l&quot;, ylab = &quot;MAPE[%]&quot;, xlab = &quot;horizon&quot;) grid() plan(sequential) The variable n in the table reports the number of h-step ahead predictions made on which the average metrics were calculated. 5.5.8 Benchmarking The tsbenchmark* method is used to benchmark a model for timing and accuracy and can be used as a unit testing function. spec &lt;- issm_modelspec(electricload[1:(24*50)], slope = T, seasonal = TRUE, seasonal_frequency = c(24), seasonal_type = &quot;trigonometric&quot;, seasonal_harmonics = c(6), ar = 2, ma = 2, lambda = 0.25) bench &lt;- tsbenchmark(spec, solver = &quot;nlminb&quot;, autodiff = TRUE) print(bench) ## start end spec ## &lt;POSc&gt; &lt;POSc&gt; &lt;list&gt; ## 1: 2022-10-13 15:05:26 2022-10-13 15:05:27 &lt;tsissm.spec[11]&gt; ## estimate solver control loglik ## &lt;list&gt; &lt;char&gt; &lt;list&gt; &lt;num&gt; ## 1: &lt;tsissm.estimate[6]&gt; nlminb 0 -7377.924 print(bench$end - bench$start) ## Time difference of 0.8560059 secs The implementation of the constraints for both the \\(\\bf{D}\\) matrix as well as the ARMA roots are non-smooth and hence care should be taken in checking the solution. References "],["tsvets.html", "Chapter 6 tsvets package 6.1 Introduction 6.2 Inclusion of External Regressors 6.3 Log-Likelihood 6.4 Dependence Structure 6.5 Grouping and Pooling 6.6 Homogeneous Coefficients and Aggregation 6.7 Demonstration", " Chapter 6 tsvets package 6.1 Introduction The vector exponential additive smoothing model (Vector ETS), introduced in De Silva, Hyndman, and Snyder (2010) naturally generalizes the univariate framework with a great deal of flexibility with the dynamics of the unobserved components. The dynamics of the states can be common, diagonal, grouped or fully parameterized, allowing for a rich set of patterns to be captured. The tsvets package implements a more general and flexible version based on Athanasopoulos and Silva (2012), with methods for estimation, inference, visualization, forecasting as well as aggregation. As of version 0.4.3, missing values are allowed in the series. This works by zeroing our the errors (i.e. taking the expectation) during the step update step for those series with missingness at time t, whilst the covariance and likelihood is calculated on the joint non-missingness data. Let the vector of \\(N\\) series at time \\(t\\), \\(\\bf{y}_t\\) be represented as a linear additive combination of their unobserved level, slope and seasonal (with frequency \\(m\\)) components, \\(\\bf{l}_{t-1}\\), \\(\\bf{b}_{t-1}\\) and \\(\\bf{s}_{t-m}\\) respectively. Formally, the conditional mean of \\(\\bf{\\hat y}_t\\) is given by: \\[\\begin{equation} \\bf{\\hat y}_t = \\bf{l}_{t - 1} + \\Phi\\bf{b}_{t - 1} + \\bf{s}_{t - m}, \\tag{6.1} \\end{equation}\\] where \\(\\Phi\\) is the \\(N\\times N\\) matrix of dampening parameters. In its reduced form, the model without seasonality is equivalent to a VARIMA(1,2,2) model,15 and \\(\\Phi\\) becomes the matrix of first order autoregressive coefficients. The 1 step ahead forecast errors, \\(\\boldsymbol{\\varepsilon}_t\\) follow a multivariate normal distribution: \\[\\begin{equation} {\\boldsymbol{\\varepsilon} _t} = {\\bf{y}_t} - {{\\bf{\\hat y}}_t},\\quad {\\boldsymbol{\\varepsilon} _t} \\sim {\\bf{N}}\\left( {{\\bf{0}},\\Sigma } \\right). \\tag{6.2} \\end{equation}\\] The state equations have the following dynamics: \\[\\begin{equation} \\begin{array}{l} {\\bf{l}_t} = {\\bf{l}_{t - 1}} + \\Phi {\\bf{b}_{t - 1}} + {\\bf{A}}{\\boldsymbol{\\varepsilon}_t},\\\\ {\\bf{b}_t} = \\Phi {\\bf{b}_{t - 1}} + {\\bf{B}}{\\boldsymbol{\\varepsilon}_t},\\\\ {\\bf{s}_t} = {\\bf{s}_{t - m}} + {G_1}K{\\boldsymbol{\\varepsilon}_t},\\\\ {\\bf{s}_{t - m}} = {\\bf{s}_{t - i}} + {G_2}K{\\boldsymbol{\\varepsilon}_t}, \\end{array} \\tag{6.3} \\end{equation}\\] where the matrices \\(\\mathbf{A}\\), \\(\\mathbf{B}\\) and \\(\\mathbf{K}\\) represent the adjustment of the vector components to the errors, and can be diagonal, fully parameterized or scalar (common adjustment). In vector innovations state space form, the system can be written as follows: \\[\\begin{equation} \\begin{array}{l} {\\bf{y}_t} = H{\\bf{x}_{t - 1}} + {\\boldsymbol{\\varepsilon}_t},\\\\ {\\bf{x}_t} = F{\\bf{x}_{t - 1}} + GA{\\boldsymbol{\\varepsilon}_t} \\end{array} \\tag{6.4} \\end{equation}\\] where the matrices \\(H\\), \\(F\\), \\(G\\) and \\(A\\) are composed as follows: \\[\\begin{equation} \\mathop H\\limits_{\\left( {m + 2} \\right)N \\times N} = \\left[ {\\begin{array}{*{20}{c}} {{I_N}}\\\\ {{I_N}}\\\\ {{0_{N \\times N}}}\\\\ \\vdots \\\\ {{0_{N \\times N}}}\\\\ {{I_N}} \\end{array}} \\right],\\quad \\mathop A\\limits_{\\left( {m + 2} \\right)N \\times N} = \\left[ {\\begin{array}{*{20}{c}} {\\bf{A}}\\\\ {\\bf{B}}\\\\ {\\bf{K}}\\\\ \\vdots \\\\ \\vdots \\\\ {\\bf{K}} \\end{array}} \\right], \\tag{6.5} \\end{equation}\\] \\[\\begin{equation} \\mathop F\\limits_{\\left( {m + 2} \\right)N \\times \\left( {m + 2} \\right)N} = \\left[ {\\begin{array}{*{20}{c}} {{I_N}}&amp;{{I_N}}&amp;{{0_{N \\times mN}}}\\\\ {{0_{N \\times N}}}&amp;{{\\Phi _N}}&amp;{{0_{N \\times mN}}}\\\\ {{0_{mN \\times N}}}&amp;{{0_{mN \\times N}}}&amp;{\\tilde{F} \\otimes {I_N}} \\end{array}} \\right],\\mathop {\\tilde{F}}\\limits_{m \\times m} = \\left[ {\\begin{array}{*{20}{c}} 0&amp;0&amp;0&amp; \\cdots &amp;0&amp;1\\\\ 1&amp;0&amp;0&amp; \\cdots &amp;0&amp;0\\\\ 0&amp;1&amp;0&amp; \\cdots &amp;0&amp;0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0&amp;0&amp;0&amp; \\cdots &amp;1&amp;0 \\end{array}} \\right], \\tag{6.6} \\end{equation}\\] \\[\\begin{equation} \\mathop G\\limits_{\\left( {m + 2} \\right)N \\times \\left( {m + 2} \\right)N} = \\left[ {\\begin{array}{*{20}{c}} {{I_N}}&amp;{{0_{N \\times N}}}&amp;{{0_{N \\times mN}}}\\\\ {{0_{N \\times N}}}&amp;{{I_N}}&amp;{{0_{N \\times mN}}}\\\\ {{0_{mN \\times N}}}&amp;{{0_{mN \\times N}}}&amp;{\\tilde{G} \\otimes {I_N}} \\end{array}} \\right], \\tag{6.7} \\end{equation}\\] \\[\\begin{equation} \\mathop {\\tilde{G}}\\limits_{m \\times m} = \\left[ {\\begin{array}{*{20}{c}} {\\frac{{m - 1}}{m}}&amp;0&amp; \\cdots &amp;0&amp;0\\\\ 0&amp;{ - \\frac{1}{m}}&amp; \\cdots &amp;0&amp;0\\\\ 0&amp;0&amp;{ - \\frac{1}{m}}&amp;0&amp;0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0&amp;0&amp;0&amp; \\cdots &amp;{ - \\frac{1}{m}} \\end{array}} \\right], \\tag{6.8} \\end{equation}\\] \\[\\begin{equation} \\mathop {{\\bf{x}_t}}\\limits_{\\left( {m + 2} \\right)N \\times N} = \\left[ {\\begin{array}{*{20}{c}} {{\\bf{l}_t}}\\\\ {{\\bf{b}_t}}\\\\ {{\\bf{s}_t}}\\\\ \\vdots \\\\ {{\\bf{s}_{t - m + 2}}}\\\\ {{\\bf{s}_{t - m + 1}}} \\end{array}} \\right]. \\tag{6.9} \\end{equation}\\] The values in the seasonal matrix \\(\\tilde{G}\\) represent normalization terms which ensure that the seasonal component adds to zero throughout the updating process without becoming contaminated by the level component. 6.2 Inclusion of External Regressors We augment the model with the ability to include external regressors (\\(z_t\\)) such that: \\[\\begin{equation} \\begin{array}{l} {\\bf{y}_t} = H{\\bf{x}_{t - 1}} + \\mathbf{W}\\bf{z}_t + {\\boldsymbol{\\varepsilon}_t},\\\\ {\\bf{x}_t} = F{\\bf{x}_{t - 1}} + GA{\\boldsymbol{\\varepsilon}_t}, \\end{array} \\tag{6.10} \\end{equation}\\] where the \\(W\\) is the matrix of coefficients which are time invariant. The xreg_include argument in the vets_modelspec function is a design matrix which allows one to define which coefficients should be set to zero, which should be estimated individually as well as which should be pooled (see the documentation for more details). The index on \\(\\bf{z}\\) is \\(t\\) and not \\(t-1\\) and it is left to the user to pre-lag any regressors passed to the function. 6.3 Log-Likelihood The log-likelihood (\\(L\\)) of the model follows from the assumption that the innovations \\(\\boldsymbol{\\varepsilon_t}\\sim N\\left(\\bf{0}, \\Sigma\\right)\\), leading to the following function: \\[\\begin{equation} L\\left(\\boldsymbol{\\theta} \\right) = \\frac{T}{2}\\left( {N\\log \\left( {2\\pi } \\right) + N\\log \\left| \\Sigma \\right|} \\right) - \\frac{1}{2}\\sum\\limits_{t = 1}^T {\\boldsymbol{\\varepsilon_t}{\\Sigma ^{ - 1}}\\boldsymbol{\\varepsilon_t}}, \\tag{6.11} \\end{equation}\\] where \\(\\boldsymbol{\\theta}\\) is the vector of parameters being optimized. We also concentrate out the parameters of the covariance matrix by using its ML estimator \\[\\begin{equation} \\hat \\Sigma = \\frac{1}{T}\\sum\\limits_{t = 1}^T {{\\boldsymbol{\\varepsilon}_t}{{\\boldsymbol{\\varepsilon}&#39;}_t}}. \\tag{6.12} \\end{equation}\\] Therefore, the vector ETS log-likelihood is proportional to \\[\\begin{equation} L\\left(\\boldsymbol{\\theta} \\right) \\propto \\sum\\limits_{t = 1}^T {{{\\boldsymbol{\\varepsilon}&#39;}_t}{{\\hat \\Sigma }^{ - 1}}{\\boldsymbol{\\varepsilon}_t}}, \\tag{6.13} \\end{equation}\\] which requires looping through each \\(t\\) to calculate the quadratic form which is expensive. Instead, we can make use of the following relationship, assuming positive-definite or positive-semi-definite \\(\\hat \\Sigma\\), \\({\\hat\\Sigma ^{-1}} = Q\\Lambda^{-1} Q&#39;\\), where \\(\\Lambda\\) is the diagonal matrix of eigenvalues of \\(\\hat\\Sigma\\). We then have \\({\\varepsilon _t} = Q{\\bf{u}_t}\\) where \\(\\bf{u}_t\\) are the projections onto the eigenvectors \\(Q\\). The negative of the log-likelihood can be represented as: \\[\\begin{equation} L\\left( \\boldsymbol{\\theta} \\right) = \\frac{1}{2}T\\left( {N\\log 2\\pi - \\log \\left| {\\hat \\Sigma } \\right|} \\right) + \\frac{1}{2}{{{\\bf{1&#39;}}}_T}\\left( {{{\\left( {\\varepsilon Q} \\right)}^2}\\frac{1}{\\lambda }} \\right), \\tag{6.14} \\end{equation}\\] where we have used the relation \\(Q^{-1} = Q&#39;\\) due to \\(\\hat{\\Sigma}\\) being symmetric (and thus \\(Q\\) is orthogonal), and \\(1 / \\lambda\\) is a vector containing the reporicals of the eigenvalues. Additionally, we constrain the \\(N+1\\) largest eigenvalues (\\(\\lambda_s\\)) to be less than 1 to ensure invertibility of the system, such that \\[\\begin{equation} {\\lambda _s}\\left( D \\right) &lt; 1,D = F - GAH. \\tag{6.15} \\end{equation}\\] This is added as a soft barrier constraint. The diagonal elements of the level, slope, dampening and seasonal matrices are bounded between 0 and 1, while the off diagonal elements are allowed to vary between -1 and 1.16 Finally, the initial seed values for each of the states are approximated using the heuristic approach from the univariate ETS model as described in Section 4.5.2. 6.4 Dependence Structure While Equation (5.10) assumes a full covariance matrix, allowing contemporaneous associations among the residuals, we also offer 3 additional estimators for the dependence structure: diagonal covariance, equicorrelation and shrinkage covariance based on the estimator of Ledoit and Wolf (2004). 6.4.1 Diagonal Covariance The diagonal covariance matrix is the one used by Athanasopoulos and Silva (2012), and leads to the fastest estimation. In this case, the log-likelihood is greatly simplified and equal to \\[\\begin{equation} \\frac{T}{2}\\left( {N\\log \\left( {2\\pi } \\right) + N\\log \\left| \\Sigma \\right|} \\right) - \\frac{1}{2}\\sum\\limits_{t = 1}^T {\\sum\\limits_{i = 1}^N {\\varepsilon _{it}^2/\\sigma _i^2} }. \\tag{6.16} \\end{equation}\\] 6.4.2 Equicorrelation The equicorrelation covariance assumes that the correlation across all series is set to some common value \\(\\rho\\). The correlation matrix \\(\\bf{R}\\) can be calculated as \\[\\begin{equation} {\\bf{R}} = \\rho {\\bf{11&#39;}} + \\left( {1 - \\rho } \\right){\\bf{I}}, \\tag{6.17} \\end{equation}\\] which is guaranteed to be positive definite as long as \\(-\\frac{1}{{N - 1}}&lt;\\rho&lt; 1\\). The covariance is then equal to: \\[\\begin{equation} {\\Sigma} = {\\bf{DRD&#39;}}, \\tag{6.18} \\end{equation}\\] where \\({\\bf{D}} = diag\\left( {{\\hat \\sigma _1},\\dots,{\\hat \\sigma _n}} \\right)\\). Some of the advantages of assuming equicorrelation are discussed in Clements, Scott, and Silvennoinen (2015). 6.4.3 Shrinkage Covariance The shrinkage estimator of Ledoit and Wolf (2004) follows from the observation that the eigenvalues of the estimated correlations tend to be more dispersed than the eigenvalues of the true data generating process. The shrinkage estimator of the covariance is based on a convex combination of the sample covariance \\(\\hat\\Sigma\\) and a target covariance set to a multiple of the identity matrix. It is this combination weight \\(\\rho\\) which we estimate in the case of the shrinkage estimator \\[\\begin{equation} {\\Sigma} = \\left( {1 - \\rho } \\right)\\Sigma + \\frac{\\rho }{n}tr\\left( \\Sigma \\right){\\bf{I}}. \\tag{6.18} \\end{equation}\\] 6.5 Grouping and Pooling The tsvets package allows both global pooling of state component coefficients as well as group-wise pooling. For instance, if we had a large-dimensional system composed of series which have some common grouping structure (e.g. geographical, feature or statistical based), we could impose that these groups have common dynamics for some or all of the components. We provide an example of this in the demonstration section. 6.6 Homogeneous Coefficients and Aggregation When a model is estimated with all components pooled (i.e. common coefficients), then we can aggregate the model to obtain an aggregated representation in closed form, following Section 17.1.2 of R. Hyndman et al. (2008). When this is not the case, we can still obtain an aggregated series from the estimation and prediction objects. This functionality is implemented via the tsaggregate method and we provide an example in the demonstration section. 6.7 Demonstration 6.7.1 Specification The specification function defines the entry point for setting up a vets model: suppressWarnings(suppressPackageStartupMessages(library(tsvets))) args(vets_modelspec) ## function (y, level = c(&quot;constant&quot;, &quot;diagonal&quot;, &quot;common&quot;, &quot;full&quot;, ## &quot;grouped&quot;), slope = c(&quot;none&quot;, &quot;constant&quot;, &quot;common&quot;, &quot;diagonal&quot;, ## &quot;full&quot;, &quot;grouped&quot;), damped = c(&quot;none&quot;, &quot;common&quot;, &quot;diagonal&quot;, ## &quot;full&quot;, &quot;grouped&quot;), seasonal = c(&quot;none&quot;, &quot;common&quot;, &quot;diagonal&quot;, ## &quot;full&quot;, &quot;grouped&quot;), group = NULL, xreg = NULL, xreg_include = NULL, ## frequency = 1, transformation = &quot;box-cox&quot;, lambda = NULL, ## lower = 0, upper = 1, dependence = c(&quot;diagonal&quot;, &quot;full&quot;, ## &quot;equicorrelation&quot;, &quot;shrinkage&quot;)) ## NULL The specification has options for how the components of level, slope, dampening, seasonality are structured as well as the type of dependence to use. We also allow multicore processing since we require the initial state vectors to be calculated through calls to the tsets package, which can be done in parallel. The lambda argument can be set either to NA, in which case the multivariate version of the Box Cox transformation is used which targets a transformation to multivariate normality based on Velilla (1993) using the powerTransform function from the car package of Fox, Weisberg, and Price (2022), a vector of length equal to the number of series of a single number to apply to all series, and NULL in which case no transformation is performed. The next sections provide fully worked examples with methods showcasing the functionality of the package under different assumptions. 6.7.2 Example: Australian Retail Sales We use a subset of the Australian retail dataset from package tsdatasets representing the monthly retail turnover in $Million AUD across different regions for the news vendor category, with common level, constant slope and diagonal seasonal and dependence structure. suppressWarnings(suppressPackageStartupMessages(library(tsvets))) suppressMessages(library(tsmethods)) suppressMessages(library(tsaux)) suppressMessages(library(xts)) data(&quot;austretail&quot;, package = &quot;tsdatasets&quot;) y &lt;- na.omit(austretail[,grepl(&quot;NEWS&quot;, colnames(austretail))]) spec &lt;- vets_modelspec(y, level = &quot;common&quot;, slope = &quot;constant&quot;, damped = &quot;none&quot;, seasonal = &quot;diagonal&quot;, lambda = NA, dependence = &quot;diagonal&quot;, frequency = 12) mod &lt;- estimate(spec, solver = &quot;solnp&quot;, control = list(trace = 0)) The joint estimation of the 8 series takes about 0.6130779 seconds. The summary object prints the full matrices for each component using the Matrix package of Bates, Maechler, and Jagan (2022). The summary method also take an optional weights argument which is used to calculate the weighted Accuracy Criteria, and when this is NULL, an equal weight vector is used instead (and hence equivalent to the Mean Criteria). Similar to other packages, there is a diagnostics method tsdiagnose which prints the eiganvalues of the \\(D\\) matrix as well as the output from a multivariate Normality Test and Multivariate Outliers based on the mvn function of the MVN package of Korkmaz, Goksuluk, and Zararsiz (2021). summary(mod) ## ## Vector ETS ## ----------------------------------- ## Level : common ## Slope : constant ## Seasonal : diagonal ## Dependence : diagonal ## No. Series : 8 ## No. TimePoints : 369 ## ## Parameter Matrices ## ## Level Matrix ## 8 x 8 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 0.7237793 . . . . . . ## [2,] . 0.7237793 . . . . . ## [3,] . . 0.7237793 . . . . ## [4,] . . . 0.7237793 . . . ## [5,] . . . . 0.7237793 . . ## [6,] . . . . . 0.7237793 . ## [7,] . . . . . . 0.7237793 ## [8,] . . . . . . . ## [,8] ## [1,] . ## [2,] . ## [3,] . ## [4,] . ## [5,] . ## [6,] . ## [7,] . ## [8,] 0.7237793 ## ## Slope Matrix ## 8 x 8 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 0 . . . . . . . ## [2,] . 0 . . . . . . ## [3,] . . 0 . . . . . ## [4,] . . . 0 . . . . ## [5,] . . . . 0 . . . ## [6,] . . . . . 0 . . ## [7,] . . . . . . 0 . ## [8,] . . . . . . . 0 ## ## Seasonal Matrix ## 8 x 8 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.2884931 . . . . . ## [2,] . 2.396104e-09 . . . . ## [3,] . . 3.553894e-09 . . . ## [4,] . . . 6.317263e-09 . . ## [5,] . . . . 4.995968e-09 . ## [6,] . . . . . 6.078841e-09 ## [7,] . . . . . . ## [8,] . . . . . . ## [,7] [,8] ## [1,] . . ## [2,] . . ## [3,] . . ## [4,] . . ## [5,] . . ## [6,] . . ## [7,] 0.1520137 . ## [8,] . 3.009261e-09 ## ## Correlation Matrix ## 8 x 8 diagonal matrix of class &quot;ddiMatrix&quot; ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS SA.NEWS T.NEWS V.NEWS WA.NEWS ## ACF.NEWS 1 . . . . . . . ## NSW.NEWS . 1 . . . . . . ## NT.NEWS . . 1 . . . . . ## Q.NEWS . . . 1 . . . . ## SA.NEWS . . . . 1 . . . ## T.NEWS . . . . . 1 . . ## V.NEWS . . . . . . 1 . ## WA.NEWS . . . . . . . 1 ## ## Information Criteria ## AIC BIC AICc ## 1536.99 2261.81 1547.42 ## ## Accuracy Criteria ## Mean Weighted ## MAPE 0.0613 0.0613 ## MSLRE 0.0067 0.0067 Additional methods are similar to what is available in other packages. Coefficients: coef(mod) ## Level[Common] Seasonal[ACF.NEWS] Seasonal[NSW.NEWS] Seasonal[NT.NEWS] ## 7.237793e-01 2.884931e-01 2.396104e-09 3.553894e-09 ## Seasonal[Q.NEWS] Seasonal[SA.NEWS] Seasonal[T.NEWS] Seasonal[V.NEWS] ## 6.317263e-09 4.995968e-09 6.078841e-09 1.520137e-01 ## Seasonal[WA.NEWS] ## 3.009261e-09 Loglikelihood and AIC: logLik(mod) ## &#39;log Lik.&#39; 1294.991 (df=121) AIC(mod) ## [1] 1536.991 Performance metrics with optional weights option: tsmetrics(mod, weights = runif(8)) ## N no_pars LogLik AIC BIC AICc MAPE MSLRE ## 1 2952 121 1294.991 1536.991 2261.81 1547.424 0.06131029 0.00673102 ## WAPE WSLRE ## 1 0.06051585 0.006598354 tscor(mod) ## 8 x 8 diagonal matrix of class &quot;ddiMatrix&quot; ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS SA.NEWS T.NEWS V.NEWS WA.NEWS ## ACF.NEWS 1 . . . . . . . ## NSW.NEWS . 1 . . . . . . ## NT.NEWS . . 1 . . . . . ## Q.NEWS . . . 1 . . . . ## SA.NEWS . . . . 1 . . . ## T.NEWS . . . . . 1 . . ## V.NEWS . . . . . . 1 . ## WA.NEWS . . . . . . . 1 Correlation and covariance matrices: tscov(mod) ## 8 x 8 diagonal matrix of class &quot;ddiMatrix&quot; ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS SA.NEWS T.NEWS ## ACF.NEWS 0.0089999 . . . . . ## NSW.NEWS . 0.003625678 . . . . ## NT.NEWS . . 0.03950474 . . . ## Q.NEWS . . . 0.004023731 . . ## SA.NEWS . . . . 0.006397141 . ## T.NEWS . . . . . 0.03014232 ## V.NEWS . . . . . . ## WA.NEWS . . . . . . ## V.NEWS WA.NEWS ## ACF.NEWS . . ## NSW.NEWS . . ## NT.NEWS . . ## Q.NEWS . . ## SA.NEWS . . ## T.NEWS . . ## V.NEWS 0.004275613 . ## WA.NEWS . 0.02804386 tscor(mod) ## 8 x 8 diagonal matrix of class &quot;ddiMatrix&quot; ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS SA.NEWS T.NEWS V.NEWS WA.NEWS ## ACF.NEWS 1 . . . . . . . ## NSW.NEWS . 1 . . . . . . ## NT.NEWS . . 1 . . . . . ## Q.NEWS . . . 1 . . . . ## SA.NEWS . . . . 1 . . . ## T.NEWS . . . . . 1 . . ## V.NEWS . . . . . . 1 . ## WA.NEWS . . . . . . . 1 Note that, even though we impose a diagonal covariance matrix structure, we still return the full covariance matrix in tscov. Fitted and residuals: head(fitted(mod)[,1:4], 4) ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS ## 1988-04-30 5.083160 73.64485 1.826273 32.42631 ## 1988-05-31 5.678305 77.54145 1.992810 33.63348 ## 1988-06-30 5.068541 74.72022 2.038204 34.26928 ## 1988-07-31 4.932006 80.55169 2.504371 36.89154 head(residuals(mod)[,1:4], 4) ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS ## 1988-04-30 0.01683990 -0.6448491 -0.026272664 -0.6263074 ## 1988-05-31 -0.07830457 2.7585476 0.007190382 3.0665182 ## 1988-06-30 -0.06854076 0.5797782 0.361795877 0.4307202 ## 1988-07-31 -0.03200619 -6.3516878 -0.104371326 2.2084582 head(residuals(mod, raw = TRUE)[,1:4], 4) ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS ## 1988-04-30 0.003307404 -0.008794762 -0.021775194 -0.01950376 ## 1988-05-31 -0.013886100 0.034956958 0.005781138 0.08725470 ## 1988-06-30 -0.013615046 0.007729372 0.281470559 0.01249037 ## 1988-07-31 -0.006510636 -0.082134913 -0.078646140 0.05814016 where argument raw denotes the non backtransformed residuals (if a Box Cox calculation was used). States decomposition: tsx &lt;- tsdecompose(mod) head(tsx$Level[,1:4], 4) ## Level[ACF.NEWS] Level[NSW.NEWS] Level[NT.NEWS] Level[Q.NEWS] ## 1988-04-30 1.691734 4.364045 0.9000652 3.546509 ## 1988-05-31 1.681585 4.389578 0.9021136 3.610841 ## 1988-06-30 1.671633 4.395405 1.1037003 3.621061 ## 1988-07-31 1.666822 4.336189 1.0446420 3.664320 head(tsx$Slope[,1:4], 4) ## Slope[ACF.NEWS] Slope[NSW.NEWS] Slope[NT.NEWS] Slope[Q.NEWS] ## 1988-04-30 -9.831044e-05 0.0002321412 -0.002135869 0.001179101 ## 1988-05-31 -9.831044e-05 0.0002321412 -0.002135869 0.001179101 ## 1988-06-30 -9.831044e-05 0.0002321412 -0.002135869 0.001179101 ## 1988-07-31 -9.831044e-05 0.0002321412 -0.002135869 0.001179101 head(tsx$Seasonal[,1:4], 4) ## Seasonal[ACF.NEWS] Seasonal[NSW.NEWS] Seasonal[NT.NEWS] ## 1988-04-30 0.045016743 -0.013464562 -0.01673148 ## 1988-05-31 -0.058434214 -0.076059727 0.01760856 ## 1988-06-30 -0.075788732 -0.006737928 0.17613856 ## 1988-07-31 -0.002813842 0.020157397 0.15300637 ## Seasonal[Q.NEWS] ## 1988-04-30 -0.032166030 ## 1988-05-31 -0.077771015 ## 1988-06-30 -0.014257399 ## 1988-07-31 0.009366033 Plot methods exist outputting the fitted values, the states and the residuals, with an argument for the series to output, with a maximum number per call of 10 series: plot(mod) plot(mod, type = &quot;states&quot;) plot(mod, type = &quot;residuals&quot;) The predict function will output a list which includes data.table of each series’ prediction object and state decomposition: p &lt;- predict(mod, h = 12) p$prediction_table ## series Level Slope Seasonal ## &lt;char&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1: ACF.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 2: NSW.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 3: NT.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 4: Q.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 5: SA.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 6: T.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 7: V.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 8: WA.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## X Error Predicted ## &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1: &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 2: &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 3: &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 4: &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 5: &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 6: &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 7: &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 8: &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; The plot method on the predicted object can output 1 series at a time using the series argument: plot(p, series = 2) The simulation method also returns a list with a data.table object with each series’ simulation object and state decomposition. Additionally, there are optional arguments for the initial state to use (init_states) and parameter vector (pars). s &lt;- simulate(mod, h = 12*8, nsim = 100, init_states = mod$spec$vets_env$States[,1]) s$simulation_table ## series Level Slope Seasonal ## &lt;char&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1: ACF.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 2: NSW.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 3: NT.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 4: Q.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 5: SA.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 6: T.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 7: V.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 8: WA.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## Simulated ## &lt;list&gt; ## 1: &lt;tsmodel.predict[2]&gt; ## 2: &lt;tsmodel.predict[2]&gt; ## 3: &lt;tsmodel.predict[2]&gt; ## 4: &lt;tsmodel.predict[2]&gt; ## 5: &lt;tsmodel.predict[2]&gt; ## 6: &lt;tsmodel.predict[2]&gt; ## 7: &lt;tsmodel.predict[2]&gt; ## 8: &lt;tsmodel.predict[2]&gt; par(mfrow = c(4,1),mar = c(3,3,3,3)) plot(s$simulation_table[series == &quot;ACF.NEWS&quot;]$Simulated[[1]], main = &quot;Simulated Series&quot;) plot(s$simulation_table[series == &quot;ACF.NEWS&quot;]$Level[[1]], main = &quot;Simulated Level&quot;) plot(s$simulation_table[series == &quot;ACF.NEWS&quot;]$Slope[[1]], main = &quot;Simulated Slope&quot;) plot(s$simulation_table[series == &quot;ACF.NEWS&quot;]$Seasonal[[1]], main = &quot;Simulated Seasonal&quot;) The tsbacktest method has similar arguments to other backtest implementations in the tsmodels framework: suppressMessages(library(future)) plan(list( tweak(sequential), tweak(multisession, workers = 5) )) b &lt;- tsbacktest(spec, h = 12, solver = &quot;nlminb&quot;, trace = FALSE, autodiff = TRUE) plan(sequential) The returned object is a list of 2 data.tables with the predictions for each series by estimation date and horizon and the summary metrics table: head(b$prediction) ## series estimation_date horizon size forecast_dates forecast actual ## &lt;char&gt; &lt;Date&gt; &lt;int&gt; &lt;int&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; ## 1: ACF.NEWS 2003-07-31 1 184 2003-08-31 11.22847 11.2 ## 2: ACF.NEWS 2003-07-31 2 184 2003-09-30 10.64990 10.1 ## 3: ACF.NEWS 2003-07-31 3 184 2003-10-31 10.57616 9.7 ## 4: ACF.NEWS 2003-07-31 4 184 2003-11-30 11.29816 9.7 ## 5: ACF.NEWS 2003-07-31 5 184 2003-12-31 15.65695 14.2 ## 6: ACF.NEWS 2003-07-31 6 184 2004-01-31 10.19482 8.6 head(b$metrics) ## series horizon MAPE MSLRE BIAS n ## &lt;char&gt; &lt;int&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;int&gt; ## 1: ACF.NEWS 1 0.08299767 0.01147201 0.01607784 185 ## 2: ACF.NEWS 2 0.09826851 0.01485794 0.02508737 184 ## 3: ACF.NEWS 3 0.10889976 0.01775767 0.03540852 183 ## 4: ACF.NEWS 4 0.11984134 0.02163159 0.04479359 182 ## 5: ACF.NEWS 5 0.11957098 0.02159764 0.05274425 181 ## 6: ACF.NEWS 6 0.12708012 0.02327741 0.06098494 180 6.7.3 Example: Australian Retail Sales Grouped Dynamics In this example, we show how grouping works on 99 series, where grouping is by sector. suppressWarnings(suppressPackageStartupMessages(library(tsvets))) suppressMessages(library(tsmethods)) suppressMessages(library(tsaux)) suppressMessages(library(xts)) suppressMessages(library(data.table)) suppressMessages(library(future)) plan(list( tweak(sequential), tweak(multisession, workers = 5) )) data(&quot;austretail&quot;, package = &quot;tsdatasets&quot;) y &lt;- austretail[&quot;2000/&quot;] include &lt;- sapply(1:ncol(y), function(i) all(!is.na(y[,i]))) y &lt;- y[,include] groups &lt;- sapply(1:ncol(y), function(i) strsplit(colnames(y[,i]), &quot;\\\\.&quot;)[[1]][2]) groups_index &lt;- sort.int(groups, index.return = TRUE) y &lt;- y[,groups_index$ix] groups &lt;- groups[groups_index$ix] groups &lt;- rle(groups)$lengths groups &lt;- unlist(sapply(1:length(groups), function(i) rep(i, groups[i]))) spec &lt;- vets_modelspec(y[,1:99], level = &quot;grouped&quot;, slope = &quot;grouped&quot;, damped = &quot;none&quot;, group = groups[1:99], seasonal = &quot;grouped&quot;, lambda = 0.5, dependence = &quot;diagonal&quot;, frequency = 12) mod &lt;- estimate(spec, solver = &quot;nlminb&quot;) plan(sequential) The joint estimation of the 99 series takes about 36.7841386 minutes. We avoid the summary method for such a large object, and instead print out the coefficients and the performance metrics: cf &lt;- coef(mod) print(data.table::data.table(Coefficient = names(cf), Value = round(cf,4))) ## Coefficient Value ## &lt;char&gt; &lt;num&gt; ## 1: Level[Group = 1] 0.4452 ## 2: Level[Group = 2] 0.5126 ## 3: Level[Group = 3] 0.7164 ## 4: Level[Group = 4] 0.6882 ## 5: Level[Group = 5] 0.1651 ## 6: Level[Group = 6] 0.5261 ## 7: Level[Group = 7] 0.5494 ## 8: Level[Group = 8] 0.4238 ## 9: Level[Group = 9] 0.5011 ## 10: Level[Group = 10] 0.5850 ## 11: Level[Group = 11] 0.7356 ## 12: Level[Group = 12] 0.4934 ## 13: Level[Group = 13] 0.7173 ## 14: Slope[Group = 1] 0.0000 ## 15: Slope[Group = 2] 0.0000 ## 16: Slope[Group = 3] 0.0000 ## 17: Slope[Group = 4] 0.0000 ## 18: Slope[Group = 5] 0.0067 ## 19: Slope[Group = 6] 0.0000 ## 20: Slope[Group = 7] 0.0000 ## 21: Slope[Group = 8] 0.0000 ## 22: Slope[Group = 9] 0.0000 ## 23: Slope[Group = 10] 0.0061 ## 24: Slope[Group = 11] 0.0000 ## 25: Slope[Group = 12] 0.0000 ## 26: Slope[Group = 13] 0.0000 ## 27: Seasonal[Group = 1] 0.3373 ## 28: Seasonal[Group = 2] 0.2047 ## 29: Seasonal[Group = 3] 0.0000 ## 30: Seasonal[Group = 4] 0.0000 ## 31: Seasonal[Group = 5] 0.1315 ## 32: Seasonal[Group = 6] 0.2795 ## 33: Seasonal[Group = 7] 0.0000 ## 34: Seasonal[Group = 8] 0.3143 ## 35: Seasonal[Group = 9] 0.3885 ## 36: Seasonal[Group = 10] 0.0000 ## 37: Seasonal[Group = 11] 0.0000 ## 38: Seasonal[Group = 12] 0.2692 ## 39: Seasonal[Group = 13] 0.0013 ## Coefficient Value tsmetrics(mod) ## N no_pars LogLik AIC BIC AICc MAPE MSLRE ## 1 22572 1425 -7613.512 -4763.512 6671.351 -4571.32 0.04063859 0.003203836 ## WAPE WSLRE ## 1 0.04063859 0.003203836 plot(mod, series = (1:99)[groups == 4]) plot(mod, series = (1:99)[groups == 4], type = &quot;states&quot;) plot(mod, series = (1:99)[groups == 4], type = &quot;residuals&quot;) 6.7.4 Example: Australian Retail Sales Filtering This example highlights the use of the tsfilter method for online filtering as already discussed in Sections 4.6.4 and 5.5.5. suppressWarnings(suppressPackageStartupMessages(library(tsvets))) suppressMessages(library(tsmethods)) suppressMessages(library(tsaux)) suppressMessages(library(xts)) data(&quot;austretail&quot;, package = &quot;tsdatasets&quot;) y &lt;- na.omit(austretail[,grepl(&quot;NEWS&quot;, colnames(austretail))]) y_train &lt;- y[1:(369 - 24)] y_test &lt;- y[(369 - 24 + 1):369] spec &lt;- vets_modelspec(y_train, level = &quot;common&quot;, slope = &quot;diagonal&quot;, damped = &quot;none&quot;, seasonal = &quot;common&quot;, lambda = 0.5, dependence = &quot;diagonal&quot;, frequency = 12) mod &lt;- estimate(spec, solver = &quot;nlminb&quot;) filt &lt;- tsfilter(mod, y = y_test) tail(fitted(mod)) ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS SA.NEWS T.NEWS V.NEWS ## 2016-07-31 6.274802 85.48929 1.563078 51.79401 12.263423 12.41307 47.63892 ## 2016-08-31 5.822149 87.91927 1.518185 54.59904 10.952639 12.97214 47.66849 ## 2016-09-30 4.902062 81.27385 1.394669 51.86892 9.940538 13.71905 45.84748 ## 2016-10-31 4.791341 83.27201 1.319689 55.07794 9.960859 12.42058 49.48286 ## 2016-11-30 5.547111 86.93625 1.357322 54.95787 10.368365 12.07245 50.89754 ## 2016-12-31 8.184714 116.27851 1.858932 72.01650 15.622342 14.69097 76.20901 ## WA.NEWS ## 2016-07-31 40.75596 ## 2016-08-31 42.52751 ## 2016-09-30 39.24565 ## 2016-10-31 40.45786 ## 2016-11-30 41.79131 ## 2016-12-31 55.58459 head(tail(fitted(filt), 24 + 1), 5) ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS SA.NEWS T.NEWS V.NEWS ## 2016-12-31 8.184714 116.27851 1.858932 72.01650 15.622342 14.69097 76.20901 ## 2017-01-31 5.136207 92.07696 1.239745 57.96096 8.790413 12.48530 47.68477 ## 2017-02-28 7.150196 94.62244 1.098936 52.06101 9.564020 13.50578 47.71324 ## 2017-03-31 5.671755 98.44475 1.105181 54.21634 11.124740 11.63927 52.55807 ## 2017-04-30 4.718627 82.17766 1.158383 51.48423 8.319089 10.94583 47.45609 ## WA.NEWS ## 2016-12-31 55.58459 ## 2017-01-31 48.38624 ## 2017-02-28 40.52982 ## 2017-03-31 40.46546 ## 2017-04-30 37.86586 6.7.5 Example: Australian Retail Sales Aggregation This demonstrates the ability to aggregate a model estimated with homogeneous coefficients. We continue with the retail sales data, which being a set of flow variables, can be aggregated. suppressWarnings(suppressPackageStartupMessages(library(tsvets))) suppressMessages(library(tsmethods)) suppressMessages(library(tsaux)) suppressMessages(library(xts)) data(&quot;austretail&quot;, package = &quot;tsdatasets&quot;) y &lt;- na.omit(austretail[,grepl(&quot;NEWS&quot;, colnames(austretail))]) We use a vector of ones as weights, to denote summation to the total for the News category. weights &lt;- rep(1, 8) Additionally, we also include, purely for expositional purposes, a matrix of 5 randomly generated regressors: xreg &lt;- xts(matrix(rnorm(nrow(y) * 5), nrow = nrow(y), ncol = 5), index(y)) xreg_include &lt;- matrix(2, ncol = 5, nrow = ncol(y)) xreg_include[2,1] &lt;- 1 Setting the include matrix to the value of 2 means pooling, and we also set regressor 1 for series 2 to a value of 1 in order to denote that we want this to be estimated seperately (non-pooled). spec &lt;- vets_modelspec(y, level = &quot;common&quot;, slope = &quot;common&quot;, seasonal = &quot;common&quot;, dependence = &quot;diagonal&quot;, frequency = 12, xreg = xreg, xreg_include = xreg_include) mod &lt;- estimate(spec, solver = &quot;nlminb&quot;, control = list(trace = 0)) We can extract specific matrices from the estimate object using the as yet unexported function p_matrix: tsvets:::p_matrix(mod)$X ## 8 x 5 Matrix of class &quot;dgeMatrix&quot; ## x1 x2 x3 x4 x5 ## ACF.NEWS 0.01394335 -0.01667844 -0.0268727 -0.0004924587 -0.01555726 ## NSW.NEWS -0.35119906 -0.01667844 -0.0268727 -0.0004924587 -0.01555726 ## NT.NEWS 0.01394335 -0.01667844 -0.0268727 -0.0004924587 -0.01555726 ## Q.NEWS 0.01394335 -0.01667844 -0.0268727 -0.0004924587 -0.01555726 ## SA.NEWS 0.01394335 -0.01667844 -0.0268727 -0.0004924587 -0.01555726 ## T.NEWS 0.01394335 -0.01667844 -0.0268727 -0.0004924587 -0.01555726 ## V.NEWS 0.01394335 -0.01667844 -0.0268727 -0.0004924587 -0.01555726 ## WA.NEWS 0.01394335 -0.01667844 -0.0268727 -0.0004924587 -0.01555726 where we can observe that the coefficients for each regressor have been pooled, with the exception of x1 for the NSW.NEWS series. We can now proceed to aggregate the model using the tsaggregate method on the estimated object: mod_aggregate &lt;- tsaggregate(mod, weights = weights, return_model = TRUE) summary(mod_aggregate) ## ## Vector ETS ## ----------------------------------- ## Level : common ## Slope : common ## Seasonal : common ## Dependence : diagonal ## No. Series : 1 ## No. TimePoints : 369 ## ## Parameter Matrices ## ## Level Matrix ## 1 x 1 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] ## [1,] 0.6224697 ## ## Slope Matrix ## 1 x 1 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] ## [1,] 1e-12 ## ## Seasonal Matrix ## 1 x 1 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] ## [1,] 0.1257502 ## ## Regressor Matrix ## 1 x 5 Matrix of class &quot;dgeMatrix&quot; ## x1 x2 x3 x4 x5 ## aggregate -0.2535956 -0.1334275 -0.2149816 -0.003939669 -0.1244581 ## ## Covariance Matrix ## 1 x 1 diagonal matrix of class &quot;ddiMatrix&quot; ## aggregate ## aggregate 222.3031 ## ## Accuracy Criteria ## Mean Weighted ## MAPE 0.0352 0.0352 ## MSLRE 0.0021 0.0021 plot(mod_aggregate) suppressWarnings(plot(mod_aggregate, type = &quot;states&quot;)) We can then proceed with prediction as well as simulation, both of which allow for the outputs to be aggregated. Alternatively, since we now have an aggregated model object, we can also predict or simulate directly from that object. newxreg &lt;- xts(matrix(rnorm(24 * 5), nrow = 24, ncol = 5), future_dates(tail(index(y),1),&quot;months&quot;,24)) p &lt;- predict(mod, newxreg = newxreg) p_agg_series &lt;- tsaggregate(p, weights = weights) p_agg_model &lt;- predict(mod_aggregate, newxreg = newxreg) plot(p_agg_series$distribution, gradient_color = &quot;whitesmoke&quot;, interval_color = &quot;grey&quot;, main = &quot;Agggregated Model&quot;) plot(p_agg_model$prediction_table[1]$Predicted[[1]]$distribution, gradient_color = &quot;whitesmoke&quot;, add = TRUE, median_color = 2, interval_color = &quot;grey10&quot;) As we can observe, the output of the 2 predictions is identical (subject to some randomness from the simulation of the predictive distribution). We do the same for the simulation method: s &lt;- simulate(mod, nsim = 100, h = 24, newxreg = newxreg, seed = 700) s_agg_series &lt;- tsaggregate(s, weights = weights) s_agg_model &lt;- simulate(mod_aggregate, nsim = 100, h = 24, newxreg = newxreg, seed = 700) plot(s_agg_series$distribution, gradient_color = &quot;whitesmoke&quot;, interval_color = &quot;grey&quot;) plot(s_agg_model$simulation_table[1]$Simulated[[1]]$distribution, gradient_color = &quot;whitesmoke&quot;, add = TRUE, median_color = 2, interval_color = &quot;grey10&quot;) 6.7.6 Example: Groupwise Pooling It is also possible to have group wise pooling. We saw above that we could specify pooling using the number of series times number of regressors matrix, xreg_include, with values 0, 1 or 2+ (0 = no beta, 1 = individual beta and 2+ = grouped beta). Group pooling can be achieved by setting values of 2+. For instance 2 series sharing one pooled estimate, and 3 other series sharing another grouped estimate would have values of (2,2,3,3,3). In the following example we have 8 series and 2 regressors, with different combinations of pooling. xreg &lt;- xts(matrix(rnorm(nrow(y)*2), ncol = 2, nrow = nrow(y)), index(y)) colnames(xreg) &lt;- c(&quot;B1&quot;,&quot;B2&quot;) xreg_include &lt;- matrix(1, ncol = 2, nrow = ncol(y)) rownames(xreg_include) &lt;- colnames(y) colnames(xreg_include) &lt;- colnames(xreg) # individual [1,1], grouped 2:3, 5:6 and 7:8 xreg_include[c(2:3),1] &lt;- 2 xreg_include[c(5:6),1] &lt;- 3 xreg_include[c(7:8),1] &lt;- 4 # individual 1, zero for 2:3, group for 4:8 xreg_include[c(2:3),2] &lt;- 0 xreg_include[c(4:8),2] &lt;- 3 xreg_include ## B1 B2 ## ACF.NEWS 1 1 ## NSW.NEWS 2 0 ## NT.NEWS 2 0 ## Q.NEWS 1 3 ## SA.NEWS 3 3 ## T.NEWS 3 3 ## V.NEWS 4 3 ## WA.NEWS 4 3 spec &lt;- vets_modelspec(y, level = &quot;grouped&quot;, slope = &quot;grouped&quot;, group = c(1,1,2,2,3,3,4,4), damped = &quot;none&quot;, seasonal = &quot;grouped&quot;, xreg = xreg, xreg_include = xreg_include, lambda = 0, dependence = &quot;diagonal&quot;, frequency = 12) mod &lt;- estimate(spec, solver = &quot;nlminb&quot;) Note that the Box Cox parameter lambda should be the same for the grouped series else the coefficients should not be pooled (as they have different scales). 6.7.7 Example: Price Volume Aggregation This final example shows how to jointly model a series based on price-volume relationship with a target of generating a model for Sales: \\(Sales = Price\\times Volume\\) While this is a multiplicative model, if we were to model the series using a log transformation we would be able to aggregate the model in logs before exponentiating to recover the Sales as the example will demonstrate. data(&quot;priceunits&quot;, package = &quot;tsdatasets&quot;) spec &lt;- vets_modelspec(priceunits, level = &quot;common&quot;, slope = &quot;common&quot;, frequency = 12, lambda = rep(0,2), dependence = &quot;full&quot;) mod &lt;- estimate(spec, solver = &quot;optim&quot;, control = list(trace = 0)) summary(mod) ## ## Vector ETS ## ----------------------------------- ## Level : common ## Slope : common ## Seasonal : none ## Dependence : full ## No. Series : 2 ## No. TimePoints : 100 ## ## Parameter Matrices ## ## Level Matrix ## 2 x 2 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] [,2] ## [1,] 0.3900419 . ## [2,] . 0.3900419 ## ## Slope Matrix ## 2 x 2 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] [,2] ## [1,] 0.09196964 . ## [2,] . 0.09196964 ## ## Correlation Matrix ## 2 x 2 Matrix of class &quot;dsyMatrix&quot; ## price units ## price 1.0000000 -0.5939792 ## units -0.5939792 1.0000000 ## ## Information Criteria ## AIC BIC AICc ## 162.58 182.37 163.02 ## ## Accuracy Criteria ## Mean Weighted ## MAPE 0.0508 0.0508 ## MSLRE 0.0042 0.0042 plot(mod) plot(mod, type = &quot;states&quot;) weights &lt;- c(1,1) mod_aggregate &lt;- tsaggregate(mod, weights = weights, return_model = TRUE) summary(mod_aggregate) ## ## Vector ETS ## ----------------------------------- ## Level : common ## Slope : common ## Seasonal : none ## Dependence : diagonal ## No. Series : 1 ## No. TimePoints : 100 ## ## Parameter Matrices ## ## Level Matrix ## 1 x 1 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] ## [1,] 0.3900419 ## ## Slope Matrix ## 1 x 1 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] ## [1,] 0.09196964 ## ## Covariance Matrix ## 1 x 1 diagonal matrix of class &quot;ddiMatrix&quot; ## aggregate ## aggregate 0.003901452 ## ## Accuracy Criteria ## Mean Weighted ## MAPE 0.0496 0.0496 ## MSLRE 0.0039 0.0039 p &lt;- predict(mod, h = 24, nsim = 5000) p_agg_model &lt;- predict(mod_aggregate, h = 24, nsim = 5000) p_agg_series &lt;- tsaggregate(p, weights = weights) plot(p_agg_model$prediction_table[1]$Predicted[[1]]$distribution, gradient_color = &quot;whitesmoke&quot;, interval_color = &quot;grey&quot;) plot(p_agg_series$distribution, gradient_color = &quot;whitesmoke&quot;, add = TRUE, median_color = 2, interval_color = &quot;grey10&quot;) In the presence of a non-homogeneous model, or a homogeneous model with Box Cox parameter not equal to either 0 (log) or 1 (no transform), then the tsaggregate method will still work but the return_model arguments needs to be set to FALSE and the resulting output will just be the aggregated series. References "],["tsforeign.html", "Chapter 7 tsforeign package 7.1 Introduction 7.2 ARIMA model 7.3 bsts model", " Chapter 7 tsforeign package 7.1 Introduction The tsforeign package provides custom wrappers for the auto.arima function from the forecast package of R. Hyndman et al. (2022) package and the bsts method from bsts package of Steven L. Scott (2022), with methods for estimation, some diagnostics and prediction. This package may be extended in future to provide wrappers for other interesting models. For the bsts model, we have also made use of the tsconvert method from the tsmethods package to convert the output of the model to one conforming to the required inputs of the dlm package, for which we provide an example in the demonstration section. Since both of those packages have their own vignettes and documentation, we proceed here directly into a demonstration of the functionality. 7.2 ARIMA model The entry point for the ARIMA model is the arima_modelspec function suppressPackageStartupMessages(library(tsforeign)) library(xts) library(tsaux) args(arima_modelspec) ## function (y, xreg = NULL, frequency = NULL, seasonal = FALSE, ## seasonal_type = &quot;regular&quot;, lambda = NULL, seasonal_harmonics = NULL, ## lambda_lower = 0, lambda_upper = 1.5, ...) ## NULL which allows for both regular and trigonometric seasonality. For the demonstration example, we’ll use the AirPassengers dataset after first converting it into an xts object: air &lt;- AirPassengers dt &lt;- future_dates(as.Date(&quot;1948-12-31&quot;), &quot;months&quot;, length(air)) air &lt;- xts(as.numeric(air), dt) spec &lt;- arima_modelspec(y = air, frequency = 12, seasonal_type = &quot;regular&quot;, seasonal = TRUE, lambda = NA) mod &lt;- estimate(spec) summary(mod) ## ARIMA(0,1,1)(0,1,1)[12] ## ------------------------ ## Estimate Std.Error t value Pr(&gt;|t|) ## ma1 -0.4018 0.08964 -4.482 7.381e-06 ## sma1 -0.5569 0.07310 -7.619 2.554e-14 ## ## sigma : 0.037 ## ## AIC BIC AICc ## -483.31 -474.69 -483.13 ## ## MAPE MASE MSLRE BIAS ## 0.0262 0.2297 0.0012 0 tsmetrics(mod) ## n no.pars LogLik AIC BIC AICc MAPE MASE ## 1 131 2 244.6574 -483.3147 -474.6891 -483.1258 0.02623633 0.2297063 ## MSLRE BIAS ## 1 0.001228422 4.059721e-05 plot(mod) The predict function generates a predictive distribution via simulation and returns an object of class tsmodel.predict: p &lt;- predict(mod, h = 12) plot(p) Additionally, there is also a backtest method: b &lt;- tsbacktest(spec, h = 12, alpha = c(0.02, 0.1), cores = 5, trace = F) b$metrics ## horizon variable MAPE MSLRE BIAS n MIS[0.02] ## &lt;int&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;int&gt; &lt;num&gt; ## 1: 1 y 0.02357239 0.0009512022 0.003713807 72 76.78371 ## 2: 2 y 0.02837170 0.0013727862 0.006297666 71 79.22509 ## 3: 3 y 0.03296483 0.0018393442 0.008375600 70 89.95706 ## 4: 4 y 0.03694681 0.0022937517 0.009861276 69 116.03158 ## 5: 5 y 0.03845403 0.0024637211 0.011796591 68 123.39160 ## 6: 6 y 0.04150696 0.0027772296 0.013512230 67 130.95317 ## 7: 7 y 0.04380133 0.0030903056 0.015835961 66 135.72532 ## 8: 8 y 0.04533363 0.0031925974 0.018676736 65 138.92727 ## 9: 9 y 0.04703576 0.0034092951 0.020971360 64 142.82301 ## 10: 10 y 0.04911560 0.0037578974 0.024080807 63 153.80218 ## 11: 11 y 0.05020916 0.0039095064 0.026176728 62 162.43629 ## 12: 12 y 0.05267677 0.0042013412 0.027876521 61 169.94190 ## MIS[0.1] ## &lt;num&gt; ## 1: 58.30995 ## 2: 63.93438 ## 3: 75.58588 ## 4: 86.38726 ## 5: 90.69070 ## 6: 96.63358 ## 7: 100.87521 ## 8: 104.42741 ## 9: 112.38600 ## 10: 118.11243 ## 11: 123.67679 ## 12: 130.53549 7.3 bsts model The bsts package of (Steven L. Scott 2022) provides functions for fitting Bayesian structural time series models. The entry point in our wrapper is the bsts_modelspec function: args(bsts_modelspec) ## function (y, xreg = NULL, frequency = NULL, differences = 0, ## level = TRUE, slope = TRUE, damped = FALSE, seasonal = FALSE, ## seasonal_frequency = 4, ar = FALSE, ar_max = 1, cycle = FALSE, ## cycle_frequency = NULL, cycle_names = NULL, seasonal_type = &quot;regular&quot;, ## lambda = NULL, lambda_lower = 0, lambda_upper = 1, seasonal_harmonics = NULL, ## distribution = &quot;gaussian&quot;, ...) ## NULL which provides a rich set of options such as degree of differencing,17 a sparse AR component, sparse regressors, regular or trigonometric seasonal components (including multiple seasonal), cyclical components and the Box Cox transformation. Once an object is estimated, and all required components have been generated, any additional methods on the estimated component are performed directly by custom written functions in the tsforeign package. For the demonstration we’ll use the priceunits dataset from the tsdatasets package. Once a series is estimated, the resulting MCMC draws are converted to an mcmc object from the coda package of Plummer et al. (2020) in order to provide a nice summary report. data(&quot;priceunits&quot;, package = &quot;tsdatasets&quot;) spec &lt;- bsts_modelspec(y = priceunits[1:80,1], frequency = 12, differences = 0, level = T, slope = T, damped = T, seasonal = T, seasonal_frequency = 12, ar = T, ar_max = 3, lambda = 0) mod &lt;- estimate(spec, n_iter = 2000, trace = FALSE) summary(mod) ## ## Iterations = 1:2000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 2000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE Prob[Include] ## obs.sigma 0.0132905 0.006549 1.464e-04 0.0006929 1.0000 ## level.sigma 0.0046552 0.003710 8.295e-05 0.0008889 1.0000 ## slope.sigma 0.0321532 0.006358 1.422e-04 0.0004551 1.0000 ## slope.ar -0.3336629 0.267831 5.989e-03 0.0248040 1.0000 ## seasonal12.sigma 0.0023919 0.002541 5.682e-05 0.0008274 1.0000 ## ar1 0.0300258 0.235424 5.264e-03 0.0208192 0.6905 ## ar2 0.0389725 0.127629 2.854e-03 0.0115642 0.4260 ## ar3 0.0009242 0.051035 1.141e-03 0.0022904 0.2050 ## ar3.sigma 0.0285918 0.006131 1.371e-04 0.0003575 1.0000 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## obs.sigma 5.301e-03 0.009145 0.012391 0.016388 0.026054 ## level.sigma 4.049e-04 0.001536 0.003908 0.006786 0.013702 ## slope.sigma 1.953e-02 0.027971 0.032119 0.036327 0.044569 ## slope.ar -7.402e-01 -0.529793 -0.369327 -0.179222 0.300217 ## seasonal12.sigma 7.983e-05 0.000488 0.001482 0.003395 0.009623 ## ar1 -4.120e-01 -0.069753 0.000000 0.123664 0.597288 ## ar2 -1.580e-01 0.000000 0.000000 0.027540 0.399110 ## ar3 -1.072e-01 0.000000 0.000000 0.000000 0.139456 ## ar3.sigma 2.054e-02 0.025165 0.028213 0.031091 0.038241 ## ## ## Harvey&#39;s Goodness of Fit Statistic: 0.2161046 ## ## MAPE MASE MSLRE BIAS ## 0.0371 0.4694 0.0023 -2e-04 tsmetrics(mod) ## n no.pars MAPE MASE MSLRE BIAS ## 1 80 9 0.03708086 0.4694221 0.002281904 -0.0001531455 plot(mod) The decomposition of the model into it’s fitted components is done via the tsdecompose method. However, note that the bsts routine returns the smoothed component states, not the filtered ones. tde &lt;- tsdecompose(mod) # distribution objects of state components str(tde) ## List of 5 ## $ Level : &#39;tsmodel.distribution&#39; num [1:1948, 1:80] 1.6 1.6 1.55 1.58 1.55 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:80] &quot;1992-08-31&quot; &quot;1992-09-30&quot; &quot;1992-10-31&quot; &quot;1992-11-30&quot; ... ## ..- attr(*, &quot;date_class&quot;)= chr &quot;Date&quot; ## $ Slope : &#39;tsmodel.distribution&#39; num [1:1948, 1:80] 0.03346 0.02428 -0.00824 0.01225 0.08149 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:80] &quot;1992-08-31&quot; &quot;1992-09-30&quot; &quot;1992-10-31&quot; &quot;1992-11-30&quot; ... ## ..- attr(*, &quot;date_class&quot;)= chr &quot;Date&quot; ## $ AR : &#39;tsmodel.distribution&#39; num [1:1948, 1:80] 0.02726 0.00632 0.09124 0.03447 0.06217 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:80] &quot;1992-08-31&quot; &quot;1992-09-30&quot; &quot;1992-10-31&quot; &quot;1992-11-30&quot; ... ## ..- attr(*, &quot;date_class&quot;)= chr &quot;Date&quot; ## $ X : NULL ## $ Seasonal12: &#39;tsmodel.distribution&#39; num [1:1948, 1:80] -0.0466 -0.0506 -0.0523 -0.0414 -0.0393 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:80] &quot;1992-08-31&quot; &quot;1992-09-30&quot; &quot;1992-10-31&quot; &quot;1992-11-30&quot; ... ## ..- attr(*, &quot;date_class&quot;)= chr &quot;Date&quot; Once bsts model is estimated, we can convert it to a dlm object18 using either the mean values of the parameters and initial states, or a specific draw using the tsconvert method. library(dlm) dlm_model &lt;- tsconvert(mod, to = &quot;dlm&quot;, draw = &quot;mean&quot;, burn = bsts::SuggestBurn(0.1, mod$model)) str(dlm_model) ## List of 6 ## $ m0: num [1:17] 1.56851 0.01552 -0.00441 -0.04117 -0.03039 ... ## $ C0: num [1:17, 1:17] 1e+07 0e+00 0e+00 0e+00 0e+00 0e+00 0e+00 0e+00 0e+00 0e+00 ... ## $ FF: num [1, 1:17] 1 0 0 1 0 0 0 0 0 0 ... ## $ V : num [1, 1] 0.0129 ## $ GG: num [1:17, 1:17] 1 0 0 0 0 0 0 0 0 0 ... ## $ W : num [1:17, 1:17] 0.00476 0 0 0 0 ... ## - attr(*, &quot;class&quot;)= chr &quot;dlm&quot; filtered_dlm &lt;- dlmFilter(log(priceunits[1:80,1]), dlm_model) smoothed_dlm &lt;- dlmSmooth(filtered_dlm) smoothed_series &lt;- xts((dlm_model$FF %*% t(smoothed_dlm$s))[1,-1], index(priceunits)[1:80]) par(mfrow = c(2,1), mar = c(3,3,3,3)) plot(as.zoo(filtered_dlm$f), type = &quot;l&quot;, main = &quot;Filtered Series&quot;, ylab = &quot;&quot;) lines(as.zoo(fitted(mod, raw = TRUE)), col = 2) grid() legend(&quot;topright&quot;, c(&quot;DLM&quot;,&quot;BSTS&quot;), col = 1:2, lty = 1, bty = &quot;n&quot;) plot(as.zoo(smoothed_series), type = &quot;l&quot;, main = &quot;Smoothed Series&quot;, ylab = &quot;&quot;) lines(as.zoo(fitted(mod, raw = TRUE, type = &quot;smoothed&quot;)), col = 2) grid() legend(&quot;topright&quot;, c(&quot;DLM&quot;,&quot;BSTS&quot;), col = 1:2, lty = 1, bty = &quot;n&quot;) We compare the predictive distribution of the predicted object from calling the predict method of the tsforeign package and that of the bsts package. Note that there are a lot more arguments which can be passed to the predict routine, including user overrides for the last state means and the posterior means of the parameters (see the documentation for more details). p1 &lt;- predict(mod, h = 12) # BSTS method p2 &lt;- bsts::predict.bsts(mod$model, horizon = 12) # convert predictive distribution to tsmodel.distribution for comparison p2d &lt;- p2$distribution colnames(p2d) &lt;- colnames(p1$distribution) class(p2d) &lt;- &quot;tsmodel.distribution&quot; plot(log(p1$distribution), main = &quot;tsforeign native predict c++ routine vs bsts routine&quot;) plot(p2d, add = TRUE, median_color = 2, interval_color = 2, median_type = 2) A predicted object can also be decomposed into it’s structural components, something we are able to do because of the custom predict routine since bsts does not return this information. td &lt;- tsdecompose(p1) par(mfrow = c(2,2), mar = c(3,3,3,3)) plot(td$Level, main = &quot;Level&quot;) plot(td$Slope, main = &quot;Slope&quot;) plot(td$Seasonal12, main = &quot;Seasonal&quot;) plot(td$AR, main = &quot;AR&quot;) References "],["tscausal.html", "Chapter 8 tscausal package 8.1 Introduction 8.2 Measures of Evaluation 8.3 Evaluation of Significance 8.4 Package Features 8.5 Package Demo", " Chapter 8 tscausal package 8.1 Introduction The objective of the tscausal package is to provide an easy to use framework to evaluate any time series based forecast distribution, proxy for the counterfactual distribution, and generate an automatic set of diagnostics on the significance of an intervention. It is heavily influenced by the CausalImpact package described in Brodersen et al. (2015), and from which it borrows parts of the code diagnostics and output narrative.19 Unlike CausalImpact, the package is agnostic to any particular time series model, but it does require that the forecast distribution conform to certain requirements, namely that it inherits the class tsmodel.distribution. Any matrix of size \\(N\\times h\\), representing the \\(N\\) simulated points for each forecast period \\(t\\in h\\), where \\(t&gt;T\\) (the intervention date), can be coerced to this class. It is assumed that the user can and has trained a model on the pre-intervention period and our code repository provides for a wide array of models, with common calling conventions and the ability to generate frequentist simulated or Bayesian predictive distributions, all of which inherit class tsmodel.distribution. 8.2 Measures of Evaluation This section provides a summary overview of the measures used to evaluate the possible presence of a causal effect, and follow Brodersen et al. (2015) closely. 8.2.1 Pointwise Impact Given an intervention date \\(T\\), and a post-intervention period under consideration \\(T+1,\\ldots,T+h\\), the causal pointwise effect \\(\\phi^{(s)}_{t}\\) for all \\(t \\in T+1,\\ldots,T+h\\) given a draw \\(s\\) from the predictive distribution is: \\[ \\phi^{(s)}_t = y_t - \\hat y^{(s)}_t \\] where \\(y_t\\) represents the outcome variable and \\(\\hat{y}_t\\) the forecasted (or counterfactual) variable. Because we have a distributional forecast, which represents the uncertainty about the forecasted variable \\(\\hat{y}_t\\) at each date in the horizon \\(T+1,\\ldots,T+h\\), and draws indexed by \\((s)_1,\\ldots,(s)_N\\), we can generate an empirical distribution of any function applied to the distribution (e.g. summation, differences etc). Given a significance level \\(a\\)%, the \\(100-a\\)% confidence interval can be evaluated by making use of the empirical quantile function from this distribution. 8.2.2 Cumulative Impact The cumulative impact is derived from the pointwise impact as: \\[ \\Phi _t^{(s)} = \\sum\\limits_{i = T + 1}^t {\\phi _i^{(s)}}, \\] which is a useful quantity when the outcome represents a flow variable (e.g. revenue, signups), measured over an interval of time, however is not interpretable when it represents a stock variable (e.g. inventory, subscribers). 8.2.3 Mean Effect Given an intervention date \\(T\\), and a post-intervention period \\(T+1,\\ldots,T+h\\), then the average effect given a draw \\(s\\) from the predictive distribution is: \\[ {\\alpha ^{(s)}} = \\frac{1}{{t - T}}\\sum\\limits_{i = T + 1}^{T + h} {{{y_i} - \\hat y_i^{(s)}} }. \\] 8.2.4 Mean Relative Effect A related measure, showing the relative effect of the intervention can be defined: \\[ {r^{(s)}} = \\frac{{{\\alpha ^{(s)}}}}{{{\\mu _{\\hat y}}}}, \\] where \\(\\mu_{\\bar y}\\) is the mean of the forecasted variable \\(\\forall t&gt;T\\) and \\(\\forall s\\). 8.3 Evaluation of Significance Given a significance level \\(a\\), under the null hypothesis that the intervention was not significant, we fail to reject if \\(F^{-1}_n\\left(\\alpha/2\\right)&lt;0\\) and \\(F^{-1}_n\\left(1-\\alpha/2\\right)&gt;0\\), and reject otherwise. For instance, if the effect was positive with a lower quantile value above zero then we would reject the null and conclude that the effect was positive and significant. If the effect was negative with an upper quantile value less than zero we would also reject the null and conclude that the effect was negative and significant. Another statistic which can be used to evaluate whether the results could have occurred by chance is given by the following: \\[ \\frac{1}{N}\\sum\\limits_{j = 1}^N {\\left[ {\\left( {\\sum\\limits_{i = T + 1}^{T + h} {\\phi _t^{(j)}} } \\right) \\geqslant 0} \\right]}, \\] which measures the percent of times the total net effect of the distribution was positive, measured over the entire forecast horizon and across all draws. 8.4 Package Features The package has one main method called tscausal which takes as arguments a tsmodel.distribution object, the outcome variable (as xts object which includes both the pre and post intervention data), an optional xts vector (or tsmodel.distribution) of the in-sample fitted values and the significance level for hypothesis testing alpha, as shown below: ## function (object, actual, fitted = NULL, alpha = 0.05, include_cumulative = TRUE, ## ...) ## NULL Additionally, once the causal object has been created, there are methods for plotting (plot), printing of results (print) as well as a reporting (tsreport) which can be used to automatically generate a pdf, doc or html report with full evaluation and graphics. The next section provides a demonstration. 8.5 Package Demo We use the priceunits dataset from the tsdatasets package, in particular the units series, and add an artificial structural break during 1999-03, which could represent for instance a large drop in prices, a large substitution effect or a successful marketing campaign. data(priceunits, package = &quot;tsdatasets&quot;) priceunits &lt;- priceunits[, 2] priceunits[&quot;1999-03/&quot;] &lt;- priceunits[&quot;1999-03/&quot;] + 10 train &lt;- &quot;/1999-02&quot; test = &quot;1999-03/1999-08&quot; y_train = priceunits[train] y_test = priceunits[test] spec &lt;- ets_modelspec(y_train, frequency = 12, lambda = 0, model = &quot;AAN&quot;) mod &lt;- estimate(spec) summary(mod) ## ## ETS Model [ AAN ] ## ## Parameter Description Est[Value] Std. Error t value Pr(&gt;|t|) ## ---------- ------------------ ----------- ----------- -------- --------- ## alpha State[Level-coef] 0.3253 0.0683 4.7651 0.0000 ## beta State[Slope-coef] 0.1057 0.0344 3.0674 0.0022 ## l0 State[Level-init] 1.3528 0.0482 28.0470 0.0000 ## b0 State[Slope-init] 0.0632 0.0181 3.4834 0.0005 ## ## ## LogLik AIC BIC AICc ## -------- ------- ------- ------- ## 25.0856 -40.17 -28.32 -39.35 ## ## ## MAPE MASE MSLRE BIAS ## ------- ------- ------- ------- ## 0.0692 0.9563 0.0067 0.0054 prediction &lt;- predict(mod, h = nrow(y_test), nsim = 5000) Note that the prediction object has a number of slots, one of which is called distribution and inherits the following classes: tsets.distribution, tsmodel.distribution. All our packages have a predictable and common set of slots in the returned prediction objects. The final step is to pass the forecast distribution to the tscausal method and generate a report with a narrative explaining the results and a decision on whether to reject or not the null of no intervention effect. cause &lt;- tscausal(prediction$distribution, actual = rbind(y_train, y_test), fitted = fitted(mod)) print(cause) ## Predictive inference {tscausal} ## ## Average Cumulative ## Actual 37.33 224.00 ## Prediction (s.d.) 27.88 (2.541) 167.26 (15.246) ## 95% CI [23, 33] [140, 199] ## ## Absolute effect (s.d.) 9.457 (2.541) 56.740 (15.246) ## 95% CI [4.1, 14] [24.9, 84] ## ## Relative effect (s.d.) 33.92% (9.115%) 33.92% (9.115%) ## 95% CI [15%, 50%] [15%, 50%] ## ## Predictive Distribution tail-area probability p: 6e-04 ## Predictive Distribution prob. of a causal effect: 99.94001% tsreport(cause) During the post-intervention period, the response variable had an average value of approx. 37.3333. By contrast, in the absence of an intervention, we would have expected an average response of 27.88. The 95% interval of this counterfactual prediction is [23.3552, 33.1860]. Subtracting this prediction from the observed response yields an estimate of the causal effect the intervention had on the response variable. This effect is 9.4567 with a 95% interval of [4.1473, 13.9782]. For a discussion of the significance of this effect, see below. Summing up the individual data points during the post-intervention period (which can only sometimes be meaningfully interpreted), the response variable had an overall value of 224.0000. By contrast, had the intervention not taken place, we would have expected a sum of 167.26. The 95% interval of this prediction is [140.1310, 199.1162]. The above results are given in terms of absolute numbers. In relative terms, the response variable showed an increase of +34%. The 95% interval of this percentage is [+15%, +50%]. This means that the positive effect observed during the intervention period is statistically significant and unlikely to be due to random fluctuations. It should be noted, however, that the question of whether this increase also bears substantive significance can only be answered by comparing the absolute effect (9.4567) to the original goal of the underlying intervention. The probability of obtaining this effect by chance is very small (Bayesian one-sided tail-area probability p = 0.001). This means the causal effect can be considered statistically significant. NULL The tsreport method has options for printing to screen or generating a pdf, html or docx (with option to use a template as well). Example usage is provided below: tsreport(cause, type = &quot;pdf&quot;, output_dir = &quot;~/tmp&quot;, args = list(model = &quot;ETS[AAN]&quot;, frequency = 12, name = &quot;Units&quot;)) tsreport(cause, type = &quot;html&quot;, output_dir = &quot;~/tmp&quot;, args = list(model = &quot;ETS[AAN]&quot;, frequency = 12, name = &quot;Units&quot;)) tsreport(cause, type = &quot;doc&quot;, doc_template = &quot;~/tmp/mytemplate.docx&quot;, output_dir = &quot;~/tmp&quot;, args = list(model = &quot;ETS[AAN]&quot;, frequency = 12, name = &quot;Units&quot;)) References "],["tsdistributions.html", "Chapter 9 tsdistributions package 9.1 Introduction 9.2 Distributions 9.3 A note on analytic higher moments 9.4 Implementation and Code Examples", " Chapter 9 tsdistributions package 9.1 Introduction The objective of the tsdistributions package is to provide a set of location-scale invariant distributions re-parameterized in mean-variance space. The distributions in the package are based on those of the rugarch package and re-written to more closely follow the convention used by the distributions in stats. Maximum likelihood estimation is included and makes use of automatic differentiation via the functionality of the TMB package. It is part of a longer term plan for a full re-write of rugarch, with this package forming a foundational set piece. 9.2 Distributions 9.2.1 The Normal Distribution The Normal Distribution is a spherical distribution described completely by it first two moments, the mean and variance. Formally, the random variable \\(x\\) is said to be normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\) (both of which may be time varying), with density given by, \\[ f\\left( x \\right) = \\frac{{{e^{\\frac{{ - 0.5{{\\left( {x - \\mu } \\right)}^2}}} {{{\\sigma ^2}}}}}}} {{\\sigma \\sqrt {2\\pi } }}. \\] Following some mean filtration or whitening process, the residuals \\(\\varepsilon\\), standardized by \\(\\sigma\\) yield the standard normal density given by, \\[ f\\left( {\\frac{{x - \\mu }}{\\sigma }} \\right) = \\frac{1} {\\sigma }f\\left( z \\right) = \\frac{1} {\\sigma }\\left( {\\frac{{{e^{ - 0.5{z^2}}}}} {{\\sqrt {2\\pi } }}} \\right). \\] To obtain the conditional likelihood of some generating process at each point in time (\\(LL_t\\)), the conditional standard deviation \\(\\sigma_t\\) (this could also be constant), acts as a scaling factor on the density, so that: \\[ L{L_t}\\left( {{z_t};{\\sigma _t}} \\right) = \\frac{1} {{{\\sigma _t}}}f\\left( {{z_t}} \\right) \\] which illustrates the importance of the scaling property. Finally, the normal distribution has zero skewness and zero excess kurtosis. 9.2.2 The Student Distribution It Student distribution is described completely by a shape parameter \\(\\nu\\), but for standardization we proceed by using its 3 parameter representation as follows: \\[ f\\left( x \\right) = \\frac{{\\Gamma \\left( {\\frac{{\\nu + 1}} {2}} \\right)}} {{\\sqrt {\\beta \\nu \\pi } \\Gamma \\left( {\\frac{\\nu } {2}} \\right)}}{\\left( {1 + \\frac{{{{\\left( {x - \\alpha } \\right)}^2}}} {{\\beta \\nu }}} \\right)^{ - \\left( {\\frac{{\\nu + 1}} {2}} \\right)}} \\] where \\(\\alpha\\), \\(\\beta\\), and \\(\\nu\\) are the location, scale20 and shape parameters respectively, and \\(\\Gamma\\) is the Gamma function. Similar to the GED distribution described later, this is a unimodal and symmetric distribution where the location parameter \\(\\alpha\\) is the mean (and mode) of the distribution while the variance is: \\[ Var\\left( x \\right) = \\frac{{\\beta \\nu }}{{\\left( {\\nu - 2} \\right)}}. \\] For the purposes of standardization we require that: \\[ \\begin{gathered} Var(x) = \\frac{{\\beta \\nu }} {{\\left( {\\nu - 2} \\right)}} = 1 \\hfill \\\\ \\therefore \\beta = \\frac{{\\nu - 2}} {\\nu } \\hfill \\\\ \\end{gathered} \\] Substituting \\(\\frac{(\\nu- 2)}{\\nu }\\) we obtain the standardized Student’s distribution: \\[ f\\left( {\\frac{{x - \\mu }}{\\sigma }} \\right) = \\frac{1} {\\sigma }f\\left( z \\right) = \\frac{1} {\\sigma }\\frac{{\\Gamma \\left( {\\frac{{\\nu + 1}} {2}} \\right)}}{{\\sqrt {\\left( {\\nu - 2} \\right)\\pi } \\Gamma \\left( {\\frac{\\nu } {2}} \\right)}}{\\left( {1 + \\frac{{{z^2}}} {{\\left( {\\nu - 2} \\right)}}} \\right)^{ - \\left( {\\frac{{\\nu + 1}} {2}} \\right)}}. \\] In terms of R’s standard implementation of the Student density (dt), and including a scaling by the standard deviation, this can be represented as: \\[ \\frac{{dt\\left( {\\frac{{{\\varepsilon _t}}} {{\\sigma \\sqrt {\\left( {v - 2} \\right)/\\nu } }},\\nu } \\right)}} {{\\sigma \\sqrt {\\left( {v - 2} \\right)/\\nu } }} \\] The Student distribution has zero skewness and excess kurtosis equal to \\(6/(\\nu - 4)\\) for \\(\\nu &gt; 4\\). 9.2.3 The Generalized Error Distribution The Generalized Error Distribution is a 3 parameter distribution belonging to the exponential family with conditional density given by, \\[ f\\left( x \\right) = \\frac{{\\kappa {e^{ - 0.5{{\\left| {\\frac{{x - \\alpha }} {\\beta }} \\right|}^\\kappa }}}}} {{{2^{1 + {\\kappa ^{ - 1}}}}\\beta \\Gamma \\left( {{\\kappa ^{ - 1}}} \\right)}} \\] with \\(\\alpha\\), \\(\\beta\\) and \\(\\kappa\\) representing the location, scale and shape parameters. Since the distribution is symmetric and unimodal the location parameter is also the mode, median and mean of the distribution (i.e. \\(\\mu\\)). By symmetry, all odd moments beyond the mean are zero. The variance and kurtosis are given by, \\[ \\begin{gathered} Var\\left( x \\right) = {\\beta ^2}{2^{2/\\kappa }}\\frac{{\\Gamma \\left( {3{\\kappa ^{ - 1}}} \\right)}} {{\\Gamma \\left( {{\\kappa ^{ - 1}}} \\right)}} \\hfill \\\\ Ku\\left( x \\right) = \\frac{{\\Gamma \\left( {5{\\kappa ^{ - 1}}} \\right)\\Gamma \\left( {{\\kappa ^{ - 1}}} \\right)}} {{\\Gamma \\left( {3{\\kappa ^{ - 1}}} \\right)\\Gamma \\left( {3{\\kappa ^{ - 1}}} \\right)}} \\hfill \\\\ \\end{gathered} \\] As \\(\\kappa\\) decreases the density gets flatter and flatter while in the limit as \\(\\kappa \\to \\infty\\), the distribution tends towards the uniform. Special cases are the Normal when \\(\\kappa=2\\), the Laplace when \\(\\kappa=1\\). Standardization is simple and involves re-scaling the density to have unit standard deviation: \\[ \\begin{gathered} Var\\left( x \\right) = {\\beta ^2}{2^{2/\\kappa }}\\frac{{\\Gamma \\left( {3{\\kappa ^{ - 1}}} \\right)}} {{\\Gamma \\left( {{\\kappa ^{ - 1}}} \\right)}} = 1 \\hfill \\\\ \\therefore \\beta = \\sqrt {{2^{ - 2/\\kappa }}\\frac{{\\Gamma \\left( {{\\kappa ^{ - 1}}} \\right)}} {{\\Gamma \\left( {3{\\kappa ^{ - 1}}} \\right)}}} \\hfill \\\\ \\end{gathered} \\] Finally, substituting into the scaled density of \\(z\\): \\[ f\\left( {\\frac{{x - \\mu }} {\\sigma }} \\right) = \\frac{1} {\\sigma }f\\left( z \\right) = \\frac{1} {\\sigma }\\frac{{\\kappa {e^{ - 0.5{{\\left| {\\sqrt {{2^{ - 2/\\kappa }}\\frac{{\\Gamma \\left( {{\\kappa ^{ - 1}}} \\right)}} {{\\Gamma \\left( {3{\\kappa ^{ - 1}}} \\right)}}} z} \\right|}^\\kappa }}}}} {{\\sqrt {{2^{ - 2/\\kappa }}\\frac{{\\Gamma \\left( {{\\kappa ^{ - 1}}} \\right)}} {{\\Gamma \\left( {3{\\kappa ^{ - 1}}} \\right)}}} {2^{1 + {\\kappa ^{ - 1}}}}\\Gamma \\left( {{\\kappa ^{ - 1}}} \\right)}} \\] 9.2.4 Skewed Distributions by Inverse Scale Factors Fernandez and Steel (1998) proposed introducing skewness into unimodal and symmetric distributions by introducing inverse scale factors in the positive and negative real half lines. Given a skew parameter, \\(\\xi\\), the density of a random variable z can be represented as: \\[ f\\left( {z|\\xi } \\right) = \\frac{2} {{\\xi + {\\xi ^{ - 1}}}}\\left[ {f\\left( {\\xi z} \\right)H\\left( { - z} \\right) + f\\left( {{\\xi ^{ - 1}}z} \\right)H\\left( z \\right)} \\right] \\] where \\(\\xi \\in {\\mathbb{R}^ + }\\) and \\(H(.)\\) is the Heaviside function. The absolute moments, required for deriving the central moments, are generated from the following function: \\[ {M_r} = 2\\int_0^\\infty {{z^r}f\\left( z \\right)dz}. \\] The mean and variance are then defined as: \\[ \\begin{gathered} E\\left( z \\right) = {M_1}\\left( {\\xi - {\\xi ^{ - 1}}} \\right) \\hfill \\\\ Var\\left( z \\right) = \\left( {{M_2} - M_1^2} \\right)\\left( {{\\xi ^2} + {\\xi ^{ - 2}}} \\right) + 2M_1^2 - {M_2} \\hfill \\\\ \\end{gathered} \\] The Normal, Student and GED distributions have skew variants which have been standardized to zero mean, unit variance by making use of the moment conditions given above. These are named as snorm sged and sstd in the package. 9.2.5 The Generalized Hyperbolic Distribution and Sub-Families In distributions where the expected moments are functions of all the parameters, it is not immediately obvious how to perform such a transformation. In the case of the ghyp distribution, because of the existence of location and scale invariant parameterizations and the possibility of expressing the variance in terms of one of those, namely the \\((\\zeta, \\rho)\\), the task of standardizing and estimating the density can be broken down to one of estimating those 2 parameters, representing a combination of shape and skewness, followed by a series of transformation steps to demean, scale and then translate the parameters into the \\((\\alpha, \\beta, \\delta, \\mu)\\) parameterization for which standard formula exist for the likelihood function. The \\((\\xi, \\chi)\\) parameterization, which is a simple transformation of the \\((\\zeta, \\rho)\\), could also be used in the first step and then transformed into the latter before proceeding further. The only difference is the kind of ‘immediate’ inference one can make from the different parameterizations, each providing a different direct insight into the kind of dynamics produced and their place in the overall ghyp family particularly with regards to the limit cases. The package performs estimation using the \\((\\zeta, \\rho)\\) parameterization, after which a series of steps transform those parameters into the \\((\\alpha, \\beta, \\delta, \\mu)\\) while at the same time including the necessary recursive substitution of parameters in order to standardize the resulting distribution. Proof. The Standardized Generalized Hyperbolic Distribution. Let \\(\\varepsilon_t\\) be a r.v. with mean \\((0)\\) and variance \\(({\\sigma}^2)\\) distributed as \\(ghyp(\\zeta, \\rho)\\), and let \\(z\\) be a scaled version of the r.v. \\(\\varepsilon\\) with variance \\((1)\\) and also distributed as \\(ghyp(\\zeta, \\rho)\\).21 The density \\(f(.)\\) of \\(z\\) can be expressed as \\[ f(\\frac{\\varepsilon_t}{\\sigma}; \\zeta ,\\rho ) = \\frac{1}{\\sigma}f_t(z;\\zeta ,\\rho ) = \\frac{1}{\\sigma}f_t(z;\\tilde \\alpha, \\tilde \\beta, \\tilde \\delta ,\\tilde \\mu ), \\] where we make use of the \\((\\alpha, \\beta, \\delta, \\mu)\\) parameterization since we can only naturally express the density in that parameterization. The steps to transforming from the \\((\\zeta, \\rho)\\) to the \\((\\alpha, \\beta, \\delta, \\mu)\\) parameterization, while at the same time standardizing for zero mean and unit variance are given henceforth. Let \\[ \\begin{eqnarray} \\zeta &amp; = &amp; \\delta \\sqrt {{\\alpha ^2} - {\\beta ^2}} \\hfill \\\\ \\rho &amp; = &amp; \\frac{\\beta }{\\alpha }, \\hfill \\end{eqnarray} \\] which after some substitution may be also written in terms of \\(\\alpha\\) and \\(\\beta\\) as, \\[ \\begin{eqnarray} \\alpha &amp; = &amp; \\frac{\\zeta }{{\\delta \\sqrt {(1 - {\\rho ^2})} }},\\hfill\\\\ \\beta &amp; = &amp;\\alpha \\rho.\\hfill \\end{eqnarray} \\] For standardization we require that, \\[ \\begin{eqnarray} E\\left(X\\right) &amp; = &amp; \\mu + \\frac{{\\beta \\delta }}{{\\sqrt {{\\alpha ^2} - {\\beta ^2}} }}\\frac{{{K_{\\lambda + 1}}\\left(\\zeta \\right)}}{{{K_\\lambda }\\left(\\zeta \\right)}} = \\mu + \\frac{{\\beta {\\delta ^2}}}{\\zeta }\\frac{{{K_{\\lambda + 1}}\\left(\\zeta \\right)}}{{{K_\\lambda }\\left(\\zeta \\right)}} = 0 \\hfill \\\\ \\therefore \\mu &amp; = &amp; - \\frac{{\\beta {\\delta ^2}}}{\\zeta }\\frac{{{K_{\\lambda + 1}}\\left(\\zeta \\right)}}{{{K_\\lambda }\\left(\\zeta \\right)}}\\hfill \\\\ Var\\left(X\\right) &amp; = &amp; {\\delta ^2}\\left(\\frac{{{K_{\\lambda + 1}}\\left(\\zeta \\right)}}{{\\zeta {K_\\lambda }\\left(\\zeta \\right)}} + \\frac{{{\\beta ^2}}}{{{\\alpha ^2} - {\\beta ^2}}}\\left(\\frac{{{K_{\\lambda + 2}}\\left(\\zeta \\right)}}{{{K_\\lambda }\\left(\\zeta \\right)}} - {\\left(\\frac{{{K_{\\lambda + 1}}\\left(\\zeta \\right)}}{{{K_\\lambda }\\left(\\zeta \\right)}}\\right)^2}\\right)\\right) = 1 \\hfill\\nonumber \\\\ \\therefore \\delta &amp; = &amp; {\\left(\\frac{{{K_{\\lambda + 1}}\\left(\\zeta \\right)}}{{\\zeta {K_\\lambda }\\left(\\zeta \\right)}} + \\frac{{{\\beta ^2}}}{{{\\alpha ^2} - {\\beta ^2}}}\\left(\\frac{{{K_{\\lambda + 2}}\\left(\\zeta \\right)}}{{{K_\\lambda }\\left(\\zeta \\right)}} - {\\left(\\frac{{{K_{\\lambda + 1}}\\left(\\zeta \\right)}}{{{K_\\lambda }\\left(\\zeta \\right)}}\\right)^2}\\right)\\right)^{ - 0.5}} \\hfill \\end{eqnarray} \\] Since we can express, \\(\\beta^2/\\left(\\alpha^2 - \\beta^2\\right)\\) as, \\[ \\frac{{{\\beta ^2}}}{{{\\alpha ^2} - {\\beta ^2}}} = \\frac{{{\\alpha ^2}{\\rho ^2}}}{{{a^2} - {\\alpha ^2}{\\rho ^2}}} = \\frac{{{\\alpha ^2}{\\rho ^2}}}{{{a^2}\\left(1 - {\\rho ^2}\\right)}} = \\frac{{{\\rho ^2}}}{{\\left(1 - {\\rho ^2}\\right)}}, \\] then we can re-write the formula for \\(\\delta\\) in terms of the estimated parameters \\(\\hat\\zeta\\) and \\(\\hat\\rho\\) as, \\[ \\delta = {\\left(\\frac{{{K_{\\lambda + 1}}\\left(\\hat \\zeta \\right)}}{{\\hat \\zeta {K_\\lambda }\\left(\\hat \\zeta \\right)}} + \\frac{{{{\\hat \\rho }^2}}}{{\\left(1 - {{\\hat \\rho }^2}\\right)}}\\left(\\frac{{{K_{\\lambda + 2}}\\left(\\hat \\zeta \\right)}}{{{K_\\lambda }\\left(\\hat \\zeta \\right)}} - {\\left(\\frac{{{K_{\\lambda + 1}}\\left(\\hat \\zeta \\right)}}{{{K_\\lambda }\\left(\\hat \\zeta \\right)}}\\right)^2}\\right)\\right)^{ - 0.5}} \\] Transforming into the \\((\\tilde \\alpha ,\\tilde \\beta ,\\tilde \\delta ,\\tilde \\mu )\\) parameterization proceeds by first substituting \\(\\ref{sgh44}\\) into \\(\\ref{sgh31}\\) and simplifying, \\[ \\begin{eqnarray} \\tilde \\alpha &amp; = &amp; \\,{\\frac{{\\hat \\zeta \\left( {\\frac{{{{\\text{K}}_{\\lambda + 1}}\\left( {\\hat \\zeta } \\right)}}{{\\hat \\zeta {{\\text{K}}_\\lambda }\\left( {\\hat \\zeta } \\right)}} + \\frac{{{{\\hat \\rho }^2}\\left( {\\frac{{{{\\text{K}}_{\\lambda + 2}}\\left( {\\hat \\zeta } \\right)}}{{{{\\text{K}}_\\lambda }\\left( {\\hat \\zeta } \\right)}} - \\frac{{{{\\left( {{{\\text{K}}_{\\lambda + 1}}\\left( {\\hat \\zeta } \\right)} \\right)}^2}}}{{{{\\left( {{{\\text{K}}_\\lambda }\\left( {\\hat \\zeta } \\right)} \\right)}^2}}}} \\right)}}{{\\left( {1 - {{\\hat \\rho }^2}} \\right)}}} \\right)}}{{\\sqrt {(1 - {{\\hat \\rho }^2})} }}^{0.5}}, \\hfill\\nonumber \\\\ &amp; = &amp;\\,{\\frac{{\\left( {\\frac{{\\hat \\zeta {{\\text{K}}_{\\lambda + 1}}\\left( {\\hat \\zeta } \\right)}}{{{{\\text{K}}_\\lambda }\\left( {\\hat \\zeta } \\right)}} + \\frac{{{{\\hat \\zeta }^2}{{\\hat \\rho }^2}\\left( {\\frac{{{{\\text{K}}_{\\lambda + 2}}\\left( {\\hat \\zeta } \\right)}}{{{{\\text{K}}_\\lambda }\\left( {\\hat \\zeta } \\right)}} - \\frac{{{{\\left( {{{\\text{K}}_{\\lambda + 1}}\\left( {\\hat \\zeta } \\right)} \\right)}^2}}}{{{{\\left( {{{\\text{K}}_\\lambda }\\left( {\\hat \\zeta } \\right)} \\right)}^2}}}} \\right)}}{{\\left( {1 - {{\\hat \\rho }^2}} \\right)}}} \\right)}}{{\\sqrt {(1 - {\\hat \\rho ^2})} }}^{0.5}}, \\hfill\\nonumber \\\\ &amp; = &amp; {\\left( {\\left. {\\frac{{\\frac{{\\hat \\zeta {{\\text{K}}_{\\lambda + 1}}\\left( {\\hat \\zeta } \\right)}}{{{{\\text{K}}_\\lambda }\\left( {\\hat \\zeta } \\right)}}}}{{(1 - {{\\hat \\rho }^2})}} + \\frac{{{\\hat \\zeta ^2}{\\hat \\rho ^2}\\left( {\\frac{{{{\\text{K}}_{\\lambda + 2}}\\left( {\\hat \\zeta } \\right)}}{{{{\\text{K}}_{\\lambda + 1}}\\left( {\\hat \\zeta } \\right)}}\\frac{{{{\\text{K}}_{\\lambda + 1}}\\left( {\\hat \\zeta } \\right)}}{{{{\\text{K}}_\\lambda }\\left( {\\hat \\zeta } \\right)}} - \\frac{{{{\\left( {{{\\text{K}}_{\\lambda + 1}}\\left( {\\hat \\zeta } \\right)} \\right)}^2}}}{{{{\\left( {{{\\text{K}}_\\lambda }\\left( {\\hat \\zeta } \\right)} \\right)}^2}}}} \\right)}}{{{{\\left( {1 - {{\\hat \\rho }^2}} \\right)}^2}}}} \\right)} \\right.^{0.5}}, \\hfill\\nonumber \\\\ &amp; = &amp; {\\left( {\\left. {\\frac{{\\frac{{\\hat \\zeta {{\\text{K}}_{\\lambda + 1}}\\left( {\\hat \\zeta } \\right)}}{{{{\\text{K}}_\\lambda }\\left( {\\hat \\zeta } \\right)}}}}{{(1 - {{\\hat \\rho }^2})}}\\left(1 + \\frac{{\\hat \\zeta {{\\hat \\rho }^2}\\left( {\\frac{{{{\\text{K}}_{\\lambda + 2}}\\left( {\\hat \\zeta } \\right)}}{{{{\\text{K}}_{\\lambda + 1}}\\left( {\\hat \\zeta } \\right)}} - \\frac{{{{\\text{K}}_{\\lambda + 1}}\\left( {\\hat \\zeta } \\right)}}{{{{\\text{K}}_\\lambda }\\left( {\\hat \\zeta } \\right)}}} \\right)}}{{\\left( {1 - {{\\hat \\rho }^2}} \\right)}}\\right)} \\right)} \\right.^{0.5}}. \\hfill \\end{eqnarray} \\] Finally, the rest of the parameters are derived recursively from \\(\\tilde\\alpha\\) and the previous results, \\[ \\begin{eqnarray} \\tilde \\beta &amp; = &amp; \\tilde \\alpha \\hat \\rho,\\hfill\\\\ \\tilde \\delta &amp; = &amp; \\frac{{\\hat \\zeta }}{{\\tilde \\alpha \\sqrt {1 - {{\\hat \\rho }^2}} }}, \\hfill\\\\ \\tilde \\mu &amp; = &amp; \\frac{{ - \\tilde \\beta {{\\tilde \\delta }^2}{K_{\\lambda + 1}}\\left(\\hat \\zeta \\right)}}{{\\hat \\zeta {K_\\lambda }\\left(\\hat \\zeta \\right)}}.\\hfill \\end{eqnarray} \\] For the use of the \\((\\xi, \\chi)\\) parameterization in estimation, the additional preliminary steps of converting to the \\((\\zeta, \\rho)\\) are, \\[ \\begin{eqnarray} \\zeta &amp; = &amp; \\frac{1}{{{{\\hat \\xi }^2}}} - 1, \\hfill\\\\ \\rho &amp; = &amp; \\frac{{\\hat \\chi }}{{\\hat \\xi }}. \\hfill \\end{eqnarray} \\] 9.2.6 The Generalized Hyperbolic Skew Student Distribution The ghyp Skew-Student distribution was popularized by Aas and Haff (2006) because of its uniqueness in the ghyp family in having one tail with polynomial and one with exponential behavior. This distribution is a limiting case of the ghyp when \\(\\alpha \\to \\left| \\beta \\right|\\) and \\(\\lambda=-\\nu/2\\), where \\(\\nu\\) is the shape parameter of the Student distribution. The domain of variation of the parameters is \\(\\beta \\in \\mathbb{R}\\) and \\(\\nu&gt;0\\), but for the variance to be finite \\(\\nu&gt;4\\), while for the existence of skewness and kurtosis, \\(\\nu&gt;6\\) and \\(\\nu&gt;8\\) respectively. The density of the random variable \\(x\\) is then given by: \\[ f\\left( x \\right) = \\frac{{{2^{\\left( {1 - \\nu } \\right)/2}}{\\delta ^\\nu }{{\\left| \\beta \\right|}^{\\left( {\\nu + 1} \\right)/2}}{K_{\\left( {\\nu + 1} \\right)/2}}\\left( {\\sqrt {{\\beta ^2}\\left( {{\\delta ^2} + {{\\left( {x - \\mu } \\right)}^2}} \\right)} } \\right)\\exp \\left( {\\beta \\left( {x - \\mu } \\right)} \\right)}} {{\\Gamma \\left( {\\nu /2} \\right)\\sqrt \\pi {{\\left( {\\sqrt {{\\delta ^2} + {{\\left( {x - \\mu } \\right)}^2}} } \\right)}^{\\left( {\\nu + 1} \\right)/2}}}} \\] To standardize the distribution to have zero mean and unit variance, I make use of the first two moment conditions for the distribution which are: \\[ \\begin{gathered} E\\left( x \\right) = \\mu + \\frac{{\\beta {\\delta ^2}}} {{\\nu - 2}} \\hfill \\\\ Var\\left( x \\right) = \\frac{{2{\\beta ^2}{\\delta ^4}}} {{{{\\left( {\\nu - 2} \\right)}^2}\\left( {\\nu - 4} \\right)}} + \\frac{{{\\delta ^2}}} {{\\nu - 2}} \\hfill \\\\ \\end{gathered} \\] We require that \\(Var(x)=1\\), thus: \\[ \\delta = {\\left( {\\frac{{2{{\\bar \\beta }^2}}} {{{{\\left( {\\nu - 2} \\right)}^2}\\left( {\\nu - 4} \\right)}} + \\frac{1} {{\\nu - 2}}} \\right)^{ - 1/2}} \\] where I have made use of the \\(4^{th}\\) parameterization of the ghyp distribution given in Prause (1999) where \\(\\hat \\beta = \\beta \\delta\\). The location parameter is then rescaled by substituting into the first moment formula \\(\\delta\\) so that it has zero mean: \\[ \\bar \\mu = - \\frac{{\\beta {\\delta ^2}}}{{\\nu - 2}} \\] Therefore, we model the ghyp Skew-Student using the location-scale invariant parameterization \\((\\bar \\beta, \\nu)\\) and then translate the parameters into the usual ghyp distribution’s \\((\\alpha, \\beta, \\delta, \\mu)\\), setting \\(\\alpha = abs(\\beta)+1e-12\\). The quantile function is calculated using the SkewHyperbolic package of D. Scott and Grimson (2018) using the spline method (for speed), as is the cumulative distribution function. 9.2.7 Johnson’s reparameterized SU Distribution The reparameterized Johnson SU distribution, discussed in Rigby and Stasinopoulos (2005), is a four parameter distribution denoted by \\(JSU\\left(\\mu,\\sigma,\\nu,\\tau\\right)\\), with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) for all values of the skew and shape parameters \\(\\nu\\) and \\(\\tau\\) respectively. The implementation is taken from the GAMLSS package of Stasinopoulos, Rigby, and Akantziliotou (2009) and the reader is referred there for further details. 9.3 A note on analytic higher moments The skewness and kurtosis of all distributions is based on analytic forms, with the exception of the skew student distribution for which a bijection between this and Hansen (1994) Generalized Skew-T distribution is used.22 9.4 Implementation and Code Examples The package implements standard d,p,q,r functions for all distributions in addition to the wrapper functions ddist,pdist,qdist and rdist. The skewness and kurtosis are provider in wrapper functions for all distributions in dskewness and dkurtosis as well as the tsmoments function on an estimated object of class tsdistribution.estimate. The following short demo showcases some of the available functionality. library(tsdistributions) dist &lt;- &quot;ghst&quot; r &lt;- rdist(dist, 2000, mu = 0.1, sigma = 0.3, skew = -10, shape = 5) spec &lt;- distribution_modelspec(r, dist) mod &lt;- estimate(spec) summary(mod) ## ## ## Parameter Est[Value] Std. Error t value Pr(&gt;|t|) ## ---------- ----------- ----------- -------- --------- ## mu 0.1061 0.0047 22.6870 0.0000 ## sigma 0.2222 0.0247 8.9969 0.0000 ## skew -11.2932 3.9833 -2.8352 0.0046 ## shape 5.9021 0.5673 10.4041 0.0000 pars &lt;- coef(mod) # equivalence rx &lt;- (r - pars[1])/pars[2] test_llh &lt;- sum(-log(ddist(dist, rx, mu = 0, sigma = 1, skew = pars[3], shape = pars[4])/pars[2])) all.equal(test_llh, mod$llh) ## [1] TRUE References "],["tsgam.html", "Chapter 10 tsgam package 10.1 Introduction 10.2 Package Demo: Electricity Load", " Chapter 10 tsgam package 10.1 Introduction The tsgam package provides a custom wrapper of the gam function from the mgcv package of Wood (2022), in order to conform to the methods and outputs of the tsmodels framework. In particular, the predict method will return the simulated distribution of the mean forecast, and optionally includes the possibility of metafitting and using one of the distributions from the tsdistributions package for the uncertainty. The reason for introducing this model into the framework is due to the flexibility and robustness of the smooth basis functions in Generalized Additive Models (GAMs), the speed of estimation when working with large data sets, the ability to capture complex and multiple seasonal periods and potentially many regressors which are available at the forecast horizon which may not be purely linear in the response. 10.2 Package Demo: Electricity Load We highlight the package functionality using the Total electricity load in Greece dataset from the tsdatasets package. 10.2.1 Estimation The model specification, unlike previous packages in the tsmodels repo, takes as argument a formula describing the GAM model, in line with how many linear models work in R. This is a flexible approach, but does require an understanding of the types of smooth constructs available in mgcv. We first load the data and construct some additional information from the index and load information. We use the 48 hour lagged value of load to be conservative in terms of release schedules of the actual load and forecast delays. We also construct hour of day, day of week and month of year variables to capture stable seasonal patterns using cyclic cubic regression splines. In practice, one would hopefully have more information to include such as forecast temperatures, forecast demand etc. Sys.setenv(TZ = &quot;UTC&quot;) library(tsdatasets) library(tsgam) library(tsaux) library(data.table) library(xts) library(gratia) library(visreg) library(ggplot2) x &lt;- tsdatasets::electricload x &lt;- data.table(index = index(x), load = as.numeric(x)) x[,lag48 := as.numeric(lag.xts(xts(load, index),48))] x[,hour := hour(index)] x[,day := wday(index)] x[,month := month(index)] x &lt;- na.omit(as.xts(x)) head(x) ## load lag48 hour day month ## 2016-10-19 00:00:00 3847 3745 0 4 10 ## 2016-10-19 01:00:00 3838 3702 1 4 10 ## 2016-10-19 02:00:00 3965 3783 2 4 10 ## 2016-10-19 03:00:00 4351 4139 3 4 10 ## 2016-10-19 04:00:00 4978 4691 4 4 10 ## 2016-10-19 05:00:00 5520 5104 5 4 10 We’ll estimate 2 models and compare them using an F-test. The first model simply uses the 48 hour lag of the load for prediction, whilst the second model also makes use of the seasonal factors. We’ll also use approximately 90% of the data for training and leave 10% for out of sample prediction. train_dataset &lt;- x[1:20000] test_dataset &lt;- x[20001:nrow(x)] f_baseline &lt;- &#39;load~s(lag48, bs = &quot;tp&quot;)&#39; f_extended &lt;- &#39;load~s(lag48, bs = &quot;tp&quot;) + s(hour, bs = &quot;cc&quot;, k = 23) + s(day, bs = &quot;cc&quot;, k = 6) + s(month, bs = &quot;cc&quot;, k = 11)&#39; spec_baseline &lt;- gam_modelspec(f_baseline, data = train_dataset) spec_extended &lt;- gam_modelspec(f_extended, data = train_dataset) mod_baseline &lt;- estimate(spec_baseline, select = TRUE, method = &quot;REML&quot;) mod_extended &lt;- estimate(spec_extended, select = TRUE, method = &quot;REML&quot;) The estimated objects include a slot for model which is the mgcv estimated object, from which we can use existing methods for evaluation. We first compare the extended model with the baseline model using a F-test. anova(mod_baseline$model, mod_extended$model, test = &quot;F&quot;) ## Analysis of Deviance Table ## ## Model 1: load ~ s(lag48, bs = &quot;tp&quot;) ## Model 2: load ~ s(lag48, bs = &quot;tp&quot;) + s(hour, bs = &quot;cc&quot;, k = 23) + s(day, ## bs = &quot;cc&quot;, k = 6) + s(month, bs = &quot;cc&quot;, k = 11) ## Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) ## 1 19991 6386420897 ## 2 19960 3728763057 31.286 2657657840 454.78 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Even though the extended model has significantly more parameters, the test indicates that it provides a better fit than the baseline. We therefore proceed with this model. summary(mod_extended) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## load ~ s(lag48, bs = &quot;tp&quot;) + s(hour, bs = &quot;cc&quot;, k = 23) + s(day, ## bs = &quot;cc&quot;, k = 6) + s(month, bs = &quot;cc&quot;, k = 11) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5902.478 3.056 1931 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(lag48) 7.785 9 1226.1 &lt;2e-16 *** ## s(hour) 15.517 21 133.3 &lt;2e-16 *** ## s(day) 3.996 4 1755.0 &lt;2e-16 *** ## s(month) 8.874 9 270.8 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.853 Deviance explained = 85.4% ## -REML = 1.4984e+05 Scale est. = 1.8679e+05 n = 20000 The extended model explains about 85% of the deviance. Looking at the QQ plot of the residuals using the appraise function from the gratia package suggests that the Gaussian assumptions is reasonable. appraise(mod_extended$model) The figure below shows the marginal effect plot of the months on Load, with a clear peak during summer and winter months as load goes up due to temperature and hence the use of air-conditioners and heaters respectively (although the inference will be slightly misleading here since we are using UTC whereas the local time zone is EET). visreg(fit = mod_extended$model, xvar = c(&quot;month&quot;), data = as.data.frame(train_dataset), type = &quot;conditional&quot;, gg = T, overlay = T, partial = F, rug = F, ylab = &quot;Load (MW)&quot;, xlab = &quot;Month&quot;) + ggtitle(&quot;Month&quot;) + ggthemes::theme_pander() We can also plot the additive decomposition of the components using the existing tsdecompose method which is available for both estimate and predicted objects. tsx &lt;- tsdecompose(mod_extended) # confirm decomposition composure = fitted all.equal(as.numeric(rowSums(tsx)), as.numeric(fitted(mod_extended))) ## [1] TRUE plot(tsx[,-1], multi.panel = TRUE, main = &quot;&quot;) 10.2.2 Prediction The prediction method does not have any n.ahead arguments as in other time series methods in the tsmodels framework, instead being completely determined by the newdata argument. Additionally, the argument distribution can be used to proxy the simulated distribution by any other in the tsdistributions package if it is believed that the Gaussian assumption is too restrictive. Choosing another distribution will lead to the estimation of the distributional parameters on the response residuals followed by simulation of the distribution given these parameters and the forecast mean. Comparison of different simulated predictive distributions can then be carried out using for instance the CRSP or MIS functions from the tsaux package. A further argument, tabulate, returns a data.table object of the draws, forecast dates and predictions in long format which may be useful in production settings when such values need to be stored in a database. p_gam &lt;- predict(mod_extended, newdata = test_dataset[,c(&quot;lag48&quot;, &quot;hour&quot;, &quot;day&quot;, &quot;month&quot;)], nsim = 5000, tabular = FALSE, distribution = NULL) Note that, even though the training dataset is of length 20,000 (hours), this does not mean that we are performing such a long range forecast since the 48 hour lag of the actual load effectively allows us to forecast, at time T, from T+1 to T+48. Without re-estimation, this is equivalent, in the other state space type models, to a horizon of 48 with updating every 48 hours (filtering with new data and then re-forecasting). An example and comparison is provided in the next section. Plotting the simulated predictive distribution against the realized values indicates reasonable performance, which we confirm with a table of statistics. plot(p_gam, n_original = 24*30*2, gradient_color = &quot;white&quot;, interval_color = &quot;grey&quot;, median_width = 2) lines(index(test_dataset), as.numeric(test_dataset$load), col = 2, lwd = 0.5, lty = 2) p_zoom &lt;- p_gam$distribution[,1:(24*10)] class(p_zoom) &lt;- &quot;tsmodel.distribution&quot; plot(p_zoom, date_class = &quot;POSIXct&quot;, main = &quot;Prediction Sub-Period&quot;) lines(index(test_dataset[colnames(p_zoom)]), as.numeric(test_dataset[colnames(p_zoom)]$load), col = 2, lwd = 2, lty = 2) print(data.table(`MAPE (%)` = mape(test_dataset$load, p_gam$mean) * 100, `SMAPE (%)` = smape(test_dataset$load, p_gam$mean) * 100, `BIAS (%)` = bias(test_dataset$load, p_gam$mean) * 100, CRPS = crps(test_dataset$load, p_gam$distribution)), digits = 3) ## MAPE (%) SMAPE (%) BIAS (%) CRPS ## &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; ## 1: 6.06 3.04 -0.823 251 10.2.3 Comparison to ISSM model We compare the results of the GAM model to one estimated using the ISSM model from the tsissm package. library(tsissm) spec &lt;- issm_modelspec(train_dataset$load, slope = FALSE, seasonal = TRUE, seasonal_frequency = c(24, 24*30.25), seasonal_harmonics = c(7,7), seasonal_type = &quot;trigonometric&quot;, sampling = &quot;hours&quot;) mod &lt;- estimate(spec, autodiff = TRUE) s &lt;- seq(0, nrow(test_dataset), by = 48) p_issm &lt;- predict(mod, h = 48, exact_moments = T) pred_list &lt;- vector(mode = &quot;list&quot;, length = length(s)) pred_list[[1]] &lt;- data.table(index = 1, forecast_dates = index(p_issm$analytic_mean), issm_forecast = as.numeric(p_issm$analytic_mean)) for (i in 2:length(s)) { xmod &lt;- tsfilter(mod, y = test_dataset[1:s[i]]$load) p_issm &lt;- predict(xmod, h = 48, exact_moments = T) pred_list[[i]] &lt;- data.table(index = i, forecast_dates = index(p_issm$analytic_mean), issm_forecast = as.numeric(p_issm$analytic_mean)) } pred_list &lt;- rbindlist(pred_list) pred_list &lt;- merge(pred_list, data.table(forecast_dates = index(test_dataset), actual = as.numeric(test_dataset$load)), by = &quot;forecast_dates&quot;) gamp &lt;- data.table(forecast_dates = index(p_gam$mean), gam_forecast = as.numeric(p_gam$mean)) pred_list &lt;- merge(pred_list, gamp, by = &quot;forecast_dates&quot;) print(data.table(&quot;Metric&quot; = c(&quot;MAPE (%)&quot;,&quot;SMAPE (%)&quot;,&quot;BIAS (%)&quot;), &quot;ISSM&quot; = c( mape(pred_list$actual, pred_list$issm_forecast) * 100, smape(pred_list$actual, pred_list$issm_forecast) * 100, bias(pred_list$actual, pred_list$issm_forecast) * 100), &quot;GAM&quot; = c( mape(pred_list$actual, pred_list$gam_forecast) * 100, smape(pred_list$actual, pred_list$gam_forecast) * 100, bias(pred_list$actual, pred_list$gam_forecast) * 100) ), digits = 3) ## Metric ISSM GAM ## &lt;char&gt; &lt;num&gt; &lt;num&gt; ## 1: MAPE (%) 7.69 6.061 ## 2: SMAPE (%) 3.82 3.038 ## 3: BIAS (%) 1.84 -0.823 The simple GAM model appears to outperform, for this series, the ISSM model on all 3 metrics. There is also a tsbacktest method which will be expanded on in a separate post as this takes a training and testing split list of time indices which departs from the current approach in the other packages but offers more flexibility and is more suitable for models of this type. References "],["other.html", "Chapter 11 Other packages", " Chapter 11 Other packages The tsaux package contains a set of functions used across all the packages of our framework. Included are performance metrics such as mape and crps as well as the tstransform function which includes the Box-Cox and Logit transformations and their inverse. Additionally, the auto_regressors and auto_clean functions provide for automatic anomaly detection and decontamination making use of a customized stepwise routine which includes the use of the tso function from the tsoutliers package, capturing additive ouutliers, temporary changes and permanent level shifts. The tsdatasets packages contains some sample datasets we use for benchmarking and examples. This may be expanded in the future. "],["experimental.html", "Chapter 12 Experimental Functionality 12.1 Calibration of Predictive Distributions 12.2 Beyond Box-Cox : The Logit-Normal Assumption for Ratios", " Chapter 12 Experimental Functionality We have included some experimental functionality which we provide details of here. 12.1 Calibration of Predictive Distributions Many time series models due to their parametric nature make strong assumptions about both the shape of the predictive distribution (shape) as well as the multi-step ahead evolution of the variance (scale). In practice we find that both the shape and form of the variance curve results in under or over sizing as measured by the % of exceedances falling within each quantile. In the additive ETS model without seasonality (AAN), the shape of the standardized error distribution is N(0,1) while the variance evolves based on the following formula: \\[ v_{t+h|t} = \\sigma^2\\left[1 + \\left(h-1\\right)\\left\\{\\alpha^2 + \\alpha\\beta h + \\frac{1}{6}\\beta^2 h \\left(2h - 1\\right)\\right\\}\\right] \\] which is linear in h when the coefficient on the slope is zero and cubic otherwise. The issue of well calibrated distributions has been discussed in many papers, particularly in the ML domain for both regression and classification. The purpose of the calibration function is to empirically adjust the multi-step variance based on the observed variance from a rolling prediction (backtest) as well as provide an alternative empirical sampling for the shape of the distribution. 12.1.1 Example [Coming Soon] 12.2 Beyond Box-Cox : The Logit-Normal Assumption for Ratios For series which are strongly bounded, for instance ratios or probabilities, a simple approach is to assume that the log transformed odd ratios are normal, leading to a logit-normal distributional assumption, which is a simpler alternative to using the Beta distribution. The tstransform function provides the option for using either the “box-cox” or “logit” transforms, returning both the transform and inverse functions which are used by a number of packages in our framework. The transformation argument in the model specification across most of the internally developed packages will now accept either “box-cox” or “logit” as valid options. "],["news.html", "Chapter 13 News", " Chapter 13 News Date: 2022-05-12 Added the tsdistributions package. Estimation uses automatic differentiation (AD) as in the other packages. Date: 2022-04-30 Missing value handling now available in the tsvets package for both estimation and filtering. Date: 2022-04-13 Most of the packages have replaced the use of snow and foreach with the functionality of the future package. This eliminates the cores argument in many functions (such as tsbacktest) and hence may break some backwards compatibility. However, the user can now provide a future plan (including using remote servers) and is generally much more flexible. Additionally, tracing is now handled by the progressr package which requires the user to provide handlers for the type of output they wish (including beeps). A lot of additional functionality has been added, some of it experimental such as the tscalibrate method for empirical distribution calibration via backtesting with support in the predict function for the output of this calibration. In other places we have added extra features such as in the tsdecompose method to return a simplifed state space set of components (Trend, Seasonal, ARMA if available, Irregular). Additionally, we have changed the indexing of the decomposed states so that summing them up will return the exact predictive distribution. The logit transformation is part of the tstransform method, from the tsaux package, which also includes the Box-Cox. Unifying the transformations under this auxiliary function will allow us to expand to potentially other methods going forward. The logit was chosen in order to model certain series which are bounded in the [0,1] domain, and makes an assumption that they follow a Logit-Normal distribution. Date: 2021-11-19 The tsvetsad package now implements automatic differentiation (AD) for the VETS model implemented in the tsvets package. An argument use_autofiff is now available in the estimate method which will then dispatch to the estimate_ad method. Gradients and Hessian are implemented, but for the latter an additional option use_hessian needs to be set to TRUE and it currently defaulted to FALSE in the tsvetsad package. The package is still in beta testing as speed optimization need to be undertaken to perform more competitively to the non-AD method. Date: 2021-06-20 The tsissmad package now implements automatic differentiation (AD) for the ISSM model implemented in the tsissm package. An argument use_autofiff is now available in the estimate method which will then dispatch to the estimate_ad method. Gradients and Hessian are implemented, but for the latter an additional option use_hessian needs to be set to TRUE and it currently defaulted to FALSE in the tsissmad package. Only trigonometric seasonality is implemented but regular seasonality may be included in a future update. Additionally, only the nlminb solver is used for autodiff. Date: 2021-05-17 The tsetad package now implements automatic differentiation (AD) for all ETS models implemented in the tsets package. An argument use_autofiff is now available in the estimate method which will then dispatch to the estimate_ad method. Gradients and Hessian are implemented, and it is suggested that the nlminb solver be used which makes use of both. Speedups for highly parameterized or difficult (powerMAM) models has been observed to be upto 16x when using AD. Addition of AD to the tsissm and tsvets packages may be imlemented in due course. Missing values are now handled during model estimation using the predict step and only calculating the likelihood for non-missing values. Applies to the tests and tsissm packages. For the tsvets package we have not yet decided on the implementation. This may come at a future date. "],["team.html", "Chapter 14 The tsmodels team", " Chapter 14 The tsmodels team The original core team comprised of a group of friends, and at some point ex-colleagues, with an interest in time series forecasting: Alexios Galanos (aut,cre) Keith O’Hara (ctb) James Nesbit (ctb) Professor Diego J. Pedregal (ctb and advisor) Professor Eric Zivot (academic sponsor and advisor) Professor Doug Martin (advisor) Since 2020 the only contributor and maintainer has been the author. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
