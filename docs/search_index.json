[["index.html", "A Time Series Framework Chapter 1 Installation", " A Time Series Framework The tsmodels team 2021-11-21 Chapter 1 Installation Until such time as the packages make it to CRAN, please use the following: devtools::install_github(&quot;tsmodels/tsmethods&quot;) devtools::install_github(&quot;tsmodels/tsdatasets&quot;) devtools::install_github(&quot;tsmodels/tsaux&quot;) devtools::install_github(&quot;tsmodels/tsetsad&quot;) devtools::install_github(&quot;tsmodels/tsets&quot;) devtools::install_github(&quot;tsmodels/tsissmad&quot;) devtools::install_github(&quot;tsmodels/tsissm&quot;) devtools::install_github(&quot;tsmodels/tysvetsad&quot;) devtools::install_github(&quot;tsmodels/tsvets&quot;) devtools::install_github(&quot;tsmodels/tsforeign&quot;) devtools::install_github(&quot;tsmodels/tscausal&quot;) The first 2 are required to be installed before any of the other packages. Also note, that additional dependencies may be required such as Rcpp and RcppArmadillo, and bsts for tsforeign. "],["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction The tsmodels framework provides a set of probabilistic time series forecasting models with common calling conventions and methods. Our objective was to provide a set of models which offer fast estimation, explainable decomposition and probabilistic predictions which can also be ensembled. Many of the models we have so far released are based on R. Hyndman et al. (2008), some of which are already implemented in the forecast package, but we have chosen to re-write the models from scratch based on our own design choices. Some of these choices include exclusive use of xts objects to represent time series data, data.table for certain outputs, probabilistic distributions for predictions via simulation as well as a feature rich set of methods to work with these models. Most of our models benefit from automatic handling of missing data via the prediction step, and all models currently implement automatic differentiation during model estimation, making use of the TMB package. We would be remiss if we were not to mention that alternative implementations exist within the single source of error (SEM) framework in R, including the forecast package of R. Hyndman et al. (2021) and smooth package of Svetunkov (2020), with their own design decisions. We make no representation as to what is likely to be more useful for a particular user, simply that our representation and framework is the right one for us. The set of packages we have released and plan to release may include bugs, and we encourage users to submit bug reports through the github issues system. A summary of the currently available packages is given below: Package Description tsmethods Time Series S3 Methods, plotting functionality and ensembling tsaux Auxilliary functions used by all packages tsdatasets Datasets for benchmarking and examples tsets ETS Models tsissm Linear Innovations State Space Models with Multiple Seasonality tsvets Vector Innovation Linear State Space Models tsforeign Wrapper for other models [bsts (BSTS package), auto.arima (forecast package)] tscausal Time Series Casual Inference The following packages will be released in the future as time allows: Package Description tspyramid Hierarchical Probabilistic Reconcilitation using Tree Stuctures tsssm Linear State Space Models tslifecycle Models for Life Cycle Prediction tsfactor Factor Models See the News section for updates. References "],["tsmethods.html", "Chapter 3 tsmethods package 3.1 Introduction 3.2 Ensembling Forecast Distributions 3.3 Demonstration", " Chapter 3 tsmethods package 3.1 Introduction The tsmethods package provides a set of common methods for use in other packages in our framework. Additionally, it also exports plot methods for objects of class tsmodel.predict, which is the output class from all calls to the predict method, as well as for objects of class tsmodel.distribution, which is a subclass within tsmodel.predict providing the simulated or MCMC based forecast distribution.1 Finally, the package also includes the methods for ensembling distributions. Table 3.1 below provides the currently exported methods for each of the packages which have been released. We have tried to work with existing S3 methods in the stats package where possible including: summary, coef, logLik, AIC, fitted, residuals, predict and simulate. However, we have also created custom methods such as estimate (for model estimation from a specification object), tsdecompose (for time series decomposition of structural time series type models), tsfilter for online filtering,2 tsbacktest (for automatic backtesting) as well as a number of other methods documented in their individual packages. TABLE 3.1: Implemented Methods methods tsets tsissm tsvets tsforeign estimate ✓ ✓ ✓ ✓ summary ✓ ✓ ✓ ✓ coef ✓ ✓ ✓ logLik ✓ ✓ ✓ AIC ✓ ✓ ✓ fitted ✓ ✓ ✓ ✓ residuals ✓ ✓ ✓ ✓ plot ✓ ✓ ✓ ✓ tsmetrics ✓ ✓ ✓ ✓ tsspec ✓ ✓ ✓ tsdiagnose ✓ ✓ ✓ tsdecompose ✓ ✓ ✓ ✓ tsfilter ✓ ✓ ✓ tsprofile ✓ ✓ predict ✓ ✓ ✓ ✓ tsbacktest ✓ ✓ ✓ ✓ simulate ✓ ✓ ✓ tsbenchmark ✓ ✓ ✓ tsreport ✓ tscor ✓ tscov ✓ tsaggregate ✓ tsconvert ✓ tsequation 3.2 Ensembling Forecast Distributions Ensembling in the tsmodels framework proceeds as follows: (Separate) models are estimated for either the same series or a set of different series. The residuals of the estimated series are collected and the residual correlation is calculated. For very large-dimensional systems (where \\(N\\) &gt; \\(T\\)) it is possible instead to use a factor model to extract the correlations, but we leave this for a separate discussion. The estimated correlations are then used to generate correlated samples on the unit hypercube (using for instance a copula). These samples are then passed to the innov argument in the predict method of each model, at which time they are transformed to normal random variables with variance equal to the estimated model variance.3 A key reason for using uniform variates and transforming them back into normal variates within the predict method is the possible presence of the Box Cox transformation, which requires that the variance be on the transformed scale rather than the final series scale. It was therefore decided that this approach was more general and less prone to user error.4 The predictive distribution, which is generated by simulation, will then be infused with the cross-sectional correlation of the residuals passed to the predict method. Note: This approach works well for models with a single source of error such as ARIMA and ETS, but NOT for multiple source of error models such as BSTS. The predictive distributions are then passed to the ensemble_modelspec function for validation, followed by calling the tsensemble method on the resulting object with a vector of user specified weights. The next section provides a demonstration of this approach. 3.3 Demonstration Step 1: Estimation suppressMessages(library(tsmethods)) suppressWarnings(suppressMessages(library(tsets))) suppressMessages(library(xts)) suppressMessages(library(copula)) suppressWarnings(suppressMessages(library(viridis))) data(austretail, package = &quot;tsdatasets&quot;) y1 &lt;- austretail[,&quot;SA.DS&quot;] y2 &lt;- austretail[,&quot;ACF.DS&quot;] spec1 &lt;- ets_modelspec(y1, model = &quot;MMM&quot;, frequency = 12) spec2 &lt;- ets_modelspec(y2, model = &quot;MMM&quot;, frequency = 12) mod1 &lt;- estimate(spec1) mod2 &lt;- estimate(spec2) Step 2: Residual Correlation and Copula Construction/Sampling C &lt;- cor(residuals(mod1), residuals(mod2)) cop &lt;- normalCopula(as.numeric(C), dim = 2, dispstr = &quot;un&quot;) set.seed(100) U &lt;- rCopula(5000 * 12, cop) cor(U) ## [,1] [,2] ## [1,] 1.0000000 0.5958815 ## [2,] 0.5958815 1.0000000 Step 3: Prediction p1.joint &lt;- predict(mod1, h = 12, nsim = 5000, innov = U[,1]) p2.joint &lt;- predict(mod2, h = 12, nsim = 5000, innov = U[,2]) p1.indep &lt;- predict(mod1, h = 12, nsim = 5000) p2.indep &lt;- predict(mod2, h = 12, nsim = 5000) par(mfrow = c(2,2), mar = c(3,3,3,3)) plot(p1.joint, main = &quot;Correlated Predictions[y1]&quot;, n_original = 52) plot(p2.joint, main = &quot;Correlated Predictions[y2]&quot;, n_original = 52) plot(p1.indep, main = &quot;Independent Predictions[y1]&quot;, n_original = 52) plot(p2.indep, main = &quot;Independent Predictions[y2]&quot;, n_original = 52) We can also plot the distributions directly, since there is a plot method for both tsmodel.predict and tsmodel.distribution, and we can also overlay one on top of another by using the add = TRUE argument. plot(p1.joint$distribution, gradient_color = &quot;whitesmoke&quot;, median_col = &quot;grey&quot;, median_width = 4, interval_color = 3, interval_quantiles = c(0.01, 0.99), main = &quot;SA.DS 12 Month Prediction&quot;) plot(p1.indep$distribution, add = TRUE, median_col = 2, median_width = 1, gradient_color = &quot;whitesmoke&quot;, interval_color = 2, interval_quantiles = c(0.01, 0.99), interval_width = 0.5) Evaluation of Predictive Distributions on Forecast Growth Rates p1.growth.joint &lt;- tsgrowth(p1.joint, d = 1, type = &#39;diff&#39;) p2.growth.joint &lt;- tsgrowth(p2.joint, d = 1, type = &#39;diff&#39;) p1.growth.indep &lt;- tsgrowth(p1.indep, d = 1, type = &#39;diff&#39;) p2.growth.indep &lt;- tsgrowth(p2.indep, d = 1, type = &#39;diff&#39;) D.joint &lt;- cor(p1.growth.joint$distribution, p2.growth.joint$distribution) D.indep &lt;- cor(p1.growth.indep$distribution, p2.growth.indep$distribution) print(round(data.frame(Correlated = diag(D.joint), Independent = diag(D.indep)), 3)) ## Correlated Independent ## 2019-01-31 0.608 -0.021 ## 2019-02-28 0.608 -0.005 ## 2019-03-31 0.605 0.018 ## 2019-04-30 0.614 0.013 ## 2019-05-31 0.606 0.004 ## 2019-06-30 0.610 0.002 ## 2019-07-31 0.613 0.004 ## 2019-08-31 0.617 0.002 ## 2019-09-30 0.632 0.000 ## 2019-10-31 0.616 -0.003 ## 2019-11-30 0.620 -0.009 ## 2019-12-31 0.610 -0.017 par(mfrow = c(2,1), mar = c(3,3,3,3)) image(as.matrix(D.joint), col = rev(grey(seq(0, 1, length = 25))), zlim = c(0, 1), main = &quot;Correlated&quot;) contour(cor(p1.joint$distribution, p2.joint$distribution), add = TRUE, drawlabels = TRUE) image(D.indep, col = rev(grey(seq(0, 1, length = 25))), zlim = c(0, 1), main = &quot;Independent&quot;) contour(D.indep, add = TRUE, drawlabels = TRUE) Step 4: Forecast Ensembling par(mfrow = c(1,1), mar = c(3,3,3,3)) spec.joint &lt;- ensemble_modelspec(p1.joint, p2.joint) ensemble.joint &lt;- tsensemble(spec.joint, weights = c(0.5, 0.5)) plot(ensemble.joint, main = &quot;Ensemble Weighted Forecast&quot;, n_original = 52) The current choice of using base R plotting may change in the future to use either ggplot2 or plotly.↩︎ Unlike some, we have avoided using stats::filter as this is a function rather than a method.↩︎ The samples can be transformed to any other distribution using the quantile method of the distribution.↩︎ Additionally the marginal distributions of the individual series may not all be the same, so this approach provides some greater degree of flexibility.↩︎ "],["tsets.html", "Chapter 4 tsets package 4.1 Introduction 4.2 Taxonomy of Models 4.3 Extensions 4.4 Some Encompasing Alternatives 4.5 Package Implementation 4.6 Demonstration", " Chapter 4 tsets package 4.1 Introduction Exponential smoothing was proposed by Robert G. Brown, originally in Brown (1959) and later in Brown (1962), where he developed the general exponential smoothing methodology in the context of inventory management, production planning and control. Independently, Charles C. Holt developed a similar method for exponential smoothing of additive trends and an entirely different method for smoothing seasonal data in Holt (1957). This approach gained popularity following Winters (1960), which tested Holt’s methods with empirical data, from whence the now popular Holt-Winters forecasting system came to prominence. More recently, R. J. Hyndman et al. (2002) and Taylor (2003) formalized the framework and provided a taxonomy of the various models under different assumptions on the type of Error (E), Trend (T) and Seasonality (S) components. In their most basic form, exponential smoothing methods are weighted sums of past observations, with the weights decaying exponentially with older observations. They form a simpler alternative to the more complex structural time series models (see Harvey (1990) and West and Harrison (2006)), by adopting the innovations formulation of the state space representation with all sources of error perfectly correlated.5 The Single Source of Errors model is observationally equivalent to the Multiple Source of Errors model under non-restrictive assumptions, and the interested reader is referred to Casals, Sotoca, and Jerez (1999) for a proof of this. Formally, the general linear innovations state space model can be written as: \\[\\begin{equation} \\begin{array}{l} {y_t} &amp;= {\\bf{w&#39;}}{{\\bf{x}}_{t - 1}} + {\\varepsilon _t} ,\\\\ {{\\bf{x}}_t} &amp;= {\\bf{F}}{{\\bf{x}}_{t - 1}} + {\\bf{g}}{\\varepsilon _t}, \\end{array} \\tag{4.1} \\end{equation}\\] where \\(y_t\\) is the observed value at time \\(t\\), \\(\\mathbf{x}_t\\) the vector of state variables (which may include information about the level, slope, seasonal patterns and exogenous regressors), \\(\\mathbf{w}\\) is the observation matrix and \\(\\mathbf{F}\\) the state transition matrix. An innovations state space model can be reduced to an equivalent ARIMA model with the help of the lag operator \\(L\\). The state equation can be rewritten as:6 \\[\\begin{equation} \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){\\mathbf{x}_t} = {\\bf{g}}{\\varepsilon _t}. \\tag{4.2} \\end{equation}\\] Since \\({\\bf{I}} - {\\bf{F}}L\\) may not have an inverse, both sides are multiplied by its adjugate7 \\(adj{\\left( {{\\bf{I}} - {\\bf{F}}L} \\right)}\\) to obtain: \\[\\begin{equation} \\det\\left( {{\\bf{I}} - {\\bf{F}}L} \\right){\\mathbf{x}_t} = adj{\\left( {{\\bf{I}} - {\\bf{F}}L} \\right)}{\\bf{g}}{\\varepsilon _t}. \\tag{4.3} \\end{equation}\\] Applying the \\(\\det\\left( {{\\bf{I}} - {\\bf{F}}L} \\right)\\) to the observation equation: \\[\\begin{equation} \\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){y_t} = {\\bf{w&#39;}}\\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){{\\mathbf{x}}_{t - 1}} + \\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){\\varepsilon _t}, \\tag{4.4} \\end{equation}\\] and finally replacing \\(\\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){{\\mathbf{x}}_{t - 1}}\\) with the state equation formula we obtain: \\[\\begin{equation} \\det \\left({\\bf{I}} - {\\bf{F}}L \\right){y_t} = {\\bf{w&#39;}}\\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){\\bf{g}}{\\varepsilon _{t - 1}} + \\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){\\varepsilon _t}. \\tag{4.5} \\end{equation}\\] Defining \\(\\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right)\\) as \\(\\eta\\left(L\\right)\\) and \\({\\bf{w&#39;}}\\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right){\\bf{g}}{L} + \\det \\left( {{\\bf{I}} - {\\bf{F}}L} \\right)\\) as \\(\\theta\\left(L\\right)\\) we obtain the typical ARIMA representation8 \\(\\eta \\left( L \\right){y_t} = \\theta \\left( L \\right){\\varepsilon _t}\\), where \\(\\eta\\left(L\\right)\\) and \\(\\theta\\left(L\\right)\\) are polynomials in the lag operator \\(L\\) and may include powers of \\(L\\) related to the seasonal period \\(m\\). To obtain the ARIMA representation, set \\(\\eta \\left( L \\right){y_t} = \\phi \\left( L \\right)\\delta \\left( L \\right){y_t}\\), where \\(\\delta\\left(L\\right)\\) contains all unit roots of the polynomial. Typical examples include the damped local trend model,9 which can be represented by an ARIMA(1,1,2) model and the local linear trend model, which can be represented as an ARIMA(0,2,2) model. On the other hand, an ARIMA(2,0,2) model with complex roots, which gives rise to cyclical behavior, cannot be represented by an exponential smoothing model. 4.2 Taxonomy of Models Table 4.1 presents the taxonomy proposed by Pegels (1969) and Gardner Jr (1985) for exponential smoothing models. In addition, for each of the 12 model combinations presented, it is possible to have either additive or multiplicative errors, giving rise to the ETS formulation of Error, Trend and Seasonal. For instance, the MAM model corresponds to Multiplicative Error, Additive Trend and Multiplicative Seasonality. TABLE 4.1: Exponential Smoothing Model Taxonomy N A M (none) (additive) (multiplicative) N (none) NN NA NM A (additive) AN AA AM M (multiplicative) MN MA MM D (damped) DN DA DM Following Ord, Koehler, and Snyder (1997) and R. J. Hyndman et al. (2002), we present a state space formulation the ETS modelling system. \\[\\begin{equation} \\begin{array}{l} {y_t} = h\\left( {{\\mathbf{x}_{t - 1}}} \\right) + k\\left( {{\\mathbf{x}_{t - 1}}} \\right){\\varepsilon _t},\\\\ {\\mathbf{x}_t} = f\\left( {{\\mathbf{x}_{t - 1}}} \\right) + g\\left( {{\\mathbf{x}_{t - 1}}} \\right){\\varepsilon _t}, \\end{array} \\tag{4.6} \\end{equation}\\] where \\({\\varepsilon _t} \\sim N\\left( {0,{\\sigma ^2}} \\right)\\) and \\({\\mathbf{x}_t} = \\left\\{ {{l_t},{b_t},{s_{t - 1}},...,{s_{t - \\left( {m - 1} \\right)}}} \\right\\}\\). Defining \\({e_t} = k\\left( {{x_{t - 1}}} \\right){\\varepsilon _t}\\) and \\({\\mu _t} = h\\left( {{x_{t - 1}}} \\right)\\), then \\({y_t} = {\\mu _t} + {e_t}\\). When errors are additive, then \\(y_t=\\mu_t + \\varepsilon_t\\) and \\(k\\left( {{x_{t - 1}}} \\right)=1\\), whilst when errors are multiplicative, then \\(y_t=\\mu_t\\left(1+\\varepsilon_t\\right)\\) and therefore \\(k\\left( {{x_{t - 1}}} \\right)=\\mu_t\\) so that \\(\\varepsilon_t=\\frac{\\left(y_t-\\mu_t\\right)}{\\mu_t}\\) represents a relative error. For illustration, we show below the additive error with additive trend and seasonality: \\[\\begin{equation} \\begin{array}{l} {{\\hat y}_t} = {l_{t - 1}} + \\phi {b_{t - 1}} + {s_{t - m}},\\\\ {\\varepsilon _t} = {y_t} - {{\\hat y}_t},\\\\ {l_t} = {l_{t - 1}} + \\phi {b_{t - 1}} + \\alpha {\\varepsilon _t},\\\\ {b_t} = \\phi {b_{t - 1}} + \\beta {\\varepsilon _t},\\\\ {s_t} = {s_{t - m}} + \\gamma {\\varepsilon _t}, \\end{array} \\tag{4.7} \\end{equation}\\] with \\(\\phi\\) representing the damping parameter which is equal to one when there is no damping. The \\(h\\)-step ahead forecast is then given as: \\[\\begin{equation} {y_{t + h}} = \\left\\{ {\\begin{array}{*{20}{c}} {{l_t} + h{b_t} + {s_{t - m + h_m^ + }}},&amp;{\\phi \\in \\left\\{ \\emptyset \\right\\}},\\\\ {{l_t} + {\\phi^h}{b_t} + {s_{t - m + h_m^ + }}},&amp;{\\phi \\in [0,1]}, \\end{array}} \\right. \\tag{4.8} \\end{equation}\\] where \\(h_m^ + = \\left[ {\\left( {h - 1} \\right)\\bmod m} \\right] + 1\\). 4.3 Extensions A number of extensions have been suggested and too numerous to outline here. One interesting model proposed by Koehler, Snyder, and Ord (2001) is the decomposition of the MAM model to include power terms as follows: \\[\\begin{equation} \\begin{array}{l} {y_t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right){s_{t - m}} + {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^\\theta }{s^\\delta }_{t - m}{\\varepsilon _t},\\\\ {l_t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right) + \\alpha {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^\\theta }{s_{t - m}^{\\delta - 1}}{\\varepsilon _t},\\\\ {b_t} = \\phi {b_{t - 1}} + \\beta {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^\\theta }{s_{t - m}^{\\delta - 1}}{\\varepsilon _t},\\\\ {s_t} = {s_{t - m}} + \\gamma {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^{\\theta - 1}}{s_{t - m}^\\delta}{\\varepsilon _t}. \\end{array} \\tag{4.9} \\end{equation}\\] When \\(\\theta=1\\) and \\(\\delta=1\\), this reduces to the standard MAM model. When \\(\\theta=0\\) and \\(\\delta=0\\) this is reduces to the AAM model or the AAN model if there is no seasonal term. The exponents can be thought as controlling the degree of heteroscedasticity in the data, since the unscaled residuals \\(\\epsilon_t\\) are distributed as: \\[\\begin{equation} {\\epsilon_t} = {y_t} - \\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right){s_{t - m}},\\quad {\\epsilon_t} | \\mathcal{F}_{t-1} \\sim N\\left( {0,{{\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)}^{2\\theta} }{s_{t-m}^{2\\delta}} \\sigma^2 } \\right), \\tag{4.10} \\end{equation}\\] which is an appealing alternative to Box Cox and related transformations. For instance, as R. Hyndman et al. (2008) 4.4.5 notes, a value of \\(\\theta=1/3\\) would produce a variance proportional to the \\(2/3\\) power of the mean, similar to the cube root transformation. Normalized seasonality, discussed in Roberts (1982) and McKenzie (1986), can be used to de-seasonalize the data, by acting as a filter, and is required for implementing the Wiener-Kolmogorov (WK) filter (smoother). This is discussed in more detail in Chapter 8 of R. Hyndman et al. (2008), and is implemented as an option in the tsets package. Another avenue of interest is in the multivariate generalization of the model presented in De Silva, Hyndman, and Snyder (2010). It has the ability to incorporate common levels, trends or seasonality and is implemented in the tsvets package. 4.4 Some Encompasing Alternatives The general state space representation (see Harvey (1990)), based on the multiple sources of error (MSOE) state space model, provides a more general implementation of the unobserved components model, albeit requiring the use of the Kalman filter for estimation. The model is flexible enough to incorporate many types of additive models, including cyclical behavior and regressors, although the nonlinear (multiplicative) variations require the extended Kalman filter for estimation. The bsts package of Steven L. Scott and Varian (2015) provides fast and efficient computation of Bayesian Unobserved Components, with the option of a spike and slab prior for regressor regularization, for which we provide a wrapper in the tsforeign package. 4.5 Package Implementation The tsets package implements 4 families of models, whose equations are given in Tables 4.2 and 4.3. These are the full equations assuming all variables (trend, damped, seasonal and regressors) enter the model, but any and all combinations of the variables are allowed. Methods implemented include Quasi-ML estimation, prediction, simulation, plotting and post-estimation diagnostics. TABLE 4.2: AAA and MMM Model Equations Equation AAA MMM Observation \\({y_t} = {l_{t - 1}} + \\phi{b_{t - 1}} + {s_{t - m}} + \\bf{x}_{t - 1}\\bf{w} + \\varepsilon_t\\) \\({y_t} = {l_{t - 1}}{b_{t-1}^\\phi}\\left(1.0 + \\bf{x}_{t-1}\\bf{w}\\right){s_{t - m}}\\left(1 + \\varepsilon _t\\right)\\) Mean \\({\\mu _t} = {l_{t - 1}} + \\phi{b_{t - 1}} + {s_{t - m}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}\\) \\({\\mu_t} = {l_{t - 1}}{b^\\phi_{t - 1}}\\left( {{{1.0 + \\bf{x}}_{t-1}}{\\bf{w}}} \\right){s_{t - m}}\\) Distribution \\({\\hat y_t}\\left| {{\\Im _{t - 1}} \\sim N\\left( {{\\mu _t},\\sigma } \\right)} \\right.\\) \\({{\\hat y}_t}\\left| {{\\Im _{t - 1}} \\sim N\\left( {{\\mu _t},{l_{t - 1}}{b_{t-1}^\\phi}\\left( {{{\\bf{x}}_{t-1}}{\\bf{w}}} \\right){s_{t - m}}\\sigma } \\right)} \\right.\\) Error \\({\\varepsilon _t} = {y_t} - {\\mu _t}\\) \\({\\varepsilon _t} = \\frac{{{y_t} - {\\mu _t}}}{{{l_{t - 1}}{b}_{t - 1}^\\phi \\left( {{{1.0 + \\bf{x}}_{t-1}}{\\bf{w}}} \\right){s_{t - m}}}}\\) Level[State] \\({l_t} = {l_{t - 1}} + \\phi {b_{t - 1}} + \\alpha {\\varepsilon _t}\\) \\({l_t} = {l_{t - 1}}{b_{t-1}^\\phi}\\left( {1 + \\alpha {\\varepsilon _t}} \\right)\\) Trend[State] \\({b_t} = \\phi {b_{t - 1}} + \\beta {\\varepsilon _t}\\) \\({b_t} = {b_{t-1}^\\phi}\\left( {1 + \\beta {\\varepsilon _t}} \\right)\\) Seasonal[State] \\({s_t} = {s_{t - m}} + \\gamma {\\varepsilon _t}\\) \\({s_t} = {s_{t - m}}\\left( {1 + \\gamma {\\varepsilon _t}} \\right)\\) TABLE 4.3: MAM and powerMAM Model Equations Equation MAM powerMAM Observation \\({y_t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right){s_{t - m}}\\left( {1 + {\\varepsilon _t}} \\right)\\) \\({y_t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right){s_{t - m}} + {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right)^\\theta }s_{t - m}^\\delta {\\varepsilon _t}\\) Mean \\({\\mu _t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right){s_{t - m}}\\) \\({\\mu _t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right){s_{t - m}}\\) Distribution \\({\\hat y_t}\\left| {{\\Im _{t - 1}} \\sim N\\left( {{\\mu _t},\\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right){s_{t - m}}\\sigma } \\right)} \\right.\\) \\({\\hat y_t}\\left| {{\\Im _{t - 1}} \\sim N\\left( {{\\mu _t},{{\\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right)}^\\theta }s_{t - m}^\\delta \\sigma } \\right)} \\right.\\) Error \\({\\varepsilon _t} = \\frac{{{y_t} - {\\mu _t}}}{{\\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right){s_{t - m}}}}\\) \\({\\varepsilon _t} = \\frac{{{y_t} - {\\mu _t}}}{{{{\\left( {{l_{t - 1}} + \\phi {b_{t - 1}} + {{\\bf{x}}_{t - 1}}{\\bf{w}}} \\right)}^\\theta }s_{t - m}^\\delta }}\\) Level[State] \\({l_t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)\\left( {1 + \\alpha {\\varepsilon _t}} \\right)\\) \\({l_t} = \\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right) + \\alpha {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^\\theta }s_{t - m}^{\\delta - 1}{\\varepsilon _t}\\) Trend[State] \\({b_t} = \\phi {b_{t - 1}} + \\beta \\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right){\\varepsilon _t}\\) \\({b_t} = \\phi {b_{t - 1}} + \\beta {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^\\theta }s_{t - m}^{\\delta - 1}{\\varepsilon _t}\\) Seasonal[State] \\({s_t} = {s_{t - m}}\\left( {1 + \\gamma {\\varepsilon _t}} \\right)\\) \\({s_t} = {s_{t - m}} + \\gamma {\\left( {{l_{t - 1}} + \\phi {b_{t - 1}}} \\right)^{\\theta - 1}}s_{t - m}^\\delta {\\varepsilon _t}\\) 4.5.1 Constraints Variables in the additive ETS type models need to be constrained in order to achieve stability and forecastability, and the interested reader should consult Section 10.2 of R. J. Hyndman, Akram, and Archibald (2008) for more details. These conditions become increasingly complex depending on the components included in the model. In our implementation we have opted for the following simple conditions: \\(\\alpha \\in \\left[ {0,1} \\right]\\), \\(\\beta \\in \\left[ {0,\\alpha } \\right]\\), \\(\\gamma \\in \\left[ {0,1 - \\alpha } \\right]\\), \\(\\phi \\in \\left[ {0.5,1} \\right]\\), \\(\\theta \\in \\left[ {0,1} \\right]\\), \\(\\delta \\in \\left[ {0,1} \\right]\\), \\(\\sigma \\in {{\\bf{R}}_+ }\\). For multiplicative models, we simply impose that \\(\\max\\left(\\alpha,\\beta,\\gamma\\right)&lt;1\\) and \\(\\varepsilon_t&gt;-1\\), although we do not impose the last condition for estimation (only for simulation). 4.5.2 Initialization To obtain a reasonable set of parameters for the initialization conditions of the states as well as the parameters, we obtain values for \\(l_0\\), \\(b_0\\) and \\(s_0\\) using the heuristic approach described in Section 5.2 of R. Hyndman et al. (2008). There is also an option for estimating the initial states for the seasonal component. 4.5.3 Transformations Variance stabilizing transformations form an important part of the pre-processing of the outcome variable in order to achieve certain statistical properties which help reduce misspecification of the model. At present, we implement the Box-Cox transformation with an option for automatic tuning of the parameter \\(\\lambda\\) using the method of Guerrero (1993), from the tsaux package. 4.6 Demonstration 4.6.1 The Specification Object The entry specification function is called ets_modelspec suppressWarnings(suppressMessages(library(tsets))) args(ets_modelspec) ## function (y, model = &quot;AAN&quot;, damped = FALSE, power = FALSE, xreg = NULL, ## frequency = NULL, lambda = NULL, normalized_seasonality = TRUE, ## fixed_pars = NULL, scale = FALSE, seasonal_init = &quot;fixed&quot;, ## lambda_lower = 0, lambda_upper = 1, sampling = NULL, xreg_init = TRUE, ## ...) ## NULL This requires passing in an xts vector y, followed by a number of options described below: model: The ETS model type. damped: Whether to dampen the trend. power: The power MAM model (only applicable to the MAM). xreg: An xts matrix of regressors. frequency: The seasonal frequency of y, only needed if using a seasonal model. lambda: The Box Box transformation parameter. If NA, will estimate it. normalized_seasonality: Whether to impose the normalized approach of McKenzie (1986). fixed_pars: An optional named vector of fixed parameters. scale: Whether to pre-scale the data prior to estimation (will rescale back after estimation). seasonal_init: Whether to “estimate” or use the heuristic (“fixed”) values for the initial states. sampling: An optional string denoting the sampling frequency of the data (will try to discover it if NULL). 4.6.2 Estimation For illustration, we use the gas dataset from the tsdatasets package, representing weekly US finished motor gasoline products supplied (in thousands of barrels per day) from February 1991 to May 2005. data(gas, package = &quot;tsdatasets&quot;) spec &lt;- ets_modelspec(gas[1:(NROW(gas) - 52)], model = &quot;AAA&quot;, frequency = 52, lambda = NA) str(spec, max.level = 1) ## List of 5 ## $ target :List of 6 ## $ model :List of 14 ## $ seasonal :List of 1 ## $ transform:List of 3 ## $ xreg :List of 3 ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;tsets.spec&quot; &quot;tsmodel.spec&quot; The specification object returns a list which inherits the tsmodel.spec class and has a number of slots. Common across the tsmodels framework will be the target slot, which has the target variable with details on its sampling frequency among others, the transform slot, which contains the Box Cox transformation and its inverse, and the xreg slot, which contains any optionally including external regressors. mod &lt;- estimate(spec) # automatic selection # mod_auto &lt;- auto_ets(gas[1:(NROW(gas) - 52)], frequency = 52, lambda = NA, # metric = &quot;MSLRE&quot;, cores = 4) summary(mod) ## ## ETS Model [ AAA ] ## ## Parameter Description Est[Value] ## ---------- --------------------- ----------- ## alpha State[Level-coef] 0.0348 ## beta State[Slope-coef] 0.0001 ## gamma State[Seasonal-coef] 0.0000 ## l0 State[Level-init] 7108.5604 ## b0 State[Slope-init] 2.7619 ## ## ## LogLik AIC BIC AICc ## --------- --------- -------- --------- ## -6123.43 12360.86 12619.7 12371.27 ## ## ## MAPE MASE MSLRE BIAS ## ------ ------- ------- ------ ## 0.026 0.6328 0.0011 9e-04 A number of methods exist for post-estimation inference which we illustrate below. logLik(mod) ## &#39;log Lik.&#39; -6123.43 (df=6) AIC(mod) ## [1] 12360.86 coef(mod) ## alpha beta gamma l0 b0 ## 0.03476941 0.00010000 0.00000000 7108.56039379 2.76189969 ## s0 s1 s2 s3 s4 ## -511.18996952 -456.41526245 -461.98649919 -139.91350053 -54.69925501 ## s5 s6 s7 s8 s9 ## 130.45382240 30.67198353 44.64905141 -183.07147849 -118.14143955 ## s10 s11 s12 s13 s14 ## -36.61803477 106.86088082 64.97841126 60.58207265 133.10219815 ## s15 s16 s17 s18 s19 ## 59.11091891 -32.12207765 -112.14202044 31.88357988 78.64699634 ## s20 s21 s22 s23 s24 ## 166.54294243 315.25674137 247.26950318 357.09149148 240.91993671 ## s25 s26 s27 s28 s29 ## 403.59736605 216.67098944 346.01213061 229.17309895 342.80928619 ## s30 s31 s32 s33 s34 ## 354.71663251 245.57277925 80.87671647 80.08645793 70.46072950 ## s35 s36 s37 s38 s39 ## 165.45430465 70.35842296 -50.38183625 -130.95815688 111.89388805 ## s40 s41 s42 s43 s44 ## -47.23340484 -120.85562760 -187.81733374 -108.53261498 -107.25369102 ## s45 s46 s47 s48 s49 ## 4.20810004 -259.16694666 -266.68011313 -211.33469029 -269.16731225 ## s50 sigma ## -497.75901659 261.46393207 head(residuals(mod, raw = TRUE)) ## residuals ## 1991-02-01 -68.26345 ## 1991-02-08 -185.24805 ## 1991-02-15 -261.22180 ## 1991-02-22 328.94264 ## 1991-03-01 21.31197 ## 1991-03-08 82.27028 head(residuals(mod, raw = FALSE)) ## residuals ## 1991-02-01 -68.30319 ## 1991-02-08 -185.35564 ## 1991-02-15 -261.37401 ## 1991-02-22 329.13541 ## 1991-03-01 21.32442 ## 1991-03-08 82.31837 head(fitted(mod)) ## fitted ## 1991-02-01 6689.303 ## 1991-02-08 6618.356 ## 1991-02-15 6843.374 ## 1991-02-22 6894.865 ## 1991-03-01 6853.676 ## 1991-03-08 6864.682 plot(mod) tsdiagnose(mod) ## ## Weighted Ljung-Box Test [scaled residuals] ## ------------------------------------------ ## Lag statistic pvalue ## [1] 9.01 0.00268 ## [2] 9.39 0.00276 ## [3] 10.77 0.00197 ## [4] 11.48 0.00221 ## ## Parameter Bounds and Conditions ## ------------------------------------------ ## coef value &gt;lb &lt;ub condition condition_pass ## alpha 0.0348 TRUE TRUE NA NA ## beta 0.0001 TRUE TRUE &lt; alpha TRUE ## gamma 0.0000 TRUE TRUE &lt; (1 - alpha) TRUE ## phi NA TRUE TRUE NA NA ## theta NA TRUE TRUE NA NA ## delta NA TRUE TRUE NA NA ## ## Outlier Diagnostics (based on Rosner Test) ## ------------------------------------------ ## Outliers: 1998-03-27 tsmetrics(mod) ## n no.pars LogLik AIC BIC AICc MAPE MASE ## 1 693 56 -6123.43 12360.86 12619.7 12371.27 0.02604335 0.6328421 ## MSLRE BIAS ## 1 0.00108786 0.0008550908 tsd_mod &lt;- tsdecompose(mod) tsd_mod &lt;- do.call(cbind, lapply(1:length(tsd_mod), function(i) tsd_mod[[i]])) head(tsd_mod) ## fitted Residuals Level Slope Seasonal ## 1991-02-01 6689.303 -68.26345 7108.949 2.755073 -497.7590 ## 1991-02-08 6618.356 -185.24805 7105.263 2.736549 -269.1673 ## 1991-02-15 6843.374 -261.22180 7098.917 2.710426 -211.3347 ## 1991-02-22 6894.865 328.94264 7113.065 2.743321 -266.6801 ## 1991-03-01 6853.676 21.31197 7116.549 2.745452 -259.1669 ## 1991-03-08 6864.682 82.27028 7122.155 2.753679 4.2081 4.6.3 Prediction All prediction objects in the tsmodels framework are of class tsmodel.predict, with slots for the original series and the forecast distribution (the latter being of class tsmodel.distribution). Some prediction objects will contain additional slots, usually the original specification object as well as the state component predicted decomposition (also of class tsmodel.distribution). p &lt;- predict(mod, h = 52, nsim = 5000) class(p) ## [1] &quot;tsets.predict&quot; &quot;tsmodel.predict&quot; str(p, max.level = 1) ## List of 6 ## $ distribution : &#39;tsets.distribution&#39; num [1:5000, 1:52] 9357 8700 8935 9116 9279 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## ..- attr(*, &quot;date_class&quot;)= chr &quot;Date&quot; ## $ original_series:&#39;zoo&#39; series from 1991-02-01 to 2004-05-07 ## Data: num [1:693] 6621 6433 6582 7224 6875 ... ## Index: Date[1:693], format: &quot;1991-02-01&quot; &quot;1991-02-08&quot; ... ## $ h : num 52 ## $ spec :List of 5 ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;tsets.spec&quot; &quot;tsmodel.spec&quot; ## $ decomposition :List of 6 ## $ mean :&#39;zoo&#39; series from 2004-05-14 to 2005-05-06 ## Data: Named num [1:52] 9143 9156 9161 9323 9435 ... ## ..- attr(*, &quot;names&quot;)= chr [1:52] &quot;2004-05-14&quot; &quot;2004-05-21&quot; &quot;2004-05-28&quot; &quot;2004-06-04&quot; ... ## Index: Date[1:52], format: &quot;2004-05-14&quot; &quot;2004-05-21&quot; ... ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;tsets.predict&quot; &quot;tsmodel.predict&quot; plot(p, n_original = 52*4) p_decomp &lt;- tsdecompose(p) par(mfrow = c(2,2), mar = c(3,3,3,3)) plot(p_decomp$Level, main = &quot;Level[State] Predicted Distribution&quot;) plot(p_decomp$Slope, main = &quot;Slope[State] Predicted Distribution&quot;) plot(p_decomp$Seasonal, main = &quot;Seasonal[State] Predicted Distribution&quot;) plot(p_decomp$Error, main = &quot;Simulated Error Distribution&quot;) Since we left 52 points for out-of-sample testing, we are able to evaluate the prediction using the tsmetrics method on a predicted object. This method also takes the original series as an input in order to calculate MASE in the presence of seasonality. The alpha parameter is the coverage rate for calculation of the Mean Interval Score of Gneiting and Raftery (2007). tsmetrics(p, tail(gas, 52), original_series = spec$target$y_orig, alpha = 0.05) ## h MAPE MASE MSLRE BIAS MIS CRPS ## 1 52 0.01447461 0.4016107 0.0003889429 0.003547331 1074.333 104.6745 tsd_predict &lt;- tsdecompose(p) par(mfrow = c(2,2), mar = c(3,3,3,3)) plot(tsd_predict$forecast, main = &quot;Prediction&quot;) plot(tsd_predict$Level, main = &quot;Level[State]&quot;) plot(tsd_predict$Slope, main = &quot;Slope[State]&quot;) plot(tsd_predict$Seasonal, main = &quot;Seasonal[State]&quot;) 4.6.4 Filtering Online filtering is when new data arrives and instead of re-estimating the model, we instead just filter the new data based on an existing model. In the tsets package the tsfilter method updates an object of class tsmodel.estimate with new data as the example below illustrates. Because the class of the model is retained and only updated (both data and states) with new information, it is also possible to apply any method to that which admits that object (e.g. predict). mod_filter &lt;- tsfilter(mod, y = gas[(NROW(gas) - 52 + 1)]) tail(fitted(mod),1) ## fitted ## 2004-05-07 9239.359 tail(fitted(mod_filter),2) ## fitted ## 2004-05-07 9239.359 ## 2004-05-14 9143.025 mod_filter &lt;- tsfilter(mod_filter, y = gas[(NROW(gas) - 52 + 2)]) tail(fitted(mod),1) ## fitted ## 2004-05-07 9239.359 tail(fitted(mod_filter),3) ## fitted ## 2004-05-07 9239.359 ## 2004-05-14 9143.025 ## 2004-05-21 9155.940 head(predict(mod, h = 12)$mean) ## 2004-05-14 2004-05-21 2004-05-28 2004-06-04 2004-06-11 2004-06-18 ## 9158.698 9170.661 9153.781 9329.027 9439.320 9448.865 head(predict(mod_filter, h = 12)$mean) ## 2004-05-28 2004-06-04 2004-06-11 2004-06-18 2004-06-25 2004-07-02 ## 9156.777 9321.691 9441.089 9417.545 9320.598 9447.503 4.6.5 Simulation An estimated object can also be simulated from, with the parameters and initial states overridden by passing them as named values in the pars argument. The default is to initialize the states from the seed states used in the estimated object, with h equal to the length of the original series (default for NULL h). Innovations for the simulation can either be parametric (normal for additive or truncated normal for multiplicative error models), based on the estimated residuals (bootstrap argument) or a user supplied set of uniform random numbers (which are then translated into normal or truncated normal using standard deviation equal to the model sigma and optionally scaled by sigma_scale). sim &lt;- simulate(mod, nsim = 10, h = 52*2) plot(sim) par(mar = c(3,3,3,3), mfrow = c(4, 1)) matplot(as.Date(colnames(sim$Simulated)), t(sim$Simulated[1:10, ]), type = &quot;l&quot;, ylab = &quot;&quot;, main = &quot;Simulated Paths&quot;, xlab = &quot;&quot;) grid() matplot(as.Date(colnames(sim$Simulated)), t(sim$Level[1:10, ]), type = &quot;l&quot;, ylab = &quot;&quot;, main = &quot;Simulated Level&quot;, xlab = &quot;&quot;) grid() matplot(as.Date(colnames(sim$Simulated)), t(sim$Slope[1:10, ]), type = &quot;l&quot;, ylab = &quot;&quot;, main = &quot;Simulated Slope&quot;, xlab = &quot;&quot;) grid() # not much on the seasonal since gamma coefficient is zero matplot(as.Date(colnames(sim$Simulated)), t(sim$Seasonal[1:10, ]), type = &quot;l&quot;, ylab = &quot;&quot;, main = &quot;Simulated Seasonal&quot;, xlab = &quot;&quot;) grid() 4.6.6 Profiling The tsprofile function profiles an estimated model by simulating and then estimating multiple paths from the assumed DGP while leaving h values out for prediction evaluation. Each simulated path is equal to the size of the original dataset plus h additional values, and initialized with the initial state vector from the model. The resulting output contains the distribution of the MAPE, percent bias (BIAS) and mean squared log relative error (MSLRE) per horizon h. Since these matrices are of class tsmodel.distribution, they can be readily plotted with the special purpose plot function for this class from the tsmethods package. Additionally, a data.table matrix is also return with the distribution of the coefficients from each path estimation. # profiling prof &lt;- tsprofile(mod, h = 52, nsim = 1000, cores = 4, trace = 0) plot(prof, type = &quot;metrics&quot;) plot(prof, type = &quot;coef&quot;) 4.6.7 Backtesting The tsbacktest method generates an expanding window, walk forward backtest, returning a list with the estimation/horizon predictions against actuals, as well as a table of average performance metrics by horizon. b &lt;- tsbacktest(spec, h = 52, alpha = c(0.02, 0.1), trace = 0, cores = 4) head(b$metrics) ## horizon variable MAPE MSLRE BIAS n MIS[0.02] MIS[0.1] ## 1: 1 y 0.02704803 0.001167260 -1.268449e-04 347 1697.782 1189.753 ## 2: 2 y 0.02673161 0.001145531 -9.142468e-05 346 1689.133 1173.465 ## 3: 3 y 0.02686768 0.001147393 -2.440922e-04 345 1683.990 1179.160 ## 4: 4 y 0.02700485 0.001165606 -1.765628e-04 344 1721.288 1184.287 ## 5: 5 y 0.02699399 0.001158325 -2.007896e-04 343 1691.327 1173.748 ## 6: 6 y 0.02677531 0.001147042 -4.639481e-04 342 1664.977 1189.225 plot(b$metrics$horizon, b$metrics$MAPE*100, main = &quot;Horizon MAPE&quot;, type = &quot;l&quot;, ylab = &quot;MAPE[%]&quot;, xlab = &quot;horizon&quot;) grid() 4.6.8 Benchmarking The tsbenchmark method is used to benchmark a model for timing and accuracy and can be used as a unit testing function. bench &lt;- rbind(tsbenchmark(spec, solver = &quot;optim&quot;), tsbenchmark(spec, solver = &quot;nlminb&quot;), tsbenchmark(spec, solver = &quot;solnp&quot;)) bench ## start end spec estimate ## 1: 2021-11-21 07:03:40 2021-11-21 07:03:40 &lt;tsets.spec[5]&gt; &lt;tsets.estimate[4]&gt; ## 2: 2021-11-21 07:03:40 2021-11-21 07:03:40 &lt;tsets.spec[5]&gt; &lt;tsets.estimate[4]&gt; ## 3: 2021-11-21 07:03:40 2021-11-21 07:03:40 &lt;tsets.spec[5]&gt; &lt;tsets.estimate[4]&gt; ## solver control loglik ## 1: optim 0 -6125.443 ## 2: nlminb 0 -6123.430 ## 3: solnp 0 -6123.430 References "],["tsissm.html", "Chapter 5 tsissm package 5.1 Introduction 5.2 State Initialization 5.3 Log-Likelihood 5.4 Package Implementation 5.5 Demonstration", " Chapter 5 tsissm package 5.1 Introduction The tsissm package implements the linear (and homoscedastic) innovations state space model described in De Livera, Hyndman, and Snyder (2011) and originally proposed by Anderson and Moore (2012). It is in some ways similar to the ETS based models described in R. Hyndman et al. (2008), but with the flexibility of incorporating multiple seasonality, ARMA terms and joint maximization of the Box Cox variance stabilizing lambda parameter. The following is taken from De Livera, Hyndman, and Snyder (2011). Consider the following SEM model: \\[\\begin{equation} \\begin{array}{l} y_t^\\lambda = {\\bf{w&#39;}}{{\\bf{x}}_{t - 1}} + \\bf{c&#39;}\\bf{u}_{t - 1} + {\\varepsilon _t}, \\quad \\varepsilon_t\\sim N\\left(0,\\sigma^2\\right),\\\\ {{\\bf{x}}_t} = {\\bf{F}}{{\\bf{x}}_{t - 1}} + {\\bf{g}}{\\varepsilon _t}, \\end{array} \\tag{5.1} \\end{equation}\\] where \\(\\lambda\\) represents the Box Cox parameter, \\(\\bf{w}\\) the observation coefficient vector, \\(\\bf{x}_t\\) the unobserved state vector, \\(\\bf{c}\\) a vector of coefficients on the external regressor set \\(\\bf{u}\\). Define the state vector10 as: \\[\\begin{equation} \\bf{x}_t = {\\left( {{l_t},{b_t},s_t^{(1)},\\dots,s_t^{(T)},{d_t},{d_{t - 1}},\\dots,{d_{t - p - 1}},{\\varepsilon_t},{\\varepsilon_{t - 1}},\\dots,{\\varepsilon _{t - q - 1}}} \\right)^\\prime }, \\tag{5.2} \\end{equation}\\] where \\(\\bf{s}_t^{(i)}\\) is the row vector \\(\\left( {s_{1,t}^{(i)},s_{2,t}^{(i)},\\dots,s_{{k_i},t}^{(i)},s_{1,t}^{*(i)},s_{2,t}^{*(i)},\\dots,s_{{k_i},t}^{*(i)}} \\right)\\) in the case of trigonometric seasonality, and \\(\\left( {s_t^{(i)},s_{t - 1}^{(i)},\\dots,s_{t - ({m_i} - 1)}^{(i)}} \\right)\\) in the case of regular seasonality. Also define \\(\\bf{1}_r\\) and \\(\\bf{0}_r\\) as a vector of ones and zeros, respectively, of length \\(r\\), \\(\\bf{O}_{u,v}\\) a \\(u\\times v\\) matrix of zeros and \\(\\bf{I}_{u,v}\\) a \\(u\\times v\\) diagonal matrix of ones; let \\(\\mathbf{\\gamma} = \\left(\\mathbf{\\gamma}^{(1)},\\dots,\\mathbf{\\gamma}^{(T)} \\right)\\) be a vector of seasonal parameters with \\({\\gamma ^{(i)}} = \\left( {\\gamma _1^{(i)}{{\\bf{1}}_{{k_i}}},\\gamma _2^{(i)}{{\\bf{1}}_{{k_i}}}} \\right)\\) in the trigonometric seasonality case (with \\(k\\) harmonics), and \\({\\gamma ^{(i)}} = \\left( {{\\gamma _i},{{\\bf{0}}_{{m_i} - 1}}} \\right)\\) in the regular seasonality case; define \\(\\theta = \\left( {{\\theta_1},{\\theta_2},\\dots,{\\theta _p}} \\right)\\) and \\(\\psi = \\left( {{\\psi _1},{\\psi _2},\\dots,{\\psi_q}} \\right)\\) as the vector of AR(p) and MA(q) parameters, respectively. Define the observation transition vector \\({\\bf{w}} = {\\left( {1,\\phi ,{\\bf{a}},\\theta ,\\psi } \\right)^\\prime }\\), where \\({\\bf{a}} = \\left( {{{\\bf{a}}^{(1)}},\\dots,{{\\bf{a}}^{(T)}}} \\right)\\) with \\({{\\bf{a}}^{(i)}} = \\left( {{{\\bf{1}}_{{k_i}}},{{\\bf{0}}_{{k_i}}}} \\right)\\) for the trigonometric case and \\({{\\bf{a}}^{(i)}} = \\left( {{{\\bf{0}}_{{m_i} - 1}},1} \\right)\\) for the regular seasonality case. Define the state error adjustment vector \\({\\bf{g}} = {\\left( {\\alpha ,\\beta ,\\gamma ,1,{{\\bf{0}}_{p - 1}},1,{{\\bf{0}}_{q - 1}}} \\right)^\\prime }\\). Further, let \\({\\bf{B}} = \\gamma &#39;\\theta\\), \\({\\bf{C}} = \\gamma &#39;\\psi\\) and \\({\\bf{A}} = \\oplus _{i = 1}^T{{\\bf{A}}_i}\\), with \\[\\begin{equation} {{\\bf{A}}_i} = \\left[ {\\begin{array}{*{20}{c}} {{{\\bf{C}}^{(i)}}}&amp;{{{\\bf{S}}^{(i)}}}\\\\ { - {{\\bf{S}}^{(i)}}}&amp;{{{\\bf{C}}^{(i)}}} \\end{array}} \\right], \\tag{5.3} \\end{equation}\\] for the trigonometric case and with \\(\\bf{C}^{(i)}\\) and \\(\\bf{S}^{(i)}\\) representing the \\(k_i\\times k_i\\) diagonal matrices with elements \\(cos(\\lambda_j^{(i)})\\) and \\(sin(\\lambda_j^{(i)})\\) respectively,11 and \\[\\begin{equation} {{\\bf{A}}_i} = \\left[ {\\begin{array}{*{20}{c}} {{{\\bf{0}}_{{m_i} - 1}}}&amp;1\\\\ {{{\\bf{I}}_{{m_i} - 1}}}&amp;{{{{\\bf{0&#39;}}}_{{m_i} - 1}}} \\end{array}} \\right], \\tag{5.4} \\end{equation}\\] for the regular seasonality case, with \\(\\oplus\\) being the direct sum of matrices operator. Finally, the state transition matrix \\(\\bf{F}\\) is composed as follows: \\[\\begin{equation} {\\bf{F}} = \\left[ {\\begin{array}{*{20}{c}} 1&amp;\\phi &amp;{{{\\bf{0}}_\\tau }}&amp;{\\alpha \\theta }&amp;{\\alpha \\psi }\\\\ 0&amp;\\phi &amp;{{{\\bf{0}}_\\tau }}&amp;{\\beta \\theta }&amp;{\\beta \\psi }\\\\ {{{{\\bf{0&#39;}}}_\\tau }}&amp;{{{{\\bf{0&#39;}}}_\\tau }}&amp;{\\bf{A}}&amp;{\\bf{B}}&amp;{\\bf{C}}\\\\ 0&amp;0&amp;{{{\\bf{0}}_\\tau }}&amp;\\theta &amp;\\psi \\\\ {{{{\\bf{0&#39;}}}_{p - 1}}}&amp;{{{{\\bf{0&#39;}}}_{p - 1}}}&amp;{{{\\bf{O}}_{p - 1,\\tau }}}&amp;{{{\\bf{I}}_{p - 1,p}}}&amp;{{{\\bf{O}}_{p - 1,q}}}\\\\ 0&amp;0&amp;{{{\\bf{0}}_\\tau }}&amp;{{{\\bf{0}}_p}}&amp;{{{\\bf{0}}_q}}\\\\ {{{{\\bf{0&#39;}}}_{q - 1}}}&amp;{{{{\\bf{0&#39;}}}_{q - 1}}}&amp;{{{\\bf{O}}_{q - 1,\\tau }}}&amp;{{{\\bf{O}}_{q - 1,p}}}&amp;{{{\\bf{I}}_{q - 1,q}}} \\end{array}} \\right] \\tag{5.5} \\end{equation}\\] where \\(\\tau = 2\\sum\\limits_{i = 1}^T {{k_i}}\\) for the trigonometric case and \\(\\tau = \\sum\\limits_{i = 1}^T {{m_i}}\\) for the regular seasonality case. 5.2 State Initialization A key innovation of the De Livera, Hyndman, and Snyder (2011) paper is providing the exact initialization of the non-stationary component’s seed states, the exponential smoothing analogue of the De Jong and others (1991) method for augmenting the Kalman filter to handle seed states with infinite variances. The proof, based on De Livera, Hyndman, and Snyder (2011) and expanded here is as follows: Let: \\[\\begin{equation} {\\bf{D}} = {\\bf{F}} - {\\bf{gw&#39;}}. \\tag{5.6} \\end{equation}\\] We eliminate \\(\\varepsilon_t\\) in (5.1) to give: \\[\\begin{equation} {{\\bf{x}}_t} = {\\bf{D}}{{\\bf{x}}_{t - 1}} + {\\bf{g}}{y_t}. \\tag{5.7} \\end{equation}\\] Next, we proceed by backsolving the equation for the error, given a given value of \\(\\lambda\\):12 \\[\\begin{equation} \\begin{array}{l} {\\varepsilon _t} = {y_t} - {\\bf{w}}{{{\\bf{\\hat x}}}_{t - 1}},\\\\ {\\varepsilon _t} = {y_t} - {\\bf{w&#39;}}\\left( {{\\bf{D}}{{{\\bf{\\hat x}}}_{t - 2}} + {\\bf{g}}{y_{t - 1}}} \\right). \\end{array} \\tag{5.8} \\end{equation}\\] Starting with \\(t = 4\\) and working backwards: \\[\\begin{equation} \\begin{array}{l} {\\varepsilon _4} &amp;= {y_4} - {\\bf{w&#39;}}\\left( {{\\bf{D}}{{{\\bf{\\hat x}}}_2} + {\\bf{g}}{y_3}} \\right)\\\\ &amp;= {y_4} - {\\bf{w&#39;}}\\left( {{\\bf{D}}\\left( {{\\bf{D}}{{{\\bf{\\hat x}}}_1} + {\\bf{g}}{y_2}} \\right) + {\\bf{g}}{y_3}} \\right)\\\\ &amp;= {y_4} - {\\bf{w&#39;}}\\left( {{\\bf{D}}\\left( {{\\bf{D}}\\left( {{\\bf{D}}{{{\\bf{\\hat x}}}_0} + {\\bf{g}}{y_1}} \\right) + {\\bf{g}}{y_2}} \\right) + {\\bf{g}}{y_3}} \\right)\\\\ &amp;= {y_4} - {\\bf{w&#39;}}\\left( {{\\bf{D}}\\left( {{{\\bf{D}}^2}{{\\bf{x}}_0} + {\\bf{Dg}}{y_1} + {\\bf{g}}{y_2}} \\right) + {\\bf{g}}{y_3}} \\right)\\\\ &amp;= {y_4} - {\\bf{w&#39;}}\\left( {{{\\bf{D}}^3}{{\\bf{x}}_0} + {{\\bf{D}}^2}{\\bf{g}}{y_1} + {\\bf{Dg}}{y_2} + {\\bf{g}}{y_3}} \\right)\\\\ &amp;= {y_4} - {\\bf{w&#39;}}\\sum\\limits_{j = 1}^3 {{{\\bf{D}}^{j - 1}}{\\bf{g}}{y_{4 - j}} - {\\bf{w&#39;}}{{\\bf{D}}^3}{{\\bf{x}}_0}} \\\\ \\end{array} \\tag{5.9} \\end{equation}\\] and generalizing to \\(\\varepsilon_t\\): \\[\\begin{equation} \\begin{array}{l} {\\varepsilon _t} &amp;= {y_t} - {\\bf{w&#39;}}\\left( {\\sum\\limits_{j = 1}^{t - 1} {{{\\bf{D}}^{j - 1}}{\\bf{g}}{y_{t - j}}} } \\right) - {\\bf{w&#39;}}{{\\bf{D}}^{t - 1}}{{\\bf{x}}_0}\\\\ &amp;= {y_t} - {\\bf{w&#39;}}{{{\\bf{\\tilde x}}}_{t - 1}} - {{{\\bf{w&#39;}}}_{t - 1}}{{\\bf{x}}_0}\\\\ &amp;= {{\\tilde y}_t} - {{{\\bf{w&#39;}}}_{t - 1}}{{\\bf{x}}_0}, \\end{array} \\tag{5.10} \\end{equation}\\] where \\({{\\tilde y}_t} = {y_t} - {\\bf{w&#39;}}{{{\\bf{\\tilde x}}}_{t - 1}}\\), \\({{{\\bf{\\tilde x}}}_t} = {\\bf{D}}{{{\\bf{\\tilde x}}}_{t - 1}} + {\\bf{g}}{y_t}\\), \\({{{\\bf{w&#39;}}}_t} = {\\bf{D}}{{{\\bf{w&#39;}}}_{t - 1}}\\), \\({{{\\bf{\\tilde x}}}_0} = 0\\) and \\({{{\\bf{w&#39;}}}_0} = {\\bf{w&#39;}}\\), so that \\(\\bf{x}_0\\) are the coefficients from the regression of \\(\\bf{w}\\) on \\(\\boldsymbol{\\varepsilon}\\). While this approach bypasses the need to estimate the initial states by augmenting the parameter vector, which could be very costly for multiple seasonality or large seasonal periods, it still requires one full iteration for \\(i=1,\\dots,t\\) to calculate \\(\\boldsymbol{\\varepsilon}\\) and \\(\\bf{w}\\) and then one inversion to get the coefficients for every new set of parameters (i.e. for each new candidate set in the optimization). 5.3 Log-Likelihood The log-likelihood (\\(L\\)) of the model follows from the assumption that the innovations \\(\\varepsilon_t\\sim N\\left(0, \\sigma^2\\right)\\), leading to the following form for the transformed series \\(y^{\\lambda}_t\\): \\[\\begin{equation} L\\left( \\theta \\right) = - \\frac{T}{2}\\log \\left( {2\\pi {{\\sigma }^2}} \\right) - \\frac{1}{{2{{\\sigma }^2}}}\\sum\\limits_{t = 1}^T {\\varepsilon _t^2 + \\left( {\\lambda - 1} \\right)\\sum\\limits_{t = 1}^T {\\log {y_t}} }. \\tag{5.11} \\end{equation}\\] Concentrating out \\(\\sigma^2\\) with its maximum likelihood estimate, \\({\\hat \\sigma^2} = {T^{ - 1}}\\sum\\limits_{t = 1}^T {\\varepsilon_t^2}\\), eliminating constants and taking the negative for minimization in the optimization routine leads to the following form: \\[\\begin{equation} L\\left( \\theta \\right) = T\\log \\sum\\limits_{t = 1}^T {\\varepsilon _t^2} - 2\\left( {\\lambda - 1} \\right)\\sum\\limits_{t = 1}^T {\\log {y_t}}, \\tag{5.12} \\end{equation}\\] where \\(\\theta\\) is the vector of parameters being optimized. The returned value of calling method logLik on an object of class tsissm.estimate is that of equation (5.11). 5.4 Package Implementation The implementation of the model in the tsissm package differs significantly from the one provided by R. Hyndman et al. (2021) in the tbats and bats functions, mainly in terms of a more flexible specification object and more complete methods for working with the model, but currently lacks the automatic model selection functionality. Additionally, in order to ensure that the parameters are within the forecastability region, we constrain the characteristics roots of the matrix \\(\\bf{D}\\) in (5.2), representing the non-stationary components to be inside the unit circle, and also constrain the ARMA roots for stationarity.13 5.5 Demonstration 5.5.1 Specification The specification function defines the entry point for setting up an issm model: library(tsissm) args(issm_modelspec) ## function (y, slope = TRUE, slope_damped = FALSE, seasonal = FALSE, ## seasonal_frequency = 1, seasonal_type = c(&quot;trigonometric&quot;, ## &quot;regular&quot;), seasonal_harmonics = NULL, ar = 0, ma = 0, ## xreg = NULL, lambda = 1, sampling = NULL, ...) ## NULL The specification has options for slope, dampening, seasonality (trigonometric and regular),14 AR and MA terms and external regressors. Additionally, lambda can be fixed or estimated (by setting lambda = NA). At present, we do not offer automatic model selection, but instead leave it to the user to decide on the appropriate model. In the future we may include a function for automatic selection similar to the auto_ets function in the tsets package. 5.5.2 Estimation We showcase the functionality of the package using the electricload dataset from the tsdatasets package which represents total hourly electricity load in Greece in MW as published on ENTSO-E Transparency Platform, for the period 2016-10-17 to 2019-04-30. The series appears to have multiple seasonality with periodicity 24, \\(24\\times 7\\) and \\(24\\times 7 \\times 52\\), which we model with trigonometric terms. library(tsissm) data(&quot;electricload&quot;, package = &quot;tsdatasets&quot;) # specification spec &lt;- issm_modelspec(electricload, slope = T, seasonal = TRUE, seasonal_frequency = c(24, 4500), seasonal_type = &quot;trigonometric&quot;, seasonal_harmonics = c(6, 6), ar = 2, ma = 2, lambda = 0.25) # estimation mod &lt;- suppressMessages(estimate(spec, solver = &quot;nlminb&quot;, control = list(trace = 0))) mod$opt$elapsed ## Time difference of 1.570734 mins summary(mod) ## ISSM Model: AAA(24{6}/4500{6})+ARMA(2,2) ## ## Parameter Est[Value] Lower Upper ## ------------ ----------- ------ ------ ## alpha 0.0000 0.00 0.99 ## beta 0.0000 0.00 0.99 ## gamma24.1 0.0141 -0.01 0.99 ## gamma24.2 -0.0045 -0.01 0.99 ## gamma4500.1 0.0198 -0.01 0.99 ## gamma4500.2 -0.0100 -0.01 0.99 ## theta1 0.0862 -0.99 0.99 ## theta2 0.5951 -0.99 0.99 ## psi1 0.8107 -0.99 0.99 ## psi2 -0.0112 -0.99 0.99 ## lambda 0.2500 0.25 0.25 ## ## tsissm: Performance Metrics ## ---------------------------------- ## AIC : 438046.05 (n = 40) ## MAPE : 0.01689 ## BIAS : 0.00026 ## MSLRE : 5e-04 The plot method decomposes the estimated model into it’s components: plot(mod) but we can also extract these directly and plot them using the tsdecompose method: td &lt;- tsdecompose(mod) plot(as.zoo(td), main = &quot;ISSM Decomposition&quot;) Additional inference methods include diagnostics (tsdiagnose) as well as standard extractors for the coefficients (coef), log-likelihood (logLik) and AIC (AIC): tsdiagnose(mod, plot = TRUE) ## ## ARMA roots ## ------------------------------------------ ## Inverse AR roots: 0.8157245 0.7295345 ## Inverse MA roots: 0.8242561 0.01357715 ## ## Forecastability (D roots) ## ------------------------------------------ ## Real Eigenvalues (D): 1.001 1 1 1 1 1 1 1 1 1 1 1 1 0.008 0.008 0.252 0.252 0.961 0.961 0.493 0.493 0.7 0.7 0.86 0.86 0.875 0.824 0.014 0 0 ## ## Weighted Ljung-Box Test [scaled residuals] ## ------------------------------------------ ## Lag statistic pvalue ## Lag[1] 29.7 5.13e-08 ## Lag[11] 958.2 0.00e+00 ## Lag[11] 958.2 0.00e+00 ## Lag[11] 958.2 0.00e+00 ## ## Outlier Diagnostics (based on Rosner Test) ## ------------------------------------------ ## Outliers: 2017-05-10 19:00:00 2019-01-22 14:00:00 2017-03-02 17:00:00 2019-01-22 08:00:00 2018-12-25 12:00:00 2019-01-22 05:00:00 2017-12-25 12:00:00 2017-11-10 18:00:00 2019-01-22 09:00:00 2019-01-03 07:00:00 coef(mod) ## alpha beta gamma24.1 gamma24.2 gamma4500.1 ## 5.070691e-09 8.118197e-07 1.412816e-02 -4.492748e-03 1.982569e-02 ## gamma4500.2 theta1 theta2 psi1 psi2 ## -1.000000e-02 8.618996e-02 5.950992e-01 8.106790e-01 -1.119105e-02 logLik(mod) ## &#39;log Lik.&#39; -283669.3 (df=41) AIC(mod) ## [1] 438046.1 5.5.3 Prediction Similar to the other packages in the tsmodels framework, prediction builds a distribution of possible paths by simulation, outputting an object of class tsmodel.predict: p &lt;- predict(mod, h = 24*10) plot(p, n_original = 24*20, main = &quot;10-Day Hourly Forecast&quot;) The predict object also has a decomposition method: td &lt;- tsdecompose(p) par(mfrow = c(3,2),mar = c(3,3,3,3)) plot(td$Level, n_original = 24*20, main = &quot;Level[State] Predicted Distribution&quot;) plot(td$Seasonal24, n_original = 24*20, main = &quot;Seasonal24[State] Predicted Distribution&quot;) plot(td$Seasonal4500, n_original = 24*20, main = &quot;Seasonal4500[State] Predicted Distribution&quot;) plot(td$AR2, n_original = 24*20, main = &quot;AR2[State] Predicted Distribution&quot;) plot(td$MA2, n_original = 24*20, main = &quot;MA2[State] Predicted Distribution&quot;) 5.5.4 Simulation Simulation of an estimated object has options for changing the coefficients as well as the initial states, as well as the option for providing custom innovations or bootstrapped innovations: args(tsissm:::simulate.tsissm.estimate) ## function (object, nsim = 1, seed = NULL, h = NULL, newxreg = NULL, ## sim_dates = NULL, bootstrap = FALSE, innov = NULL, sigma_scale = 1, ## pars = coef(object), init_states = object$spec$xseed, ...) ## NULL sim &lt;- simulate(mod, nsim = 100, h = 24 * 10, bootstrap = TRUE) plot(sim) While the plot function on a simulated object provides a decomposition of the actual and state components distributions, it is useful to remind ourselves that the distribution bands represent the range of uncertainty of multiple paths. This is best illustrated by plotting these separately: matplot(as.POSIXct(colnames(sim$Simulated)), t(sim$Simulated), type = &quot;l&quot;, col = 1:100, ylab = &quot;&quot;, xlab = &quot;&quot;, main = &quot;Simulated Paths&quot;) grid() 5.5.5 Filtering Online filtering is when new data arrives and instead of re-estimating the model, we instead just filter the new data based on an existing model. In the tsissm package the tsfilter method updates an object of class tsmodel.estimate with new data as the example below illustrates. Because the class of the model is retained and only updated (both data and states) with new information, it is also possible to apply any method to that which admits that object (e.g. predict). library(tsissm) suppressMessages(library(xts)) weekends &lt;- xts(matrix(0, ncol = 1, nrow = nrow(electricload)), index(electricload)) weekends[which(weekdays(as.Date(index(weekends))) %in% c(&quot;Saturday&quot;,&quot;Sunday&quot;))] &lt;- 1 colnames(weekends) &lt;- &quot;weekend&quot; spec &lt;- issm_modelspec(electricload[1:22000], slope = TRUE, seasonal = TRUE, seasonal_frequency = c(24, 4500), xreg = weekends[1:22000], seasonal_type = &quot;trigonometric&quot;, seasonal_harmonics = c(6, 6), ar = 2, ma = 2, lambda = 0.25) mod &lt;- estimate(spec, solver = &quot;nlminb&quot;) f1 &lt;- tsfilter(mod, y = electricload[22001:22100], newxreg = weekends[22001:22100]) tail(fitted(mod)) ## Warning: timezone of object (UTC) is different than current timezone (). ## [,1] ## 2019-04-21 10:00:00 5603.237 ## 2019-04-21 11:00:00 5507.236 ## 2019-04-21 12:00:00 5089.185 ## 2019-04-21 13:00:00 4914.440 ## 2019-04-21 14:00:00 4832.111 ## 2019-04-21 15:00:00 5045.866 head(tail(fitted(f1), 100),10) ## Warning: timezone of object (UTC) is different than current timezone (). ## [,1] ## 2019-04-21 16:00:00 5446.676 ## 2019-04-21 17:00:00 5795.231 ## 2019-04-21 18:00:00 6160.462 ## 2019-04-21 19:00:00 5907.228 ## 2019-04-21 20:00:00 5284.130 ## 2019-04-21 21:00:00 4887.613 ## 2019-04-21 22:00:00 4495.786 ## 2019-04-21 23:00:00 4216.449 ## 2019-04-22 00:00:00 4220.968 ## 2019-04-22 01:00:00 4099.671 plot(f1) f2 &lt;- tsfilter(f1, y = electricload[22100:22200], newxreg = weekends[22100:22200]) tail(fitted(f1)) ## Warning: timezone of object (UTC) is different than current timezone (). ## [,1] ## 2019-04-25 14:00:00 5740.874 ## 2019-04-25 15:00:00 5759.819 ## 2019-04-25 16:00:00 5968.466 ## 2019-04-25 17:00:00 6052.751 ## 2019-04-25 18:00:00 6085.724 ## 2019-04-25 19:00:00 5675.171 head(tail(fitted(f2), 100),10) ## Warning: timezone of object (UTC) is different than current timezone (). ## [,1] ## 2019-04-25 20:00:00 5083.078 ## 2019-04-25 21:00:00 4760.420 ## 2019-04-25 22:00:00 4393.773 ## 2019-04-25 23:00:00 4188.600 ## 2019-04-26 00:00:00 4145.501 ## 2019-04-26 01:00:00 4058.967 ## 2019-04-26 02:00:00 3974.263 ## 2019-04-26 03:00:00 4224.661 ## 2019-04-26 04:00:00 4393.636 ## 2019-04-26 05:00:00 4603.391 5.5.6 Profiling The tsprofile function profiles an estimated model by simulating and then estimating multiple paths from the assumed DGP while leaving h values out for prediction evaluation. Each simulated path is equal to the size of the original dataset plus h additional values, and initialized with the initial state vector from the model. The resulting output contains the distribution of the MAPE, percent bias (BIAS) and mean squared log relative error (MSLRE) per horizon h. Since these matrices are of class tsmodel.distribution they can be readily plotted with the special purpose plot function for this class from the tsmethods package. Additionally, a data.table matrix is also returned with the distribution of the coefficients from each path estimation. As of version 0.2.0, the tsprofile method on a tsissm.estimate object does not support newxreg. spec &lt;- issm_modelspec(electricload[1:(24*24)], slope = T, seasonal = TRUE, seasonal_frequency = c(24), seasonal_type = &quot;trigonometric&quot;, seasonal_harmonics = c(6), ar = 2, ma = 2, lambda = 0.25) mod &lt;- estimate(spec, solver = &quot;nlminb&quot;) prof &lt;- tsprofile(mod, h = 24*7, nsim = 100, cores = 4, solver = &quot;nlminb&quot;) plot(prof) plot(prof, type = &quot;coef&quot;) 5.5.7 Backtesting The tsbacktest method generates an expanding window walk forward backtest, returning a list with the estimation/horizon predictions against actuals as well as a table of average performance metrics by horizon. Optionally, instead of re-estimating every one period, the option to re-estimate every n-periods is available via the estimate_every option. spec &lt;- issm_modelspec(electricload[1:(24*24)], slope = T, seasonal = TRUE, seasonal_frequency = c(24), seasonal_type = &quot;trigonometric&quot;, seasonal_harmonics = c(6), ar = 2, ma = 2, lambda = 0.25) b &lt;- tsbacktest(spec, start = 24*12, h = 24, estimate_every = 24, alpha = c(0.02, 0.1), cores = 5, data_name = &quot;electricity&quot;, solver = &quot;nlminb&quot;, trace = FALSE) head(b$metrics) ## horizon variable MAPE MSLRE BIAS n MIS[0.02] ## 1: 1 electricity 0.05624924 0.015795230 -0.032746632 12 9469.196 ## 2: 2 electricity 0.06856221 0.013663906 -0.016276198 12 9163.970 ## 3: 3 electricity 0.06432079 0.006912452 0.035974543 12 5888.073 ## 4: 4 electricity 0.07081287 0.009272400 0.033881651 12 6272.546 ## 5: 5 electricity 0.05195051 0.004358340 -0.005635886 12 2412.319 ## 6: 6 electricity 0.05854094 0.005585492 -0.008948530 12 2938.341 ## MIS[0.1] ## 1: 2438.034 ## 2: 2819.816 ## 3: 2276.193 ## 4: 2686.843 ## 5: 1469.002 ## 6: 1957.761 plot(b$metrics$horizon, b$metrics$MAPE*100, main = &quot;Horizon MAPE&quot;, type = &quot;l&quot;, ylab = &quot;MAPE[%]&quot;, xlab = &quot;horizon&quot;) grid() The variable n in the table reports the number of h-step ahead predictions made on which the average metrics were calculated. 5.5.8 Benchmarking The tsbenchmark* method is used to benchmark a model for timing and accuracy and can be used as a unit testing function. spec &lt;- issm_modelspec(electricload[1:(24*50)], slope = T, seasonal = TRUE, seasonal_frequency = c(24), seasonal_type = &quot;trigonometric&quot;, seasonal_harmonics = c(6), ar = 2, ma = 2, lambda = 0.25) bench &lt;- rbind(tsbenchmark(spec, solver = &quot;nlminb&quot;), tsbenchmark(spec, solver = &quot;nlminb&quot;, autodiff = TRUE), tsbenchmark(spec, solver = &quot;optim&quot;)) print(bench) ## start end spec ## 1: 2021-11-20 09:22:34 2021-11-20 09:22:35 &lt;tsissm.spec[11]&gt; ## 2: 2021-11-20 09:22:35 2021-11-20 09:22:37 &lt;tsissm.spec[11]&gt; ## 3: 2021-11-20 09:22:37 2021-11-20 09:22:43 &lt;tsissm.spec[11]&gt; ## estimate solver control loglik ## 1: &lt;tsissm.estimate[5]&gt; nlminb 0 -15483.49 ## 2: &lt;tsissm.estimate[5]&gt; nlminb 0 -15483.49 ## 3: &lt;tsissm.estimate[5]&gt; optim 0 -15311.44 print(bench$end - bench$start) ## Time differences in secs ## [1] 1.297783 1.311464 6.788253 The implementation of the constraints for both the \\(\\bf{D}\\) matrix as well as the ARMA roots are non-smooth and hence care should be taken in checking the solution. References "],["tsvets.html", "Chapter 6 tsvets package 6.1 Introduction 6.2 Inclusion of External Regressors 6.3 Log-Likelihood 6.4 Dependence Structure 6.5 Grouping and Pooling 6.6 Homogeneous Coefficients and Aggregation 6.7 Demonstration", " Chapter 6 tsvets package 6.1 Introduction The vector exponential additive smoothing model (Vector ETS), introduced in De Silva, Hyndman, and Snyder (2010) naturally generalizes the univariate framework with a great deal of flexibility with the dynamics of the unobserved components. The dynamics of the states can be common, diagonal, grouped or fully parameterized, allowing for a rich set of patterns to be captured. The tsvets package implements a more general and flexible version based on Athanasopoulos and Silva (2012), with methods for estimation, inference, visualization, forecasting as well as aggregation. Let the vector of \\(N\\) series at time \\(t\\), \\(\\bf{y}_t\\) be represented as a linear additive combination of their unobserved level, slope and seasonal (with frequency \\(m\\)) components, \\(\\bf{l}_{t-1}\\), \\(\\bf{b}_{t-1}\\) and \\(\\bf{s}_{t-m}\\) respectively. Formally, the conditional mean of \\(\\bf{\\hat y}_t\\) is given by: \\[\\begin{equation} \\bf{\\hat y}_t = \\bf{l}_{t - 1} + \\Phi\\bf{b}_{t - 1} + \\bf{s}_{t - m}, \\tag{6.1} \\end{equation}\\] where \\(\\Phi\\) is the \\(N\\times N\\) matrix of dampening parameters. In its reduced form, the model without seasonality is equivalent to a VARIMA(1,2,2) model,15 and \\(\\Phi\\) becomes the matrix of first order autoregressive coefficients. The 1 step ahead forecast errors, \\(\\boldsymbol{\\varepsilon}_t\\) follow a multivariate normal distribution: \\[\\begin{equation} {\\boldsymbol{\\varepsilon} _t} = {\\bf{y}_t} - {{\\bf{\\hat y}}_t},\\quad {\\boldsymbol{\\varepsilon} _t} \\sim {\\bf{N}}\\left( {{\\bf{0}},\\Sigma } \\right). \\tag{6.2} \\end{equation}\\] The state equations have the following dynamics: \\[\\begin{equation} \\begin{array}{l} {\\bf{l}_t} = {\\bf{l}_{t - 1}} + \\Phi {\\bf{b}_{t - 1}} + {\\bf{A}}{\\boldsymbol{\\varepsilon}_t},\\\\ {\\bf{b}_t} = \\Phi {\\bf{b}_{t - 1}} + {\\bf{B}}{\\boldsymbol{\\varepsilon}_t},\\\\ {\\bf{s}_t} = {\\bf{s}_{t - m}} + {G_1}K{\\boldsymbol{\\varepsilon}_t},\\\\ {\\bf{s}_{t - m}} = {\\bf{s}_{t - i}} + {G_2}K{\\boldsymbol{\\varepsilon}_t}, \\end{array} \\tag{6.3} \\end{equation}\\] where the matrices \\(\\mathbf{A}\\), \\(\\mathbf{B}\\) and \\(\\mathbf{K}\\) represent the adjustment of the vector components to the errors, and can be diagonal, fully parameterized or scalar (common adjustment). In vector innovations state space form, the system can be written as follows: \\[\\begin{equation} \\begin{array}{l} {\\bf{y}_t} = H{\\bf{x}_{t - 1}} + {\\boldsymbol{\\varepsilon}_t},\\\\ {\\bf{x}_t} = F{\\bf{x}_{t - 1}} + GA{\\boldsymbol{\\varepsilon}_t} \\end{array} \\tag{6.4} \\end{equation}\\] where the matrices \\(H\\), \\(F\\), \\(G\\) and \\(A\\) are composed as follows: \\[\\begin{equation} \\mathop H\\limits_{\\left( {m + 2} \\right)N \\times N} = \\left[ {\\begin{array}{*{20}{c}} {{I_N}}\\\\ {{I_N}}\\\\ {{0_{N \\times N}}}\\\\ \\vdots \\\\ {{0_{N \\times N}}}\\\\ {{I_N}} \\end{array}} \\right],\\quad \\mathop A\\limits_{\\left( {m + 2} \\right)N \\times N} = \\left[ {\\begin{array}{*{20}{c}} {\\bf{A}}\\\\ {\\bf{B}}\\\\ {\\bf{K}}\\\\ \\vdots \\\\ \\vdots \\\\ {\\bf{K}} \\end{array}} \\right], \\tag{6.5} \\end{equation}\\] \\[\\begin{equation} \\mathop F\\limits_{\\left( {m + 2} \\right)N \\times \\left( {m + 2} \\right)N} = \\left[ {\\begin{array}{*{20}{c}} {{I_N}}&amp;{{I_N}}&amp;{{0_{N \\times mN}}}\\\\ {{0_{N \\times N}}}&amp;{{\\Phi _N}}&amp;{{0_{N \\times mN}}}\\\\ {{0_{mN \\times N}}}&amp;{{0_{mN \\times N}}}&amp;{\\tilde{F} \\otimes {I_N}} \\end{array}} \\right],\\mathop {\\tilde{F}}\\limits_{m \\times m} = \\left[ {\\begin{array}{*{20}{c}} 0&amp;0&amp;0&amp; \\cdots &amp;0&amp;1\\\\ 1&amp;0&amp;0&amp; \\cdots &amp;0&amp;0\\\\ 0&amp;1&amp;0&amp; \\cdots &amp;0&amp;0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0&amp;0&amp;0&amp; \\cdots &amp;1&amp;0 \\end{array}} \\right], \\tag{6.6} \\end{equation}\\] \\[\\begin{equation} \\mathop G\\limits_{\\left( {m + 2} \\right)N \\times \\left( {m + 2} \\right)N} = \\left[ {\\begin{array}{*{20}{c}} {{I_N}}&amp;{{0_{N \\times N}}}&amp;{{0_{N \\times mN}}}\\\\ {{0_{N \\times N}}}&amp;{{I_N}}&amp;{{0_{N \\times mN}}}\\\\ {{0_{mN \\times N}}}&amp;{{0_{mN \\times N}}}&amp;{\\tilde{G} \\otimes {I_N}} \\end{array}} \\right], \\tag{6.7} \\end{equation}\\] \\[\\begin{equation} \\mathop {\\tilde{G}}\\limits_{m \\times m} = \\left[ {\\begin{array}{*{20}{c}} {\\frac{{m - 1}}{m}}&amp;0&amp; \\cdots &amp;0&amp;0\\\\ 0&amp;{ - \\frac{1}{m}}&amp; \\cdots &amp;0&amp;0\\\\ 0&amp;0&amp;{ - \\frac{1}{m}}&amp;0&amp;0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0&amp;0&amp;0&amp; \\cdots &amp;{ - \\frac{1}{m}} \\end{array}} \\right], \\tag{6.8} \\end{equation}\\] \\[\\begin{equation} \\mathop {{\\bf{x}_t}}\\limits_{\\left( {m + 2} \\right)N \\times N} = \\left[ {\\begin{array}{*{20}{c}} {{\\bf{l}_t}}\\\\ {{\\bf{b}_t}}\\\\ {{\\bf{s}_t}}\\\\ \\vdots \\\\ {{\\bf{s}_{t - m + 2}}}\\\\ {{\\bf{s}_{t - m + 1}}} \\end{array}} \\right]. \\tag{6.9} \\end{equation}\\] The values in the seasonal matrix \\(\\tilde{G}\\) represent normalization terms which ensure that the seasonal component adds to zero throughout the updating process without becoming contaminated by the level component. 6.2 Inclusion of External Regressors We augment the model with the ability to include external regressors (\\(z_t\\)) such that: \\[\\begin{equation} \\begin{array}{l} {\\bf{y}_t} = H{\\bf{x}_{t - 1}} + \\mathbf{W}\\bf{z}_t + {\\boldsymbol{\\varepsilon}_t},\\\\ {\\bf{x}_t} = F{\\bf{x}_{t - 1}} + GA{\\boldsymbol{\\varepsilon}_t}, \\end{array} \\tag{6.10} \\end{equation}\\] where the \\(W\\) is the matrix of coefficients which are time invariant. The xreg_include argument in the vets_modelspec function is a design matrix which allows one to define which coefficients should be set to zero, which should be estimated individually as well as which should be pooled (see the documentation for more details). The index on \\(\\bf{z}\\) is \\(t\\) and not \\(t-1\\) and it is left to the user to pre-lag any regressors passed to the function. 6.3 Log-Likelihood The log-likelihood (\\(L\\)) of the model follows from the assumption that the innovations \\(\\boldsymbol{\\varepsilon_t}\\sim N\\left(\\bf{0}, \\Sigma\\right)\\), leading to the following function: \\[\\begin{equation} L\\left(\\boldsymbol{\\theta} \\right) = \\frac{T}{2}\\left( {N\\log \\left( {2\\pi } \\right) + N\\log \\left| \\Sigma \\right|} \\right) - \\frac{1}{2}\\sum\\limits_{t = 1}^T {\\boldsymbol{\\varepsilon_t}{\\Sigma ^{ - 1}}\\boldsymbol{\\varepsilon_t}}, \\tag{6.11} \\end{equation}\\] where \\(\\boldsymbol{\\theta}\\) is the vector of parameters being optimized. We also concentrate out the parameters of the covariance matrix by using its ML estimator \\[\\begin{equation} \\hat \\Sigma = \\frac{1}{T}\\sum\\limits_{t = 1}^T {{\\boldsymbol{\\varepsilon}_t}{{\\boldsymbol{\\varepsilon}&#39;}_t}}. \\tag{6.12} \\end{equation}\\] Therefore, the vector ETS log-likelihood is proportional to \\[\\begin{equation} L\\left(\\boldsymbol{\\theta} \\right) \\propto \\sum\\limits_{t = 1}^T {{{\\boldsymbol{\\varepsilon}&#39;}_t}{{\\hat \\Sigma }^{ - 1}}{\\boldsymbol{\\varepsilon}_t}}, \\tag{6.13} \\end{equation}\\] which requires looping through each \\(t\\) to calculate the quadratic form which is expensive. Instead, we can make use of the following relationship, assuming positive-definite or positive-semi-definite \\(\\hat \\Sigma\\), \\({\\hat\\Sigma ^{-1}} = Q\\Lambda^{-1} Q&#39;\\), where \\(\\Lambda\\) is the diagonal matrix of eigenvalues of \\(\\hat\\Sigma\\). We then have \\({\\varepsilon _t} = Q{\\bf{u}_t}\\) where \\(\\bf{u}_t\\) are the projections onto the eigenvectors \\(Q\\). The negative of the log-likelihood can be represented as: \\[\\begin{equation} L\\left( \\boldsymbol{\\theta} \\right) = \\frac{1}{2}T\\left( {N\\log 2\\pi - \\log \\left| {\\hat \\Sigma } \\right|} \\right) + \\frac{1}{2}{{{\\bf{1&#39;}}}_T}\\left( {{{\\left( {\\varepsilon Q} \\right)}^2}\\frac{1}{\\lambda }} \\right), \\tag{6.14} \\end{equation}\\] where we have used the relation \\(Q^{-1} = Q&#39;\\) due to \\(\\hat{\\Sigma}\\) being symmetric (and thus \\(Q\\) is orthogonal), and \\(1 / \\lambda\\) is a vector containing the reporicals of the eigenvalues. Additionally, we constrain the \\(N+1\\) largest eigenvalues (\\(\\lambda_s\\)) to be less than 1 to ensure invertibility of the system, such that \\[\\begin{equation} {\\lambda _s}\\left( D \\right) &lt; 1,D = F - GAH. \\tag{6.15} \\end{equation}\\] This is added as a soft barrier constraint. The diagonal elements of the level, slope, dampening and seasonal matrices are bounded between 0 and 1, while the off diagonal elements are allowed to vary between -1 and 1.16 Finally, the initial seed values for each of the states are approximated using the heuristic approach from the univariate ETS model as described in Section 4.5.2. 6.4 Dependence Structure While Equation (5.10) assumes a full covariance matrix, allowing contemporaneous associations among the residuals, we also offer 3 additional estimators for the dependence structure: diagonal covariance, equicorrelation and shrinkage covariance based on the estimator of Ledoit and Wolf (2004). 6.4.1 Diagonal Covariance The diagonal covariance matrix is the one used by Athanasopoulos and Silva (2012), and leads to the fastest estimation. In this case, the log-likelihood is greatly simplified and equal to \\[\\begin{equation} \\frac{T}{2}\\left( {N\\log \\left( {2\\pi } \\right) + N\\log \\left| \\Sigma \\right|} \\right) - \\frac{1}{2}\\sum\\limits_{t = 1}^T {\\sum\\limits_{i = 1}^N {\\varepsilon _{it}^2/\\sigma _i^2} }. \\tag{6.16} \\end{equation}\\] 6.4.2 Equicorrelation The equicorrelation covariance assumes that the correlation across all series is set to some common value \\(\\rho\\). The correlation matrix \\(\\bf{R}\\) can be calculated as \\[\\begin{equation} {\\bf{R}} = \\rho {\\bf{11&#39;}} + \\left( {1 - \\rho } \\right){\\bf{I}}, \\tag{6.17} \\end{equation}\\] which is guaranteed to be positive definite as long as \\(-\\frac{1}{{N - 1}}&lt;\\rho&lt; 1\\). The covariance is then equal to: \\[\\begin{equation} {\\Sigma} = {\\bf{DRD&#39;}}, \\tag{6.18} \\end{equation}\\] where \\({\\bf{D}} = diag\\left( {{\\hat \\sigma _1},\\dots,{\\hat \\sigma _n}} \\right)\\). Some of the advantages of assuming equicorrelation are discussed in Clements, Scott, and Silvennoinen (2015). 6.4.3 Shrinkage Covariance The shrinkage estimator of Ledoit and Wolf (2004) follows from the observation that the eigenvalues of the estimated correlations tend to be more dispersed than the eigenvalues of the true data generating process. The shrinkage estimator of the covariance is based on a convex combination of the sample covariance \\(\\hat\\Sigma\\) and a target covariance set to a multiple of the identity matrix. It is this combination weight \\(\\rho\\) which we estimate in the case of the shrinkage estimator \\[\\begin{equation} {\\Sigma} = \\left( {1 - \\rho } \\right)\\Sigma + \\frac{\\rho }{n}tr\\left( \\Sigma \\right){\\bf{I}}. \\tag{6.18} \\end{equation}\\] 6.5 Grouping and Pooling The tsvets package allows both global pooling of state component coefficients as well as group-wise pooling. For instance, if we had a large-dimensional system composed of series which have some common grouping structure (e.g. geographical, feature or statistical based), we could impose that these groups have common dynamics for some or all of the components. We provide an example of this in the demonstration section. 6.6 Homogeneous Coefficients and Aggregation When a model is estimated with all components pooled (i.e. common coefficients), then we can aggregate the model to obtain an aggregated representation in closed form, following Section 17.1.2 of R. Hyndman et al. (2008). When this is not the case, we can still obtain an aggregated series from the estimation and prediction objects. This functionality is implemented via the tsaggregate method and we provide an example in the demonstration section. 6.7 Demonstration 6.7.1 Specification The specification function defines the entry point for setting up a vets model: suppressWarnings(suppressPackageStartupMessages(library(tsvets))) args(vets_modelspec) ## function (y, level = c(&quot;constant&quot;, &quot;diagonal&quot;, &quot;common&quot;, &quot;full&quot;, ## &quot;grouped&quot;), slope = c(&quot;none&quot;, &quot;constant&quot;, &quot;common&quot;, &quot;diagonal&quot;, ## &quot;full&quot;, &quot;grouped&quot;), damped = c(&quot;none&quot;, &quot;common&quot;, &quot;diagonal&quot;, ## &quot;full&quot;, &quot;grouped&quot;), seasonal = c(&quot;none&quot;, &quot;common&quot;, &quot;diagonal&quot;, ## &quot;full&quot;, &quot;grouped&quot;), group = NULL, xreg = NULL, xreg_include = NULL, ## frequency = 1, lambda = NULL, lambda_lower = 0, lambda_upper = 1.5, ## dependence = c(&quot;diagonal&quot;, &quot;full&quot;, &quot;equicorrelation&quot;, &quot;shrinkage&quot;), ## cores = 1) ## NULL The specification has options for how the components of level, slope, dampening, seasonality are structured as well as the type of dependence to use. We also allow multicore processing since we require the initial state vectors to be calculated through calls to the tsets package, which can be done in parallel. The lambda argument can be set either to NA, in which case the multivariate version of the Box Cox transformation is used which targets a transformation to multivariate normality based on Velilla (1993) using the powerTransform function from the car package of Fox, Weisberg, and Price (2021), a vector of length equal to the number of series of a single number to apply to all series, and NULL in which case no transformation is performed. The next sections provide fully worked examples with methods showcasing the functionality of the package under different assumptions. 6.7.2 Example: Australian Retail Sales We use a subset of the Australian retail dataset from package tsdatasets representing the monthly retail turnover in $Million AUD across different regions for the news vendor category, with common level, constant slope and diagonal seasonal and dependence structure. suppressWarnings(suppressPackageStartupMessages(library(tsvets))) suppressMessages(library(tsmethods)) suppressMessages(library(tsaux)) suppressMessages(library(xts)) data(&quot;austretail&quot;, package = &quot;tsdatasets&quot;) y &lt;- na.omit(austretail[,grepl(&quot;NEWS&quot;, colnames(austretail))]) spec &lt;- vets_modelspec(y, level = &quot;common&quot;, slope = &quot;constant&quot;, damped = &quot;none&quot;, seasonal = &quot;diagonal&quot;, lambda = NA, dependence = &quot;diagonal&quot;, frequency = 12, cores = 3) mod &lt;- estimate(spec, solver = &quot;solnp&quot;, control = list(trace = 0)) The joint estimation of the 8 series takes about 0.280345 seconds. The summary object prints the full matrices for each component using the Matrix package of Bates and Maechler (2021). The summary method also take an optional weights argument which is used to calculate the weighted Accuracy Criteria, and when this is NULL, an equal weight vector is used instead (and hence equivalent to the Mean Criteria). Similar to other packages, there is a diagnostics method tsdiagnose which prints the eiganvalues of the \\(D\\) matrix as well as the output from a multivariate Normality Test and Multivariate Outliers based on the mvn function of the MVN package of Korkmaz, Goksuluk, and Zararsiz (2021). summary(mod) ## ## Vector ETS ## ----------------------------------- ## Level : common ## Slope : constant ## Seasonal : diagonal ## Dependence : diagonal ## No. Series : 8 ## No. TimePoints : 369 ## ## Parameter Matrices ## ## Level Matrix ## 8 x 8 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 0.7228622 . . . . . . ## [2,] . 0.7228622 . . . . . ## [3,] . . 0.7228622 . . . . ## [4,] . . . 0.7228622 . . . ## [5,] . . . . 0.7228622 . . ## [6,] . . . . . 0.7228622 . ## [7,] . . . . . . 0.7228622 ## [8,] . . . . . . . ## [,8] ## [1,] . ## [2,] . ## [3,] . ## [4,] . ## [5,] . ## [6,] . ## [7,] . ## [8,] 0.7228622 ## ## Slope Matrix ## 8 x 8 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 0 . . . . . . . ## [2,] . 0 . . . . . . ## [3,] . . 0 . . . . . ## [4,] . . . 0 . . . . ## [5,] . . . . 0 . . . ## [6,] . . . . . 0 . . ## [7,] . . . . . . 0 . ## [8,] . . . . . . . 0 ## ## Seasonal Matrix ## 8 x 8 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.2856892 . . . . . ## [2,] . 2.394973e-09 . . . . ## [3,] . . 3.846394e-09 . . . ## [4,] . . . 5.731333e-09 . . ## [5,] . . . . 3.579614e-09 . ## [6,] . . . . . 6.533816e-09 ## [7,] . . . . . . ## [8,] . . . . . . ## [,7] [,8] ## [1,] . . ## [2,] . . ## [3,] . . ## [4,] . . ## [5,] . . ## [6,] . . ## [7,] 0.1510414 . ## [8,] . 2.543775e-09 ## ## Correlation Matrix ## 8 x 8 Matrix of class &quot;dsyMatrix&quot; ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS SA.NEWS T.NEWS ## ACF.NEWS 1.00000000 0.08057297 0.03290566 0.17033180 0.05277473 0.0949446131 ## NSW.NEWS 0.08057297 1.00000000 0.20605978 0.15796179 0.11770664 0.1722585272 ## NT.NEWS 0.03290566 0.20605978 1.00000000 0.11508855 0.09843727 0.1272152787 ## Q.NEWS 0.17033180 0.15796179 0.11508855 1.00000000 0.13848105 0.0596666377 ## SA.NEWS 0.05277473 0.11770664 0.09843727 0.13848105 1.00000000 0.0589546630 ## T.NEWS 0.09494461 0.17225853 0.12721528 0.05966664 0.05895466 1.0000000000 ## V.NEWS 0.20518539 0.23059292 0.11487366 0.10417249 0.14053366 0.0728362365 ## WA.NEWS -0.02846502 0.20588546 0.19406784 0.17145175 0.12032267 -0.0006090754 ## V.NEWS WA.NEWS ## ACF.NEWS 0.20518539 -0.0284650215 ## NSW.NEWS 0.23059292 0.2058854559 ## NT.NEWS 0.11487366 0.1940678390 ## Q.NEWS 0.10417249 0.1714517524 ## SA.NEWS 0.14053366 0.1203226677 ## T.NEWS 0.07283624 -0.0006090754 ## V.NEWS 1.00000000 0.0313059007 ## WA.NEWS 0.03130590 1.0000000000 ## ## Information Criteria ## AIC BIC AICc ## 1276.07 2000.89 1286.5 ## ## Accuracy Criteria ## Mean Weighted ## MAPE 0.0618 0.0618 ## MSLRE 0.0068 0.0068 tsdiagnose(mod) ## Real Eigenvalues (D): 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0.866 0.866 0.866 0.866 0.866 0.866 0.866 0.866 0.866 0.866 0.866 0.866 0.5 0.5 0.5 0.5 0 0 0.5 0.5 0 0 0.5 0.5 0.5 0.5 0.5 0.5 0.866 0.866 1 0.866 0.866 1 0 0 0 0 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.866 0.866 1 0.866 0.866 1 0 0 0 0 0.5 0.5 0.866 0.866 1 0.5 0.5 0.866 0.866 1 0.867 0.867 0.869 0.869 0.505 0.505 0.01 0.01 0.511 0.511 0.484 0.484 0.846 0.846 0.978 0.023 0.023 0.465 0.465 0.822 0.822 0.952 0.277 0.277 0.277 0.277 0.277 0.277 0.163 0.021 ## ## Multivariate Normality Tests ## Test E df p value MVN ## 1 Doornik-Hansen 216.9969 16 2.844037e-37 NO ## ## Univariate Normality Tests ## Test Variable Statistic p value Normality ## 1 Shapiro-Francia ACF.NEWS 0.9849 0.0011 NO ## 2 Shapiro-Francia NSW.NEWS 0.9943 0.1602 YES ## 3 Shapiro-Francia NT.NEWS 0.9720 &lt;0.001 NO ## 4 Shapiro-Francia Q.NEWS 0.9860 0.0018 NO ## 5 Shapiro-Francia SA.NEWS 0.9845 9e-04 NO ## 6 Shapiro-Francia T.NEWS 0.9867 0.0026 NO ## 7 Shapiro-Francia V.NEWS 0.9688 &lt;0.001 NO ## 8 Shapiro-Francia WA.NEWS 0.9737 &lt;0.001 NO ## ## Multivariate Outliers (Mahalanobis Distance) ## Observation Mahalanobis Distance Outlier ## 9 1988-12-31 21.264 TRUE ## 21 1989-12-31 27.756 TRUE ## 24 1990-03-31 28.731 TRUE ## 33 1990-12-31 21.301 TRUE ## 54 1992-09-30 26.701 TRUE ## 57 1992-12-31 19.622 TRUE ## 59 1993-02-28 19.611 TRUE ## 60 1993-03-31 20.677 TRUE ## 63 1993-06-30 24.081 TRUE ## 64 1993-07-31 20.255 TRUE ## 66 1993-09-30 24.531 TRUE ## 69 1993-12-31 72.481 TRUE ## 72 1994-03-31 35.717 TRUE ## 79 1994-10-31 19.122 TRUE ## 94 1996-01-31 21.925 TRUE ## 105 1996-12-31 53.707 TRUE ## 106 1997-01-31 28.626 TRUE ## 109 1997-04-30 20.485 TRUE ## 121 1998-04-30 45.002 TRUE ## 124 1998-07-31 21.235 TRUE ## 136 1999-07-31 19.706 TRUE ## 142 2000-01-31 47.132 TRUE ## 143 2000-02-29 22.048 TRUE ## 148 2000-07-31 23.980 TRUE ## 157 2001-04-30 31.793 TRUE ## 166 2002-01-31 28.374 TRUE ## 169 2002-04-30 19.785 TRUE ## 172 2002-07-31 56.104 TRUE ## 184 2003-07-31 31.963 TRUE ## 223 2006-10-31 19.369 TRUE ## 226 2007-01-31 26.189 TRUE ## 235 2007-10-31 93.086 TRUE ## 238 2008-01-31 26.753 TRUE ## 241 2008-04-30 35.413 TRUE ## 251 2009-02-28 25.327 TRUE ## 256 2009-07-31 27.555 TRUE ## 259 2009-10-31 22.694 TRUE ## 261 2009-12-31 24.791 TRUE ## 262 2010-01-31 78.543 TRUE ## 263 2010-02-28 57.752 TRUE ## 265 2010-04-30 34.002 TRUE ## 268 2010-07-31 28.282 TRUE ## 274 2011-01-31 107.805 TRUE ## 285 2011-12-31 27.791 TRUE ## 286 2012-01-31 58.481 TRUE ## 287 2012-02-29 23.743 TRUE ## 293 2012-08-31 18.883 TRUE ## 295 2012-10-31 31.069 TRUE ## 301 2013-04-30 58.107 TRUE ## 302 2013-05-31 23.596 TRUE ## 307 2013-10-31 22.618 TRUE ## 309 2013-12-31 32.214 TRUE ## 310 2014-01-31 42.566 TRUE ## 316 2014-07-31 21.128 TRUE ## 322 2015-01-31 20.692 TRUE ## 364 2018-07-31 19.370 TRUE ## 365 2018-08-31 21.088 TRUE Additional methods are similar to what is available in other packages. Coefficients: coef(mod) ## Level[Common] Seasonal[ACF.NEWS] Seasonal[NSW.NEWS] Seasonal[NT.NEWS] ## 7.228622e-01 2.856892e-01 2.394973e-09 3.846394e-09 ## Seasonal[Q.NEWS] Seasonal[SA.NEWS] Seasonal[T.NEWS] Seasonal[V.NEWS] ## 5.731333e-09 3.579614e-09 6.533816e-09 1.510414e-01 ## Seasonal[WA.NEWS] ## 2.543775e-09 Loglikelihood and AIC: logLik(mod) ## &#39;log Lik.&#39; 1034.07 (df=121) AIC(mod) ## [1] 1276.07 Performance metrics with optional weights option: tsmetrics(mod, weights = runif(8)) ## N no_pars LogLik AIC BIC AICc MAPE MSLRE ## 1 2952 121 1034.07 1276.07 2000.889 1286.502 0.06177683 0.006825589 ## WAPE WSLRE ## 1 0.0619873 0.006859379 tscor(mod) ## 8 x 8 Matrix of class &quot;dsyMatrix&quot; ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS SA.NEWS T.NEWS ## ACF.NEWS 1.00000000 0.08057297 0.03290566 0.17033180 0.05277473 0.0949446131 ## NSW.NEWS 0.08057297 1.00000000 0.20605978 0.15796179 0.11770664 0.1722585272 ## NT.NEWS 0.03290566 0.20605978 1.00000000 0.11508855 0.09843727 0.1272152787 ## Q.NEWS 0.17033180 0.15796179 0.11508855 1.00000000 0.13848105 0.0596666377 ## SA.NEWS 0.05277473 0.11770664 0.09843727 0.13848105 1.00000000 0.0589546630 ## T.NEWS 0.09494461 0.17225853 0.12721528 0.05966664 0.05895466 1.0000000000 ## V.NEWS 0.20518539 0.23059292 0.11487366 0.10417249 0.14053366 0.0728362365 ## WA.NEWS -0.02846502 0.20588546 0.19406784 0.17145175 0.12032267 -0.0006090754 ## V.NEWS WA.NEWS ## ACF.NEWS 0.20518539 -0.0284650215 ## NSW.NEWS 0.23059292 0.2058854559 ## NT.NEWS 0.11487366 0.1940678390 ## Q.NEWS 0.10417249 0.1714517524 ## SA.NEWS 0.14053366 0.1203226677 ## T.NEWS 0.07283624 -0.0006090754 ## V.NEWS 1.00000000 0.0313059007 ## WA.NEWS 0.03130590 1.0000000000 Correlation and covariance matrices: tscov(mod) ## 8 x 8 Matrix of class &quot;dsyMatrix&quot; ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS SA.NEWS ## ACF.NEWS 0.0114807146 0.0005198327 0.0006257583 0.0011577660 0.0004522868 ## NSW.NEWS 0.0005198327 0.0036255973 0.0022020876 0.0006033682 0.0005668839 ## NT.NEWS 0.0006257583 0.0022020876 0.0314994638 0.0012957591 0.0013973801 ## Q.NEWS 0.0011577660 0.0006033682 0.0012957591 0.0040242151 0.0007026424 ## SA.NEWS 0.0004522868 0.0005668839 0.0013973801 0.0007026424 0.0063974488 ## T.NEWS 0.0026772075 0.0027295921 0.0059418008 0.0009960919 0.0012409344 ## V.NEWS 0.0016234402 0.0010252769 0.0015054886 0.0004879767 0.0008300212 ## WA.NEWS -0.0012121833 0.0049270593 0.0136891817 0.0043226977 0.0038249274 ## T.NEWS V.NEWS WA.NEWS ## ACF.NEWS 2.677207e-03 0.0016234402 -1.212183e-03 ## NSW.NEWS 2.729592e-03 0.0010252769 4.927059e-03 ## NT.NEWS 5.941801e-03 0.0015054886 1.368918e-02 ## Q.NEWS 9.960919e-04 0.0004879767 4.322698e-03 ## SA.NEWS 1.240934e-03 0.0008300212 3.824927e-03 ## T.NEWS 6.925553e-02 0.0014154031 -6.370458e-05 ## V.NEWS 1.415403e-03 0.0054526903 9.187640e-04 ## WA.NEWS -6.370458e-05 0.0009187640 1.579591e-01 tscor(mod) ## 8 x 8 Matrix of class &quot;dsyMatrix&quot; ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS SA.NEWS T.NEWS ## ACF.NEWS 1.00000000 0.08057297 0.03290566 0.17033180 0.05277473 0.0949446131 ## NSW.NEWS 0.08057297 1.00000000 0.20605978 0.15796179 0.11770664 0.1722585272 ## NT.NEWS 0.03290566 0.20605978 1.00000000 0.11508855 0.09843727 0.1272152787 ## Q.NEWS 0.17033180 0.15796179 0.11508855 1.00000000 0.13848105 0.0596666377 ## SA.NEWS 0.05277473 0.11770664 0.09843727 0.13848105 1.00000000 0.0589546630 ## T.NEWS 0.09494461 0.17225853 0.12721528 0.05966664 0.05895466 1.0000000000 ## V.NEWS 0.20518539 0.23059292 0.11487366 0.10417249 0.14053366 0.0728362365 ## WA.NEWS -0.02846502 0.20588546 0.19406784 0.17145175 0.12032267 -0.0006090754 ## V.NEWS WA.NEWS ## ACF.NEWS 0.20518539 -0.0284650215 ## NSW.NEWS 0.23059292 0.2058854559 ## NT.NEWS 0.11487366 0.1940678390 ## Q.NEWS 0.10417249 0.1714517524 ## SA.NEWS 0.14053366 0.1203226677 ## T.NEWS 0.07283624 -0.0006090754 ## V.NEWS 1.00000000 0.0313059007 ## WA.NEWS 0.03130590 1.0000000000 Note that, even though we impose a diagonal covariance matrix structure, we still return the full covariance matrix in tscov. Fitted and residuals: head(fitted(mod)[,1:4], 4) ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS ## 1988-04-30 5.083103 73.64493 1.826401 32.42638 ## 1988-05-31 5.676515 77.54210 1.989457 33.63410 ## 1988-06-30 5.075795 74.71801 2.035407 34.26671 ## 1988-07-31 4.934308 80.55045 2.505494 36.89035 head(residuals(mod)[,1:4], 4) ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS ## 1988-04-30 0.01689726 -0.6449279 -0.02640146 -0.6263813 ## 1988-05-31 -0.07651455 2.7578955 0.01054259 3.0658951 ## 1988-06-30 -0.07579501 0.5819926 0.36459305 0.4332870 ## 1988-07-31 -0.03430793 -6.3504510 -0.10549408 2.2096475 head(residuals(mod, raw = TRUE)[,1:4], 4) ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS ## 1988-04-30 0.003682669 -0.008795832 -0.020383862 -0.01950604 ## 1988-05-31 -0.015157705 0.034948548 0.007808924 0.08723617 ## 1988-06-30 -0.016684017 0.007759009 0.258079365 0.01256527 ## 1988-07-31 -0.007725214 -0.082119558 -0.071428367 0.05817240 where argument raw denotes the non backtransformed residuals (if a Box Cox calculation was used). States decomposition: tsx &lt;- tsdecompose(mod) head(tsx$Level[,1:4], 4) ## Level[ACF.NEWS] Level[NSW.NEWS] Level[NT.NEWS] Level[Q.NEWS] ## 1988-04-30 1.786760 4.364053 0.8568871 3.546528 ## 1988-05-31 1.775698 4.389549 0.8605698 3.610766 ## 1988-06-30 1.763532 4.395390 1.0451636 3.621028 ## 1988-07-31 1.757843 4.336261 0.9915687 3.664258 head(tsx$Slope[,1:4], 4) ## Slope[ACF.NEWS] Slope[NSW.NEWS] Slope[NT.NEWS] Slope[Q.NEWS] ## 1988-04-30 -0.0001052268 0.0002321876 -0.001962038 0.001179098 ## 1988-05-31 -0.0001052268 0.0002321876 -0.001962038 0.001179098 ## 1988-06-30 -0.0001052268 0.0002321876 -0.001962038 0.001179098 ## 1988-07-31 -0.0001052268 0.0002321876 -0.001962038 0.001179098 head(tsx$Seasonal[,1:4], 4) ## Seasonal[ACF.NEWS] Seasonal[NSW.NEWS] Seasonal[NT.NEWS] ## 1988-04-30 0.049736578 -0.013464613 -0.01416351 ## 1988-05-31 -0.063743434 -0.076059732 0.01605865 ## 1988-06-30 -0.082914767 -0.006738031 0.16097260 ## 1988-07-31 -0.003446856 0.020157472 0.13901380 ## Seasonal[Q.NEWS] ## 1988-04-30 -0.032166073 ## 1988-05-31 -0.077771070 ## 1988-06-30 -0.014257476 ## 1988-07-31 0.009366023 Plot methods exist outputting the fitted values, the states and the residuals, with an argument for the series to output, with a maximum number per call of 10 series: plot(mod) plot(mod, type = &quot;states&quot;) plot(mod, type = &quot;residuals&quot;) The predict function will output a list which includes data.table of each series’ prediction object and state decomposition: p &lt;- predict(mod, h = 12) p$prediction_table ## series Level Slope Seasonal X ## 1: ACF.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 2: NSW.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 3: NT.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 4: Q.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 5: SA.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 6: T.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 7: V.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 8: WA.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## Error Predicted ## 1: &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 2: &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 3: &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 4: &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 5: &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 6: &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 7: &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 8: &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; The plot method on the predicted object can output 1 series at a time using the series argument: plot(p, series = 2) The simulation method also returns a list with a data.table object with each series’ simulation object and state decomposition. Additionally, there are optional arguments for the initial state to use (init_states) and parameter vector (pars). s &lt;- simulate(mod, h = 12*8, nsim = 100, init_states = mod$spec$vets_env$States[,1]) s$simulation_table ## series Level Slope Seasonal ## 1: ACF.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 2: NSW.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 3: NT.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 4: Q.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 5: SA.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 6: T.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 7: V.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## 8: WA.NEWS &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; &lt;tsmodel.predict[2]&gt; ## Simulated ## 1: &lt;tsmodel.predict[2]&gt; ## 2: &lt;tsmodel.predict[2]&gt; ## 3: &lt;tsmodel.predict[2]&gt; ## 4: &lt;tsmodel.predict[2]&gt; ## 5: &lt;tsmodel.predict[2]&gt; ## 6: &lt;tsmodel.predict[2]&gt; ## 7: &lt;tsmodel.predict[2]&gt; ## 8: &lt;tsmodel.predict[2]&gt; par(mfrow = c(4,1),mar = c(3,3,3,3)) plot(s$simulation_table[series == &quot;ACF.NEWS&quot;]$Simulated[[1]], main = &quot;Simulated Series&quot;) plot(s$simulation_table[series == &quot;ACF.NEWS&quot;]$Level[[1]], main = &quot;Simulated Level&quot;) plot(s$simulation_table[series == &quot;ACF.NEWS&quot;]$Slope[[1]], main = &quot;Simulated Slope&quot;) plot(s$simulation_table[series == &quot;ACF.NEWS&quot;]$Seasonal[[1]], main = &quot;Simulated Seasonal&quot;) The tsbacktest method has similar arguments to other backtest implementations in the tsmodels framework: b &lt;- tsbacktest(spec, h = 12, cores = 5, solver = &quot;nlminb&quot;, trace = 0, autodiff = TRUE) The returned object is a list of 2 data.tables with the predictions for each series by estimation date and horizon and the summary metrics table: head(b$prediction) ## series estimation_date horizon size forecast_dates forecast actual ## 1: ACF.NEWS 2003-07-31 1 184 2003-08-31 11.15297 11.2 ## 2: ACF.NEWS 2003-07-31 2 184 2003-09-30 10.61685 10.1 ## 3: ACF.NEWS 2003-07-31 3 184 2003-10-31 10.51334 9.7 ## 4: ACF.NEWS 2003-07-31 4 184 2003-11-30 11.18095 9.7 ## 5: ACF.NEWS 2003-07-31 5 184 2003-12-31 15.55127 14.2 ## 6: ACF.NEWS 2003-07-31 6 184 2004-01-31 10.19090 8.6 head(b$metrics) ## series horizon MAPE MSLRE BIAS n ## 1: ACF.NEWS 1 0.08344100 0.01147017 0.01528171 185 ## 2: ACF.NEWS 2 0.09773936 0.01462113 0.02457635 184 ## 3: ACF.NEWS 3 0.10865025 0.01753223 0.03328862 183 ## 4: ACF.NEWS 4 0.11919299 0.02081765 0.04233478 182 ## 5: ACF.NEWS 5 0.11657522 0.02062666 0.05092162 181 ## 6: ACF.NEWS 6 0.12625000 0.02294433 0.05890854 180 6.7.3 Example: Australian Retail Sales Grouped Dynamics In this example, we show how grouping works on 99 series, where grouping is by sector. suppressWarnings(suppressPackageStartupMessages(library(tsvets))) suppressMessages(library(tsmethods)) suppressMessages(library(tsaux)) suppressMessages(library(xts)) suppressMessages(library(data.table)) data(&quot;austretail&quot;, package = &quot;tsdatasets&quot;) y &lt;- austretail[&quot;2000/&quot;] include &lt;- sapply(1:ncol(y), function(i) all(!is.na(y[,i]))) y &lt;- y[,include] groups &lt;- sapply(1:ncol(y), function(i) strsplit(colnames(y[,i]), &quot;\\\\.&quot;)[[1]][2]) groups_index &lt;- sort.int(groups, index.return = TRUE) y &lt;- y[,groups_index$ix] groups &lt;- groups[groups_index$ix] groups &lt;- rle(groups)$lengths groups &lt;- unlist(sapply(1:length(groups), function(i) rep(i, groups[i]))) spec &lt;- vets_modelspec(y[,1:99], level = &quot;grouped&quot;, slope = &quot;grouped&quot;, damped = &quot;none&quot;, group = groups[1:99], seasonal = &quot;grouped&quot;, lambda = 0.5, dependence = &quot;diagonal&quot;, frequency = 12, cores = 3) mod &lt;- estimate(spec, solver = &quot;solnp&quot;, control = list(trace = 0)) The joint estimation of the 99 series takes about 3.366302 minutes. We avoid the summary method for such a large object, and instead print out the coefficients and the performance metrics: cf &lt;- coef(mod) print(data.table::data.table(Coefficient = names(cf), Value = round(cf,4))) ## Coefficient Value ## 1: Level[Group = 1] 0.4458 ## 2: Level[Group = 2] 0.5108 ## 3: Level[Group = 3] 0.7151 ## 4: Level[Group = 4] 0.6882 ## 5: Level[Group = 5] 0.1675 ## 6: Level[Group = 6] 0.5208 ## 7: Level[Group = 7] 0.5494 ## 8: Level[Group = 8] 0.4183 ## 9: Level[Group = 9] 0.5011 ## 10: Level[Group = 10] 0.5771 ## 11: Level[Group = 11] 0.7378 ## 12: Level[Group = 12] 0.4934 ## 13: Level[Group = 13] 0.7206 ## 14: Slope[Group = 1] 0.0000 ## 15: Slope[Group = 2] 0.0000 ## 16: Slope[Group = 3] 0.0000 ## 17: Slope[Group = 4] 0.0000 ## 18: Slope[Group = 5] 0.0074 ## 19: Slope[Group = 6] 0.0000 ## 20: Slope[Group = 7] 0.0000 ## 21: Slope[Group = 8] 0.0000 ## 22: Slope[Group = 9] 0.0000 ## 23: Slope[Group = 10] 0.0061 ## 24: Slope[Group = 11] 0.0000 ## 25: Slope[Group = 12] 0.0000 ## 26: Slope[Group = 13] 0.0000 ## 27: Seasonal[Group = 1] 0.3440 ## 28: Seasonal[Group = 2] 0.1879 ## 29: Seasonal[Group = 3] 0.0000 ## 30: Seasonal[Group = 4] 0.0000 ## 31: Seasonal[Group = 5] 0.1335 ## 32: Seasonal[Group = 6] 0.2645 ## 33: Seasonal[Group = 7] 0.0000 ## 34: Seasonal[Group = 8] 0.3150 ## 35: Seasonal[Group = 9] 0.3885 ## 36: Seasonal[Group = 10] 0.0000 ## 37: Seasonal[Group = 11] 0.0000 ## 38: Seasonal[Group = 12] 0.2692 ## 39: Seasonal[Group = 13] 0.0000 ## Coefficient Value tsmetrics(mod) ## N no_pars LogLik AIC BIC AICc MAPE MSLRE ## 1 22572 1425 -7562.715 -4712.715 6722.149 -4520.522 0.04042485 0.003137474 ## WAPE WSLRE ## 1 0.04042485 0.003137474 plot(mod, series = (1:99)[groups == 4]) plot(mod, series = (1:99)[groups == 4], type = &quot;states&quot;) plot(mod, series = (1:99)[groups == 4], type = &quot;residuals&quot;) 6.7.4 Example: Australian Retail Sales Filtering This example highlights the use of the tsfilter method for online filtering as already discussed in Sections 4.6.4 and 5.5.5. suppressWarnings(suppressPackageStartupMessages(library(tsvets))) suppressMessages(library(tsmethods)) suppressMessages(library(tsaux)) suppressMessages(library(xts)) data(&quot;austretail&quot;, package = &quot;tsdatasets&quot;) y &lt;- na.omit(austretail[,grepl(&quot;NEWS&quot;, colnames(austretail))]) y_train &lt;- y[1:(369 - 24)] y_test &lt;- y[(369 - 24 + 1):369] spec &lt;- vets_modelspec(y_train, level = &quot;common&quot;, slope = &quot;diagonal&quot;, damped = &quot;none&quot;, seasonal = &quot;common&quot;, lambda = 0.5, dependence = &quot;diagonal&quot;, frequency = 12, cores = 3) mod &lt;- estimate(spec, solver = &quot;solnp&quot;, control = list(trace=0)) filt &lt;- tsfilter(mod, y = y_test) tail(fitted(mod)) ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS SA.NEWS T.NEWS V.NEWS ## 2016-07-31 5.876297 83.52440 1.512683 51.51574 11.945775 12.23984 46.78722 ## 2016-08-31 5.522048 86.27697 1.560434 54.08378 10.737312 13.03752 46.71343 ## 2016-09-30 5.206287 82.17841 1.414184 52.44660 9.951203 13.82086 45.70285 ## 2016-10-31 4.996807 83.30330 1.327481 55.07050 9.966181 12.29451 48.87455 ## 2016-11-30 5.570064 87.80605 1.311445 54.84109 10.453560 12.09789 50.22824 ## 2016-12-31 8.063519 115.33473 1.798219 71.62148 15.714844 14.80842 76.59479 ## WA.NEWS ## 2016-07-31 40.08457 ## 2016-08-31 41.81813 ## 2016-09-30 39.66528 ## 2016-10-31 40.57748 ## 2016-11-30 42.00032 ## 2016-12-31 55.08142 head(tail(fitted(filt), 24 + 1), 5) ## ACF.NEWS NSW.NEWS NT.NEWS Q.NEWS SA.NEWS T.NEWS V.NEWS ## 2016-12-31 8.063519 115.33473 1.798219 71.62148 15.714844 14.80842 76.59479 ## 2017-01-31 5.113393 92.95258 1.275950 58.22844 8.766206 12.32122 50.85179 ## 2017-02-28 6.785631 94.95922 1.141774 53.31780 9.525980 13.28888 48.24517 ## 2017-03-31 6.037202 99.96287 1.128547 54.11226 11.156456 11.88752 52.25142 ## 2017-04-30 4.924493 83.00795 1.157302 51.71266 8.581191 11.20606 47.70697 ## WA.NEWS ## 2016-12-31 55.08142 ## 2017-01-31 48.57365 ## 2017-02-28 41.41009 ## 2017-03-31 40.95814 ## 2017-04-30 38.37743 6.7.5 Example: Australian Retail Sales Aggregation This demonstrates the ability to aggregate a model estimated with homogeneous coefficients. We continue with the retail sales data, which being a set of flow variables, can be aggregated. suppressWarnings(suppressPackageStartupMessages(library(tsvets))) suppressMessages(library(tsmethods)) suppressMessages(library(tsaux)) suppressMessages(library(xts)) data(&quot;austretail&quot;, package = &quot;tsdatasets&quot;) y &lt;- na.omit(austretail[,grepl(&quot;NEWS&quot;, colnames(austretail))]) We use a vector of ones as weights, to denote summation to the total for the News category. weights &lt;- rep(1, 8) Additionally, we also include, purely for expositional purposes, a matrix of 5 randomly generated regressors: xreg &lt;- xts(matrix(rnorm(nrow(y) * 5), nrow = nrow(y), ncol = 5), index(y)) xreg_include &lt;- matrix(2, ncol = 5, nrow = ncol(y)) xreg_include[2,1] &lt;- 1 Setting the include matrix to the value of 2 means pooling, and we also set regressor 1 for series 2 to a value of 1 in order to denote that we want this to be estimated seperately (non-pooled). spec &lt;- vets_modelspec(y, level = &quot;common&quot;, slope = &quot;common&quot;, seasonal = &quot;common&quot;, dependence = &quot;diagonal&quot;, frequency = 12, cores = 2, xreg = xreg, xreg_include = xreg_include) mod &lt;- estimate(spec, solver = &quot;solnp&quot;, control = list(trace = 0)) We can extract specific matrices from the estimate object using the as yet unexported function p_matrix: tsvets:::p_matrix(mod)$X ## 8 x 5 Matrix of class &quot;dgeMatrix&quot; ## x1 x2 x3 x4 x5 ## ACF.NEWS -0.0006278521 0.003462852 -0.005961621 -0.005416136 0.007784848 ## NSW.NEWS -0.2329853731 0.003462852 -0.005961621 -0.005416136 0.007784848 ## NT.NEWS -0.0006278521 0.003462852 -0.005961621 -0.005416136 0.007784848 ## Q.NEWS -0.0006278521 0.003462852 -0.005961621 -0.005416136 0.007784848 ## SA.NEWS -0.0006278521 0.003462852 -0.005961621 -0.005416136 0.007784848 ## T.NEWS -0.0006278521 0.003462852 -0.005961621 -0.005416136 0.007784848 ## V.NEWS -0.0006278521 0.003462852 -0.005961621 -0.005416136 0.007784848 ## WA.NEWS -0.0006278521 0.003462852 -0.005961621 -0.005416136 0.007784848 where we can observe that the coefficients for each regressor have been pooled, with the exception of x1 for the NSW.NEWS series. We can now proceed to aggregate the model using the tsaggregate method on the estimated object: mod_aggregate &lt;- tsaggregate(mod, weights = weights, return_model = TRUE) summary(mod_aggregate) ## ## Vector ETS ## ----------------------------------- ## Level : common ## Slope : common ## Seasonal : common ## Dependence : diagonal ## No. Series : 1 ## No. TimePoints : 369 ## ## Parameter Matrices ## ## Level Matrix ## 1 x 1 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] ## [1,] 0.6346078 ## ## Slope Matrix ## 1 x 1 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] ## [1,] 1.687443e-09 ## ## Seasonal Matrix ## 1 x 1 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] ## [1,] 0.06527485 ## ## Regressor Matrix ## 1 x 5 Matrix of class &quot;dgeMatrix&quot; ## x1 x2 x3 x4 x5 ## aggregate -0.2373803 0.02770281 -0.04769297 -0.04332909 0.06227878 ## ## Covariance Matrix ## 1 x 1 diagonal matrix of class &quot;ddiMatrix&quot; ## aggregate ## aggregate 234.2035 ## ## Accuracy Criteria ## Mean Weighted ## MAPE 0.0363 0.0363 ## MSLRE 0.0022 0.0022 plot(mod_aggregate) suppressWarnings(plot(mod_aggregate, type = &quot;states&quot;)) We can then proceed with prediction as well as simulation, both of which allow for the outputs to be aggregated. Alternatively, since we now have an aggregated model object, we can also predict or simulate directly from that object. newxreg &lt;- xts(matrix(rnorm(24 * 5), nrow = 24, ncol = 5), future_dates(tail(index(y),1),&quot;months&quot;,24)) p &lt;- predict(mod, newxreg = newxreg) p_agg_series &lt;- tsaggregate(p, weights = weights) p_agg_model &lt;- predict(mod_aggregate, newxreg = newxreg) plot(p_agg_series$distribution, gradient_color = &quot;whitesmoke&quot;, interval_color = &quot;grey&quot;, main = &quot;Agggregated Model&quot;) plot(p_agg_model$prediction_table[1]$Predicted[[1]]$distribution, gradient_color = &quot;whitesmoke&quot;, add = TRUE, median_color = 2, interval_color = &quot;grey10&quot;) As we can observe, the output of the 2 predictions is identical (subject to some randomness from the simulation of the predictive distribution). We do the same for the simulation method: s &lt;- simulate(mod, nsim = 100, h = 24, newxreg = newxreg, seed = 700) s_agg_series &lt;- tsaggregate(s, weights = weights) s_agg_model &lt;- simulate(mod_aggregate, nsim = 100, h = 24, newxreg = newxreg, seed = 700) plot(s_agg_series$distribution, gradient_color = &quot;whitesmoke&quot;, interval_color = &quot;grey&quot;) plot(s_agg_model$simulation_table[1]$Simulated[[1]]$distribution, gradient_color = &quot;whitesmoke&quot;, add = TRUE, median_color = 2, interval_color = &quot;grey10&quot;) which is again almost identical, but we can achieve exact replication is we use the bootstrap residuals method: s &lt;- simulate(mod, nsim = 100, h = 24, newxreg = newxreg, seed = 700, bootstrap = TRUE) s_agg_series &lt;- tsaggregate(s, weights = weights) s_agg_model &lt;- simulate(mod_aggregate, nsim = 100, h = 24, newxreg = newxreg, seed = 700, bootstrap = TRUE) plot(s_agg_series$distribution, gradient_color = &quot;whitesmoke&quot;, interval_color = &quot;grey&quot;) plot(s_agg_model$simulation_table[1]$Simulated[[1]]$distribution, gradient_color = &quot;whitesmoke&quot;, add = TRUE, median_color = 2, interval_color = &quot;grey10&quot;) 6.7.6 Example: Groupwise Pooling It is also possible to have group wise pooling. We saw above that we could specify pooling using the number of series times number of regressors matrix, xreg_include, with values 0, 1 or 2+ (0 = no beta, 1 = individual beta and 2+ = grouped beta). Group pooling can be achieved by setting values of 2+. For instance 2 series sharing one pooled estimate, and 3 other series sharing another grouped estimate would have values of (2,2,3,3,3). In the following example we have 8 series and 2 regressors, with different combinations of pooling. xreg &lt;- xts(matrix(rnorm(nrow(y)*2), ncol = 2, nrow = nrow(y)), index(y)) colnames(xreg) &lt;- c(&quot;B1&quot;,&quot;B2&quot;) xreg_include &lt;- matrix(1, ncol = 2, nrow = ncol(y)) rownames(xreg_include) &lt;- colnames(y) colnames(xreg_include) &lt;- colnames(xreg) # individual [1,1], grouped 2:3, 5:6 and 7:8 xreg_include[c(2:3),1] &lt;- 2 xreg_include[c(5:6),1] &lt;- 3 xreg_include[c(7:8),1] &lt;- 4 # individual 1, zero for 2:3, group for 4:8 xreg_include[c(2:3),2] &lt;- 0 xreg_include[c(4:8),2] &lt;- 3 xreg_include ## B1 B2 ## ACF.NEWS 1 1 ## NSW.NEWS 2 0 ## NT.NEWS 2 0 ## Q.NEWS 1 3 ## SA.NEWS 3 3 ## T.NEWS 3 3 ## V.NEWS 4 3 ## WA.NEWS 4 3 spec &lt;- vets_modelspec(y, level = &quot;grouped&quot;, slope = &quot;grouped&quot;, group = c(1,1,2,2,3,3,4,4), damped = &quot;none&quot;, seasonal = &quot;grouped&quot;, xreg = xreg, xreg_include = xreg_include, lambda = 0, dependence = &quot;diagonal&quot;, frequency = 12, cores = 1) mod &lt;- estimate(spec, solver = &quot;solnp&quot;, control = list(trace=1)) ## ## Iter: 1 fn: -3268.7348 Pars: 0.568520542590 0.757063662858 0.739300626307 0.721241104486 0.000000001321 0.000000012253 0.000000014204 0.000000011224 0.252008270229 0.000000019280 0.000000026531 0.108783931664 -0.005679503277 0.001396122981 0.000772620213 0.002019528332 -0.000196995561 -0.000744253725 -0.000323345837 ## Iter: 2 fn: -3268.7348 Pars: 0.5685181872452 0.7570639668783 0.7393013865909 0.7212409676968 0.0000000008501 0.0000000114833 0.0000000132992 0.0000000104188 0.2520102598516 0.0000000182205 0.0000000252465 0.1087843044123 -0.0056795436508 0.0013961043508 0.0007726513884 0.0020195645415 -0.0001969902036 -0.0007443168563 -0.0003233704645 ## solnp--&gt; Completed in 2 iterations Note that the Box Cox parameter lambda should be the same for the grouped series else the coefficients should not be pooled (as they have different scales). 6.7.7 Example: Price Volume Aggregation This final example shows how to jointly model a series based on price-volume relationship with a target of generating a model for Sales: \\(Sales = Price\\times Volume\\) While this is a multiplicative model, if we were to model the series using a log transformation we would be able to aggregate the model in logs before exponentiating to recover the Sales as the example will demonstrate. data(&quot;priceunits&quot;, package = &quot;tsdatasets&quot;) spec &lt;- vets_modelspec(priceunits, level = &quot;common&quot;, slope = &quot;common&quot;, frequency = 12, lambda = rep(0,2), dependence = &quot;full&quot;) mod &lt;- estimate(spec, solver = &quot;optim&quot;, control = list(trace = 0)) summary(mod) ## ## Vector ETS ## ----------------------------------- ## Level : common ## Slope : common ## Seasonal : none ## Dependence : full ## No. Series : 2 ## No. TimePoints : 100 ## ## Parameter Matrices ## ## Level Matrix ## 2 x 2 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] [,2] ## [1,] 0.3900419 . ## [2,] . 0.3900419 ## ## Slope Matrix ## 2 x 2 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] [,2] ## [1,] 0.09196958 . ## [2,] . 0.09196958 ## ## Correlation Matrix ## 2 x 2 Matrix of class &quot;dsyMatrix&quot; ## price units ## price 1.0000000 -0.5939793 ## units -0.5939793 1.0000000 ## ## Information Criteria ## AIC BIC AICc ## 162.58 182.37 163.02 ## ## Accuracy Criteria ## Mean Weighted ## MAPE 0.0508 0.0508 ## MSLRE 0.0042 0.0042 plot(mod) plot(mod, type = &quot;states&quot;) weights &lt;- c(1,1) mod_aggregate &lt;- tsaggregate(mod, weights = weights, return_model = TRUE) summary(mod_aggregate) ## ## Vector ETS ## ----------------------------------- ## Level : common ## Slope : common ## Seasonal : none ## Dependence : diagonal ## No. Series : 1 ## No. TimePoints : 100 ## ## Parameter Matrices ## ## Level Matrix ## 1 x 1 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] ## [1,] 0.3900419 ## ## Slope Matrix ## 1 x 1 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] ## [1,] 0.09196958 ## ## Covariance Matrix ## 1 x 1 diagonal matrix of class &quot;ddiMatrix&quot; ## aggregate ## aggregate 0.003901451 ## ## Accuracy Criteria ## Mean Weighted ## MAPE 0.0496 0.0496 ## MSLRE 0.0039 0.0039 p &lt;- predict(mod, h = 24) p_agg_model &lt;- predict(mod_aggregate, h = 24) p_agg_series &lt;- tsaggregate(p, weights = weights) plot(p_agg_model$prediction_table[1]$Predicted[[1]]$distribution, gradient_color = &quot;whitesmoke&quot;, interval_color = &quot;grey&quot;) plot(p_agg_series$distribution, gradient_color = &quot;whitesmoke&quot;, add = TRUE, median_color = 2, interval_color = &quot;grey10&quot;) In the presence of a non-homogeneous model, or a homogeneous model with Box Cox parameter not equal to either 0 (log) or 1 (no transform), then the tsaggregate method will still work but the return_model arguments needs to be set to FALSE and the resulting output will just be the aggregated series. References "],["tsforeign.html", "Chapter 7 tsforeign package 7.1 Introduction 7.2 ARIMA model 7.3 bsts model", " Chapter 7 tsforeign package 7.1 Introduction The tsforeign package provides custom wrappers for the auto.arima function from the forecast package of R. Hyndman et al. (2021) package and the bsts method from bsts package of Steven L. Scott (2021), with methods for estimation, some diagnostics and prediction. This package may be extended in future to provide wrappers for other interesting models. For the bsts model, we have also made use of the tsconvert method from the tsmethods package to convert the output of the model to one conforming to the required inputs of the dlm package, for which we provide an example in the demonstration section. Since both of those packages have their own vignettes and documentation, we proceed here directly into a demonstration of the functionality. 7.2 ARIMA model The entry point for the ARIMA model is the arima_modelspec function suppressPackageStartupMessages(library(tsforeign)) library(xts) library(tsaux) args(arima_modelspec) ## function (y, xreg = NULL, frequency = NULL, seasonal = FALSE, ## seasonal_type = &quot;regular&quot;, lambda = NULL, seasonal_harmonics = NULL, ## lambda_lower = 0, lambda_upper = 1.5, ...) ## NULL which allows for both regular and trigonometric seasonality. For the demonstration example, we’ll use the AirPassengers dataset after first converting it into an xts object: air &lt;- AirPassengers dt &lt;- future_dates(as.Date(&quot;1948-12-31&quot;), &quot;months&quot;, length(air)) air &lt;- xts(as.numeric(air), dt) spec &lt;- arima_modelspec(y = air, frequency = 12, seasonal_type = &quot;regular&quot;, seasonal = TRUE, lambda = NA) mod &lt;- estimate(spec) summary(mod) ## ARIMA(0,1,1)(0,1,1)[12] ## ------------------------ ## Estimate Std.Error t value Pr(&gt;|t|) ## ma1 -0.4018 0.08964 -4.482 7.381e-06 ## sma1 -0.5569 0.07310 -7.619 2.554e-14 ## ## sigma : 0.037 ## ## AIC BIC AICc ## -483.31 -474.69 -483.13 ## ## MAPE MASE MSLRE BIAS ## 0.0262 0.2297 0.0012 0 tsmetrics(mod) ## n no.pars LogLik AIC BIC AICc MAPE MASE ## 1 131 2 244.6574 -483.3147 -474.6891 -483.1258 0.02623633 0.2297063 ## MSLRE BIAS ## 1 0.001228422 4.059721e-05 plot(mod) The predict function generates a predictive distribution via simulation and returns an object of class tsmodel.predict: p &lt;- predict(mod, h = 12) plot(p) Additionally, there is also a backtest method: b &lt;- tsbacktest(spec, h = 12, alpha = c(0.02, 0.1), cores = 5, trace = F) b$metrics ## horizon variable MAPE MSLRE BIAS n MIS[0.02] MIS[0.1] ## 1: 1 y 0.02359723 0.0009551475 0.003600284 72 78.94686 58.69298 ## 2: 2 y 0.02832357 0.0013691638 0.006263684 71 79.20002 64.44840 ## 3: 3 y 0.03298197 0.0018345480 0.008278681 70 93.00588 76.27669 ## 4: 4 y 0.03711406 0.0023039090 0.009785940 69 115.69397 85.80206 ## 5: 5 y 0.03852619 0.0024661099 0.011665334 68 123.99674 89.97901 ## 6: 6 y 0.04160760 0.0027693072 0.013290887 67 132.75520 96.02243 ## 7: 7 y 0.04391732 0.0031022106 0.015707828 66 139.52245 100.76898 ## 8: 8 y 0.04531778 0.0031863408 0.018558568 65 138.42669 103.98039 ## 9: 9 y 0.04704741 0.0033943049 0.020792507 64 141.69528 112.37043 ## 10: 10 y 0.04913576 0.0037630478 0.024066220 63 151.25610 118.77286 ## 11: 11 y 0.05020191 0.0039190388 0.026212273 62 165.19382 124.18195 ## 12: 12 y 0.05271486 0.0042050364 0.027817457 61 169.09282 130.58446 7.3 bsts model The bsts package of (Steven L. Scott 2021) provides functions for fitting Bayesian structural time series models. The entry point in our wrapper is the bsts_modelspec function: args(bsts_modelspec) ## function (y, xreg = NULL, frequency = NULL, differences = 0, ## level = TRUE, slope = TRUE, damped = FALSE, seasonal = FALSE, ## seasonal_frequency = 4, ar = FALSE, ar_max = 1, cycle = FALSE, ## cycle_frequency = NULL, cycle_names = NULL, seasonal_type = &quot;regular&quot;, ## lambda = NULL, lambda_lower = 0, lambda_upper = 1, seasonal_harmonics = NULL, ## distribution = &quot;gaussian&quot;, ...) ## NULL which provides a rich set of options such as degree of differencing,17 a sparse AR component, sparse regressors, regular or trigonometric seasonal components (including multiple seasonal), cyclical components and the Box Cox transformation. Once an object is estimated, and all required components have been generated, any additional methods on the estimated component are performed directly by custom written functions in the tsforeign package. For the demonstration we’ll use the priceunits dataset from the tsdatasets package. Once a series is estimated, the resulting MCMC draws are converted to an mcmc object from the coda package of Plummer et al. (2020) in order to provide a nice summary report. data(&quot;priceunits&quot;, package = &quot;tsdatasets&quot;) spec &lt;- bsts_modelspec(y = priceunits[1:80,1], frequency = 12, differences = 0, level = T, slope = T, damped = T, seasonal = T, seasonal_frequency = 12, ar = T, ar_max = 3, lambda = 0) mod &lt;- estimate(spec, n_iter = 2000, trace = FALSE) summary(mod) ## ## Iterations = 1:2000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 2000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE Prob[Include] ## obs.sigma 0.013388 0.006782 1.516e-04 0.0007126 1.0000 ## level.sigma 0.004859 0.003980 8.900e-05 0.0009739 1.0000 ## slope.sigma 0.032426 0.006154 1.376e-04 0.0004130 1.0000 ## slope.ar -0.386517 0.246273 5.507e-03 0.0212243 1.0000 ## seasonal12.sigma 0.002162 0.002382 5.326e-05 0.0008249 1.0000 ## ar1 0.086651 0.283209 6.333e-03 0.0262069 0.7170 ## ar2 0.047933 0.152370 3.407e-03 0.0138606 0.4125 ## ar3 0.001713 0.055853 1.249e-03 0.0022075 0.2230 ## ar3.sigma 0.028651 0.007280 1.628e-04 0.0004397 1.0000 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## obs.sigma 0.0053262 0.009204 0.012454 0.01650 0.026228 ## level.sigma 0.0004008 0.001554 0.003921 0.00710 0.014547 ## slope.sigma 0.0203082 0.028481 0.032512 0.03635 0.045049 ## slope.ar -0.7785795 -0.562759 -0.419128 -0.24420 0.199956 ## seasonal12.sigma 0.0001650 0.000566 0.001388 0.00281 0.009602 ## ar1 -0.4300711 -0.032893 0.000000 0.23004 0.800293 ## ar2 -0.1646536 0.000000 0.000000 0.02509 0.532574 ## ar3 -0.1201496 0.000000 0.000000 0.00000 0.126217 ## ar3.sigma 0.0201813 0.024911 0.028040 0.03152 0.039091 ## ## ## Harvey&#39;s Goodness of Fit Statistic: 0.2149929 ## ## MAPE MASE MSLRE BIAS ## 0.0371 0.4704 0.0023 -5e-04 tsmetrics(mod) ## n no.pars MAPE MASE MSLRE BIAS ## 1 80 9 0.03714795 0.4704494 0.00228517 -0.0004797903 plot(mod) The decomposition of the model into it’s fitted components is done via the tsdecompose method. However, note that the bsts routine returns the smoothed component states, not the filtered ones. tde &lt;- tsdecompose(mod) # distribution objects of state components str(tde) ## List of 5 ## $ Level : &#39;tsmodel.distribution&#39; num [1:1971, 1:80] 1.73 1.67 1.65 1.57 1.67 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:80] &quot;1992-08-31&quot; &quot;1992-09-30&quot; &quot;1992-10-31&quot; &quot;1992-11-30&quot; ... ## ..- attr(*, &quot;date_class&quot;)= chr &quot;Date&quot; ## $ Slope : &#39;tsmodel.distribution&#39; num [1:1971, 1:80] -0.0475 -0.0139 0.0316 0.0715 0.0208 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:80] &quot;1992-08-31&quot; &quot;1992-09-30&quot; &quot;1992-10-31&quot; &quot;1992-11-30&quot; ... ## ..- attr(*, &quot;date_class&quot;)= chr &quot;Date&quot; ## $ AR : &#39;tsmodel.distribution&#39; num [1:1971, 1:80] -0.0672 -0.0672 -0.0617 0.0286 -0.0556 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:80] &quot;1992-08-31&quot; &quot;1992-09-30&quot; &quot;1992-10-31&quot; &quot;1992-11-30&quot; ... ## ..- attr(*, &quot;date_class&quot;)= chr &quot;Date&quot; ## $ X : NULL ## $ Seasonal12: &#39;tsmodel.distribution&#39; num [1:1971, 1:80] -0.0581 -0.049 -0.0325 -0.0383 -0.0651 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:80] &quot;1992-08-31&quot; &quot;1992-09-30&quot; &quot;1992-10-31&quot; &quot;1992-11-30&quot; ... ## ..- attr(*, &quot;date_class&quot;)= chr &quot;Date&quot; Once bsts model is estimated, we can convert it to a dlm object18 using either the mean values of the parameters and initial states, or a specific draw using the tsconvert method. library(dlm) dlm_model &lt;- tsconvert(mod, to = &quot;dlm&quot;, draw = &quot;mean&quot;, burn = bsts::SuggestBurn(0.1, mod$model)) str(dlm_model) ## List of 6 ## $ m0: num [1:17] 1.56998 0.01946 -0.00449 -0.04085 -0.03048 ... ## $ C0: num [1:17, 1:17] 1e+07 0e+00 0e+00 0e+00 0e+00 0e+00 0e+00 0e+00 0e+00 0e+00 ... ## $ FF: num [1, 1:17] 1 0 0 1 0 0 0 0 0 0 ... ## $ V : num [1, 1] 0.0131 ## $ GG: num [1:17, 1:17] 1 0 0 0 0 0 0 0 0 0 ... ## $ W : num [1:17, 1:17] 0.00492 0 0 0 0 ... ## - attr(*, &quot;class&quot;)= chr &quot;dlm&quot; filtered_dlm &lt;- dlmFilter(log(priceunits[1:80,1]), dlm_model) smoothed_dlm &lt;- dlmSmooth(filtered_dlm) smoothed_series &lt;- xts((dlm_model$FF %*% t(smoothed_dlm$s))[1,-1], index(priceunits)[1:80]) par(mfrow = c(2,1), mar = c(3,3,3,3)) plot(as.zoo(filtered_dlm$f), type = &quot;l&quot;, main = &quot;Filtered Series&quot;, ylab = &quot;&quot;) lines(as.zoo(fitted(mod, raw = TRUE)), col = 2) grid() legend(&quot;topright&quot;, c(&quot;DLM&quot;,&quot;BSTS&quot;), col = 1:2, lty = 1, bty = &quot;n&quot;) plot(as.zoo(smoothed_series), type = &quot;l&quot;, main = &quot;Smoothed Series&quot;, ylab = &quot;&quot;) lines(as.zoo(fitted(mod, raw = TRUE, type = &quot;smoothed&quot;)), col = 2) grid() legend(&quot;topright&quot;, c(&quot;DLM&quot;,&quot;BSTS&quot;), col = 1:2, lty = 1, bty = &quot;n&quot;) We compare the predictive distribution of the predicted object from calling the predict method of the tsforeign package and that of the bsts package. Note that there are a lot more arguments which can be passed to the predict routine, including user overrides for the last state means and the posterior means of the parameters (see the documentation for more details). p1 &lt;- predict(mod, h = 12) # BSTS method p2 &lt;- bsts::predict.bsts(mod$model, horizon = 12) # convert predictive distribution to tsmodel.distribution for comparison p2d &lt;- p2$distribution colnames(p2d) &lt;- colnames(p1$distribution) class(p2d) &lt;- &quot;tsmodel.distribution&quot; plot(log(p1$distribution), main = &quot;tsforeign native predict c++ routine vs bsts routine&quot;) plot(p2d, add = TRUE, median_color = 2, interval_color = 2, median_type = 2) A predicted object can also be decomposed into it’s structural components, something we are able to do because of the custom predict routine since bsts does not return this information. td &lt;- tsdecompose(p1) par(mfrow = c(2,2), mar = c(3,3,3,3)) plot(td$Level, main = &quot;Level&quot;) plot(td$Slope, main = &quot;Slope&quot;) plot(td$Seasonal12, main = &quot;Seasonal&quot;) plot(td$AR, main = &quot;AR&quot;) References "],["tscausal.html", "Chapter 8 tscausal package 8.1 Introduction 8.2 Measures of Evaluation 8.3 Evaluation of Significance 8.4 Package Features 8.5 Package Demo", " Chapter 8 tscausal package 8.1 Introduction The objective of the tscausal package is to provide an easy to use framework to evaluate any time series based forecast distribution, proxy for the counterfactual distribution, and generate an automatic set of diagnostics on the significance of an intervention. It is heavily influenced by the CausalImpact package described in Brodersen et al. (2015), and from which it borrows parts of the code diagnostics and output narrative.19 Unlike CausalImpact, the package is agnostic to any particular time series model, but it does require that the forecast distribution conform to certain requirements, namely that it inherits the class tsmodel.distribution. Any matrix of size \\(N\\times h\\), representing the \\(N\\) simulated points for each forecast period \\(t\\in h\\), where \\(t&gt;T\\) (the intervention date), can be coerced to this class. It is assumed that the user can and has trained a model on the pre-intervention period and our code repository provides for a wide array of models, with common calling conventions and the ability to generate frequentist simulated or Bayesian predictive distributions, all of which inherit class tsmodel.distribution. 8.2 Measures of Evaluation This section provides a summary overview of the measures used to evaluate the possible presence of a causal effect, and follow Brodersen et al. (2015) closely. 8.2.1 Pointwise Impact Given an intervention date \\(T\\), and a post-intervention period under consideration \\(T+1,\\ldots,T+h\\), the causal pointwise effect \\(\\phi^{(s)}_{t}\\) for all \\(t \\in T+1,\\ldots,T+h\\) given a draw \\(s\\) from the predictive distribution is: \\[ \\phi^{(s)}_t = y_t - \\hat y^{(s)}_t \\] where \\(y_t\\) represents the outcome variable and \\(\\hat{y}_t\\) the forecasted (or counterfactual) variable. Because we have a distributional forecast, which represents the uncertainty about the forecasted variable \\(\\hat{y}_t\\) at each date in the horizon \\(T+1,\\ldots,T+h\\), and draws indexed by \\((s)_1,\\ldots,(s)_N\\), we can generate an empirical distribution of any function applied to the distribution (e.g. summation, differences etc). Given a significance level \\(a\\)%, the \\(100-a\\)% confidence interval can be evaluated by making use of the empirical quantile function from this distribution. 8.2.2 Cumulative Impact The cumulative impact is derived from the pointwise impact as: \\[ \\Phi _t^{(s)} = \\sum\\limits_{i = T + 1}^t {\\phi _i^{(s)}}, \\] which is a useful quantity when the outcome represents a flow variable (e.g. revenue, signups), measured over an interval of time, however is not interpretable when it represents a stock variable (e.g. inventory, subscribers). 8.2.3 Mean Effect Given an intervention date \\(T\\), and a post-intervention period \\(T+1,\\ldots,T+h\\), then the average effect given a draw \\(s\\) from the predictive distribution is: \\[ {\\alpha ^{(s)}} = \\frac{1}{{t - T}}\\sum\\limits_{i = T + 1}^{T + h} {{{y_i} - \\hat y_i^{(s)}} }. \\] 8.2.4 Mean Relative Effect A related measure, showing the relative effect of the intervention can be defined: \\[ {r^{(s)}} = \\frac{{{\\alpha ^{(s)}}}}{{{\\mu _{\\hat y}}}}, \\] where \\(\\mu_{\\bar y}\\) is the mean of the forecasted variable \\(\\forall t&gt;T\\) and \\(\\forall s\\). 8.3 Evaluation of Significance Given a significance level \\(a\\), under the null hypothesis that the intervention was not significant, we fail to reject if \\(F^{-1}_n\\left(\\alpha/2\\right)&lt;0\\) and \\(F^{-1}_n\\left(1-\\alpha/2\\right)&gt;0\\), and reject otherwise. For instance, if the effect was positive with a lower quantile value above zero then we would reject the null and conclude that the effect was positive and significant. If the effect was negative with an upper quantile value less than zero we would also reject the null and conclude that the effect was negative and significant. Another statistic which can be used to evaluate whether the results could have occurred by chance is given by the following: \\[ \\frac{1}{N}\\sum\\limits_{j = 1}^N {\\left[ {\\left( {\\sum\\limits_{i = T + 1}^{T + h} {\\phi _t^{(j)}} } \\right) \\geqslant 0} \\right]}, \\] which measures the percent of times the total net effect of the distribution was positive, measured over the entire forecast horizon and across all draws. 8.4 Package Features The package has one main method called tscausal which takes as arguments a tsmodel.distribution object, the outcome variable (as xts object which includes both the pre and post intervention data), an optional xts vector (or tsmodel.distribution) of the in-sample fitted values and the significance level for hypothesis testing alpha, as shown below: ## function (object, actual, fitted = NULL, alpha = 0.05, include_cumulative = TRUE, ## ...) ## NULL Additionally, once the causal object has been created, there are methods for plotting (plot), printing of results (print) as well as a reporting (tsreport) which can be used to automatically generate a pdf, doc or html report with full evaluation and graphics. The next section provides a demonstration. 8.5 Package Demo We use the priceunits dataset from the tsdatasets package, in particular the units series, and add an artificial structural break during 1999-03, which could represent for instance a large drop in prices, a large substitution effect or a successful marketing campaign. data(priceunits, package = &quot;tsdatasets&quot;) priceunits &lt;- priceunits[, 2] priceunits[&quot;1999-03/&quot;] &lt;- priceunits[&quot;1999-03/&quot;] + 10 train &lt;- &quot;/1999-02&quot; test = &quot;1999-03/1999-08&quot; y_train = priceunits[train] y_test = priceunits[test] spec &lt;- ets_modelspec(y_train, frequency = 12, lambda = 0, model = &quot;AAN&quot;) mod &lt;- estimate(spec) summary(mod) ## ## ETS Model [ AAN ] ## ## Parameter Description Est[Value] ## ---------- ------------------ ----------- ## alpha State[Level-coef] 0.3253 ## beta State[Slope-coef] 0.1057 ## l0 State[Level-init] 1.3528 ## b0 State[Slope-init] 0.0632 ## ## ## LogLik AIC BIC AICc ## -------- ------- ------- ------- ## 25.0856 -40.17 -28.32 -39.35 ## ## ## MAPE MASE MSLRE BIAS ## ------- ------- ------- ------- ## 0.0692 0.9563 0.0067 0.0054 prediction &lt;- predict(mod, h = nrow(y_test), nsim = 5000) Note that the prediction object has a number of slots, one of which is called distribution and inherits the following classes: tsets.distribution, tsmodel.distribution. All our packages have a predictable and common set of slots in the returned prediction objects. The final step is to pass the forecast distribution to the tscausal method and generate a report with a narrative explaining the results and a decision on whether to reject or not the null of no intervention effect. cause &lt;- tscausal(prediction$distribution, actual = rbind(y_train, y_test), fitted = fitted(mod)) print(cause) ## Predictive inference {tscausal} ## ## Average Cumulative ## Actual 37.33 224.00 ## Prediction (s.d.) 27.77 (2.521) 166.64 (15.124) ## 95% CI [23, 33] [138, 199] ## ## Absolute effect (s.d.) 9.559 (2.521) 57.356 (15.124) ## 95% CI [4.2, 14] [25.2, 86] ## ## Relative effect (s.d.) 34.42% (9.075%) 34.42% (9.075%) ## 95% CI [15%, 52%] [15%, 52%] ## ## Predictive Distribution tail-area probability p: 2e-04 ## Predictive Distribution prob. of a causal effect: 99.98% tsreport(cause) During the post-intervention period, the response variable had an average value of approx. 37.3333. By contrast, in the absence of an intervention, we would have expected an average response of 27.77. The 95% interval of this counterfactual prediction is [23.0249, 33.1377]. Subtracting this prediction from the observed response yields an estimate of the causal effect the intervention had on the response variable. This effect is 9.5593 with a 95% interval of [4.1957, 14.3084]. For a discussion of the significance of this effect, see below. Summing up the individual data points during the post-intervention period (which can only sometimes be meaningfully interpreted), the response variable had an overall value of 224.0000. By contrast, had the intervention not taken place, we would have expected a sum of 166.64. The 95% interval of this prediction is [138.1494, 198.8259]. The above results are given in terms of absolute numbers. In relative terms, the response variable showed an increase of +34%. The 95% interval of this percentage is [+15%, +52%]. This means that the positive effect observed during the intervention period is statistically significant and unlikely to be due to random fluctuations. It should be noted, however, that the question of whether this increase also bears substantive significance can only be answered by comparing the absolute effect (9.5593) to the original goal of the underlying intervention. The probability of obtaining this effect by chance is very small (Bayesian one-sided tail-area probability p = 0). This means the causal effect can be considered statistically significant. NULL The tsreport method has options for printing to screen or generating a pdf, html or docx (with option to use a template as well). Example usage is provided below: tsreport(cause, type = &quot;pdf&quot;, output_dir = &quot;~/tmp&quot;, args = list(model = &quot;ETS[AAN]&quot;, frequency = 12, name = &quot;Units&quot;)) tsreport(cause, type = &quot;html&quot;, output_dir = &quot;~/tmp&quot;, args = list(model = &quot;ETS[AAN]&quot;, frequency = 12, name = &quot;Units&quot;)) tsreport(cause, type = &quot;doc&quot;, doc_template = &quot;~/tmp/mytemplate.docx&quot;, output_dir = &quot;~/tmp&quot;, args = list(model = &quot;ETS[AAN]&quot;, frequency = 12, name = &quot;Units&quot;)) References "],["other.html", "Chapter 9 Other packages", " Chapter 9 Other packages The tsaux package contains a set of functions used across all the packages of our framework. The tsdatasets packages contains some sample datasets we use for benchmarking and examples. This may be expanded in the future. "],["news.html", "Chapter 10 News", " Chapter 10 News Date: 2021-11-19 The tsvetsad package now implements automatic differentiation (AD) for the VETS model implemented in the tsvets package. An argument use_autofiff is now available in the estimate method which will then dispatch to the estimate_ad method. Gradients and Hessian are implemented, but for the latter an additional option use_hessian needs to be set to TRUE and it currently defaulted to FALSE in the tsvetsad package. The package is still in beta testing as speed optimization need to be undertaken to perform more competitively to the non-AD method. Date: 2021-06-20 The tsissmad package now implements automatic differentiation (AD) for the ISSM model implemented in the tsissm package. An argument use_autofiff is now available in the estimate method which will then dispatch to the estimate_ad method. Gradients and Hessian are implemented, but for the latter an additional option use_hessian needs to be set to TRUE and it currently defaulted to FALSE in the tsissmad package. Only trigonometric seasonality is implemented but regular seasonality may be included in a future update. Additionally, only the nlminb solver is used for autodiff. Date: 2021-05-17 The tsetad package now implements automatic differentiation (AD) for all ETS models implemented in the tsets package. An argument use_autofiff is now available in the estimate method which will then dispatch to the estimate_ad method. Gradients and Hessian are implemented, and it is suggested that the nlminb solver be used which makes use of both. Speedups for highly parameterized or difficult (powerMAM) models has been observed to be upto 16x when using AD. Addition of AD to the tsissm and tsvets packages may be imlemented in due course. Missing values are now handled during model estimation using the predict step and only calculating the likelihood for non-missing values. Applies to the tests and tsissm packages. For the tsvets package we have not yet decided on the implementation. This may come at a future date. "],["team.html", "Chapter 11 The tsmodels team", " Chapter 11 The tsmodels team The core team is comprised of a group of friends, and at some point ex-colleagues, with an interest in time series forecasting: Alexios Galanos (aut,cre) Keith O’Hara (ctb) James Nesbit (ctb) Professor Diego J. Pedregal (ctb and advisor) Professor Eric Zivot (academic sponsor and advisor) Professor Doug Martin (advisor) "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
